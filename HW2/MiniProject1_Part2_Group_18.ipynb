{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment aims to introduce students to working with the BEIR dataset for information retrieval tasks. Students will:\n",
    "\n",
    "- Understand the structure of the BEIR dataset and preprocess the data.\n",
    "- Implement a system to encode queries and documents using embeddings.\n",
    "- Calculate similarity scores to rank documents based on relevance.\n",
    "- Evaluate the system's performance using metrics like Mean Average Precision (MAP).\n",
    "- Modify and fine-tune models for better retrieval results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First start by looking into the Dataset and understanding its structure.\n",
    "This will help you understand how the dataset is formed, which will be useful in the later stages of the Assignment\n",
    "\n",
    "https://huggingface.co/datasets/BeIR/nfcorpus\n",
    "\n",
    "https://huggingface.co/datasets/BeIR/nfcorpus-qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assignment consists of two key tasks: Ranking Documents and Fine-Tuning the Sentence Transformer Model. \n",
    "# Students will be graded based on their implementation and their written report.\n",
    "\n",
    "# Mention the team/Individual contributions as a part of the report..!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking Documents Report (10 Points)\n",
    "\n",
    "# Students must analyze which encoding methods performed best for document ranking.\n",
    "\n",
    "# What to include in your report:\n",
    "    \n",
    "# Comparison of Encoding Methods \n",
    "\n",
    "    # Compare GloVe embeddings vs. Sentence Transformer embeddings.\n",
    "    # Which method ranked documents better?\n",
    "    # Did the top-ranked documents make sense?\n",
    "    # How does cosine similarity behave with different embeddings?\n",
    "\n",
    "# Observations on Cosine Similarity & Ranking \n",
    "\n",
    "    # Did the ranking appear meaningful?\n",
    "    # Were there cases where documents that should be highly ranked were not?\n",
    "    # What are possible explanations for incorrect rankings?\n",
    "\n",
    "# Possible Improvements\n",
    "\n",
    "    # What can be done to improve document ranking?\n",
    "    # Would a different distance metric (e.g., Euclidean, Manhattan) help?\n",
    "    # Would preprocessing the queries or documents (e.g., removing stopwords) improve ranking?\n",
    "\n",
    "\n",
    "# Fine-Tuning Report (15 Points)\n",
    "\n",
    "# After fine-tuning, students must compare different training approaches and reflect on their findings.\n",
    "\n",
    "# What to include in your report:\n",
    "    \n",
    "# Comparison of Different Training Strategies \n",
    "\n",
    "    # [anchor, positive] vs [anchor, positive, negative].\n",
    "    # Which approach seemed to improve ranking?\n",
    "    # How did the model behave differently?\n",
    "\n",
    "# Impact on MAP Score \n",
    "\n",
    "    # Did fine-tuning improve or hurt the Mean Average Precision (MAP) score?\n",
    "    # If MAP decreased, why might that be?\n",
    "    # Is fine-tuning always necessary for retrieval models?\n",
    "\n",
    "# Observations on Training Loss & Learning Rate \n",
    "\n",
    "    # Did the loss converge?\n",
    "    # Was the learning rate too high or too low?\n",
    "    # How did freezing/unfreezing layers impact training?\n",
    "\n",
    "# Future Improvements \n",
    "\n",
    "    # Would training with more negatives help?\n",
    "    # Would changing the loss function (e.g., using Softmax Loss) improve performance?\n",
    "    # Could increasing the number of epochs lead to a better model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: sentence_transformers in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sentence_transformers) (4.48.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sentence_transformers) (1.15.1)\n",
      "Requirement already satisfied: Pillow in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: sympy in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15fa9de7b0d400987eeb7d43fefb690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your API token from your Hugging Face Account. Make sure to save it in text file or notepad for future use.\n",
    "# Will need to add it once per section\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 22:20:38.949763: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-08 22:20:38.972869: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-08 22:20:38.972895: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-08 22:20:38.972899: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-08 22:20:38.976830: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "class TextSimilarityModel:\n",
    "    def __init__(self, corpus_name, rel_name, model_name='all-MiniLM-L6-v2', top_k=10):\n",
    "        \"\"\"\n",
    "        Initialize the model with datasets and pre-trained sentence transformer.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.corpus_name = corpus_name\n",
    "        self.rel_name = rel_name\n",
    "        self.top_k = top_k\n",
    "        self.load_data()\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and filter datasets based on test queries and documents.\n",
    "        \"\"\"\n",
    "        # Load query and document datasets\n",
    "        dataset_queries = load_dataset(self.corpus_name, \"queries\")\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "\n",
    "        # Extract queries and documents\n",
    "        self.queries = dataset_queries[\"queries\"][\"text\"]\n",
    "        self.query_ids = dataset_queries[\"queries\"][\"_id\"]\n",
    "        self.documents = dataset_docs[\"corpus\"][\"text\"]\n",
    "        self.document_ids = dataset_docs[\"corpus\"][\"_id\"]\n",
    "\n",
    "        # Filter queries and documents and build relevant queries and documents mapping based on test set\n",
    "        test_qrels = load_dataset(self.rel_name)[\"test\"]\n",
    "        self.filtered_test_query_ids = set(test_qrels[\"query-id\"])\n",
    "        self.filtered_test_doc_ids = set(test_qrels[\"corpus-id\"])\n",
    "\n",
    "        self.test_queries = [q for qid, q in zip(self.query_ids, self.queries) if qid in self.filtered_test_query_ids]\n",
    "        self.test_query_ids = [qid for qid in self.query_ids if qid in self.filtered_test_query_ids]\n",
    "        self.test_documents = [doc for did, doc in zip(self.document_ids, self.documents) if did in self.filtered_test_doc_ids]\n",
    "        self.test_document_ids = [did for did in self.document_ids if did in self.filtered_test_doc_ids]\n",
    "\n",
    "        self.test_query_id_to_relevant_doc_ids = {qid: [] for qid in self.test_query_ids}\n",
    "        for qid, doc_id in zip(test_qrels[\"query-id\"], test_qrels[\"corpus-id\"]):\n",
    "            if qid in self.test_query_id_to_relevant_doc_ids:\n",
    "                self.test_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "                \n",
    "        ## Code Below this is used for creating the training set \n",
    "        # Build query and document id to text mapping\n",
    "        self.query_id_to_text = {query_id:query for query_id, query in zip(self.query_ids, self.queries)}\n",
    "        self.document_id_to_text = {document_id:document for document_id, document in zip(self.document_ids, self.documents)}\n",
    "\n",
    "        # Build relevant queries and documents mapping based on train set\n",
    "        train_qrels = load_dataset(self.rel_name)[\"train\"]\n",
    "        self.train_query_id_to_relevant_doc_ids = {qid: [] for qid in train_qrels[\"query-id\"]}\n",
    "\n",
    "        for qid, doc_id in zip(train_qrels[\"query-id\"], train_qrels[\"corpus-id\"]):\n",
    "            if qid in self.train_query_id_to_relevant_doc_ids:\n",
    "                # Append the document ID to the relevant doc mapping\n",
    "                self.train_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "        \n",
    "        # Filter queries and documents and build relevant queries and documents mapping based on validation set  \n",
    "        #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "        try:\n",
    "            val_qrels = load_dataset(self.rel_name)['validation']\n",
    "            self.filtered_val_query_ids = set(val_qrels['query-id'])\n",
    "            self.filtered_val_doc_ids = set(val_qrels['corpus-id'])\n",
    "\n",
    "            val_query_pairs = [\n",
    "                (qid, query) for qid, query in zip(self.query_ids, self.queries) \n",
    "                if qid in self.filtered_val_query_ids\n",
    "            ]\n",
    "            self.val_query_ids = [pair[0] for pair in val_query_pairs]\n",
    "            self.val_queries = [pair[1] for pair in val_query_pairs]\n",
    "\n",
    "            val_doc_pairs = [\n",
    "                (did, doc) for did, doc in zip(self.document_ids, self.documents) \n",
    "                if did in self.filtered_val_doc_ids\n",
    "            ]\n",
    "            self.val_document_ids = [pair[0] for pair in val_doc_pairs]\n",
    "            self.val_documents = [pair[1] for pair in val_doc_pairs]\n",
    "\n",
    "            self.val_query_id_to_relevant_doc_ids = {qid: [] for qid in self.val_query_ids}\n",
    "            for qid, doc_id in zip(val_qrels['query-id'], val_qrels['corpus-id']):\n",
    "                if qid in self.val_query_id_to_relevant_doc_ids:\n",
    "                    self.val_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "        except Exception as e:\n",
    "            print('No validation split available. Skipping validation set creation.')\n",
    "        ###########################################################################\n",
    "        \n",
    "\n",
    "    #Task 1: Encode Queries and Documents (10 Pts)\n",
    "\n",
    "    def encode_with_glove(self, glove_file_path: str, sentences: list[str]) -> list[np.ndarray]:\n",
    "\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - glove_file_path (str): Path to the GloVe embeddings file (e.g., \"glove.6B.50d.txt\").\n",
    "            - sentences (list[str]): A list of sentences to encode.\n",
    "\n",
    "        # Output:\n",
    "            - list[np.ndarray]: A list of sentence embeddings \n",
    "            \n",
    "        (1) Encodes sentences by averaging GloVe 50d vectors of words in each sentence.\n",
    "        (2) Return a sequence of embeddings of the sentences.\n",
    "        Download the glove vectors from here. \n",
    "        https://nlp.stanford.edu/data/glove.6B.zip\n",
    "        Handle unknown words by using zero vectors\n",
    "        \"\"\"\n",
    "        #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "        # Load GloVe embeddings dictionary\n",
    "        glove_dict = {}\n",
    "        with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                # Skip incomplete lines (should contain one word + 50 dimensions)\n",
    "                if len(parts) < 51:\n",
    "                    continue\n",
    "                word = parts[0]\n",
    "                vector = np.array(parts[1:], dtype=float)\n",
    "                glove_dict[word] = vector\n",
    "\n",
    "        embedding_dim = len(next(iter(glove_dict.values())))\n",
    "        embeddings = []\n",
    "\n",
    "        # Encode each sentence by averaging word vectors\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            vecs = []\n",
    "            for token in tokens:\n",
    "                # Use lower-case tokens to match the GloVe keys\n",
    "                token_vec = glove_dict.get(token.lower())\n",
    "                if token_vec is None:\n",
    "                    token_vec = np.zeros(embedding_dim)\n",
    "                vecs.append(token_vec)\n",
    "            if vecs:\n",
    "                sentence_embedding = np.mean(vecs, axis=0)\n",
    "            else:\n",
    "                sentence_embedding = np.zeros(embedding_dim)\n",
    "            embeddings.append(sentence_embedding)\n",
    "\n",
    "        return embeddings\n",
    "        ###########################################################################\n",
    "\n",
    "    #Task 2: Calculate Cosine Similarity and Rank Documents (20 Pts)\n",
    "    \n",
    "    def rank_documents(self, encoding_method: str = 'sentence_transformer') -> None:\n",
    "        \"\"\"\n",
    "         # Inputs:\n",
    "            - encoding_method (str): The method used for encoding queries/documents. \n",
    "                             Options: ['glove', 'sentence_transformer'].\n",
    "\n",
    "        # Output:\n",
    "            - None (updates self.query_id_to_ranked_doc_ids with ranked document IDs).\n",
    "    \n",
    "        (1) Compute cosine similarity between each document and the query\n",
    "        (2) Rank documents for each query and save the results in a dictionary \"query_id_to_ranked_doc_ids\" \n",
    "            This will be used in \"mean_average_precision\"\n",
    "            Example format {2: [125, 673], 35: [900, 822]}\n",
    "        \"\"\"\n",
    "        if encoding_method == 'glove':\n",
    "            query_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.queries)\n",
    "            document_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.documents)\n",
    "        elif encoding_method == 'sentence_transformer':\n",
    "            query_embeddings = self.model.encode(self.queries)\n",
    "            document_embeddings = self.model.encode(self.documents)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid encoding method. Choose 'glove' or 'sentence_transformer'.\")\n",
    "        \n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "        # Map test query IDs to their indices in the full query list.\n",
    "        test_query_indices = [self.query_ids.index(qid) for qid in self.test_query_ids]\n",
    "        # Map test document IDs to their indices in the full document list.\n",
    "        test_doc_indices = [self.document_ids.index(doc_id) for doc_id in self.test_document_ids]\n",
    "        \n",
    "        # Subset the embeddings for test queries and documents using the computed indices.\n",
    "        test_query_embeddings = [query_embeddings[i] for i in test_query_indices]\n",
    "        test_document_embeddings = [document_embeddings[i] for i in test_doc_indices]\n",
    "        \n",
    "        # Now compute the cosine similarity matrix only on test queries vs test documents.\n",
    "        sim_matrix = cosine_similarity(test_query_embeddings, test_document_embeddings)\n",
    "\n",
    "        # Initialize the dictionary for storing ranked document IDs.\n",
    "        self.query_id_to_ranked_doc_ids = {}\n",
    "        \n",
    "        # For each test query, rank the documents based on their similarity scores.\n",
    "        for i, qid in enumerate(self.test_query_ids):\n",
    "            sim_scores = sim_matrix[i]\n",
    "            # Sort document indices by descending similarity.\n",
    "            ranked_indices = np.argsort(sim_scores)[::-1]\n",
    "            ranked_doc_ids = [self.test_document_ids[idx] for idx in ranked_indices]\n",
    "            self.query_id_to_ranked_doc_ids[qid] = ranked_doc_ids\n",
    "        ###########################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def average_precision(relevant_docs: list[str], candidate_docs: list[str], k: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Implement steps:\n",
    "        1. Only take the first k candidate documents\n",
    "        2. Calculate the number of relevant documents in the first k documents\n",
    "        3. Calculate MAP@k\n",
    "        \n",
    "        Note:\n",
    "        - k is usually set to 10, because users rarely look at more results\n",
    "        - This approach is more realistic in practical applications\n",
    "        - It better evaluates the model's performance on the most relevant documents\n",
    "        \"\"\"\n",
    "        # Only take the first k documents\n",
    "        candidate_docs = candidate_docs[:k]\n",
    "        # Calculate which documents are relevant\n",
    "        y_true = [1 if doc_id in relevant_docs else 0 for doc_id in candidate_docs]\n",
    "        # Calculate precision at each position\n",
    "        precisions = [np.mean(y_true[:i+1]) for i in range(len(y_true)) if y_true[i]]\n",
    "        return np.mean(precisions) if precisions else 0\n",
    "\n",
    "    #Task 3: Calculate Evaluate System Performance (10 Pts)\n",
    "    \n",
    "    def mean_average_precision(self) -> float:\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - None (uses ranked documents stored in self.query_id_to_ranked_doc_ids).\n",
    "\n",
    "        # Output:\n",
    "            - float: The MAP score, computed as the mean of all average precision scores.\n",
    "    \n",
    "        (1) Compute mean average precision for all queries using the \"average_precision\" function.\n",
    "        (2) Compute the mean of all average precision scores\n",
    "        Return the mean average precision score\n",
    "        \n",
    "        reference: https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map\n",
    "        https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2\n",
    "        \"\"\"\n",
    "         #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "        ap_scores = []\n",
    "        for qid in self.test_query_ids:\n",
    "            relevant_docs = self.test_query_id_to_relevant_doc_ids.get(qid, [])\n",
    "            candidate_docs = self.query_id_to_ranked_doc_ids.get(qid, [])\n",
    "            ap = self.average_precision(relevant_docs, candidate_docs)\n",
    "            ap_scores.append(ap)\n",
    "        return np.mean(ap_scores) if ap_scores else 0.0\n",
    "        ###########################################################################\n",
    "    \n",
    "    #Task 4: Ranking the Top 10 Documents based on Similarity Scores (10 Pts)\n",
    "   \n",
    "    def show_ranking_documents(self, example_query: str) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - example_query (str): A query string for which top-ranked documents should be displayed.\n",
    "\n",
    "        # Output:\n",
    "            - None (prints the ranked documents along with similarity scores).\n",
    "        \n",
    "        (1) rank documents with given query with cosine similarity scores\n",
    "        (2) prints the top 10 results along with its similarity score.\n",
    "        \n",
    "        \"\"\"\n",
    "        #TODO Put your code here. \n",
    "        query_embedding = self.model.encode(example_query)\n",
    "        document_embeddings = self.model.encode(self.documents)\n",
    "        ###########################################################################\n",
    "        # Compute cosine similarity scores between the query and all documents\n",
    "        sim_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "        # Get indices of top K documents based on the similarity scores\n",
    "        top_k_indices = np.argsort(sim_scores)[::-1][:self.top_k]\n",
    "\n",
    "        print(f'Top {self.top_k} documents for the query: \"{example_query}\"')\n",
    "        for rank, idx in enumerate(top_k_indices, start=1):\n",
    "            doc_id = self.document_ids[idx]\n",
    "            score = sim_scores[idx]\n",
    "            print(f'Rank {rank}: Document ID: {doc_id}, Similarity Score: {score:.4f}')\n",
    "            ###########################################################################\n",
    "      \n",
    "    #Task 5:Fine tune the sentence transformer model (25 Pts)\n",
    "    # Students are not graded on achieving a high MAP score. \n",
    "    # The key is to show understanding, experimentation, and thoughtful analysis.\n",
    "    \n",
    "    def fine_tune_model(self, batch_size: int = 32, num_epochs: int = 3, save_model_path: str = \"finetuned_senBERT\") -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Fine-tunes the model using MultipleNegativesRankingLoss.\n",
    "        (1) Prepare training examples from `self.prepare_training_examples()`\n",
    "        (2) Experiment with [anchor, positive] vs [anchor, positive, negative]\n",
    "        (3) Define a loss function (`MultipleNegativesRankingLoss`)\n",
    "        (4) Freeze all model layers except the final layers\n",
    "        (5) Train the model with the specified learning rate\n",
    "        (6) Save the fine-tuned model\n",
    "        \"\"\"\n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "        \"\"\"\n",
    "        Fine-tunes the model using MultipleNegativesRankingLoss.\n",
    "        \"\"\"\n",
    "        # Import torch at the beginning of the method\n",
    "        import torch\n",
    "        from torch.utils.data import DataLoader\n",
    "        import time\n",
    "        from datetime import timedelta\n",
    "        \n",
    "        # Check device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        # Prepare training examples\n",
    "        train_examples = self.prepare_training_examples()\n",
    "        print(f\"Number of training examples: {len(train_examples)}\")\n",
    "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "        \n",
    "        # Define the loss function\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(self.model)\n",
    "        \n",
    "        # Print model parameters status\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Freeze all layers except the final layer\n",
    "        for name, param in self.model.named_parameters():\n",
    "            # Only unfreeze the final transformer layer (layer.5 for all-MiniLM-L6-v2)\n",
    "            if 'layer.5' in name:  # all-MiniLM-L6-v2 has 6 layers (0-5)\n",
    "                param.requires_grad = True\n",
    "                print(f\"Unfreezing: {name}\")\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Print trainable parameters to verify\n",
    "        print(\"\\nTrainable parameters:\")\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"- {name}\")\n",
    "                \n",
    "        # Training loop with timing\n",
    "        start_time = time.time()\n",
    "        print(\"\\nStarting training...\")\n",
    "        \n",
    "        # Fine-tune the model with warmup\n",
    "        warmup_steps = int(len(train_dataloader) * 0.1)  # 10% of training data for warmup\n",
    "        self.model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=num_epochs,\n",
    "            warmup_steps=warmup_steps,\n",
    "            show_progress_bar=True,\n",
    "            output_path=save_model_path,\n",
    "            checkpoint_path=f\"{save_model_path}_checkpoint\",\n",
    "            checkpoint_save_steps=len(train_dataloader),\n",
    "            callback=lambda score, epoch, steps: print(f\"\\nEpoch {epoch}: Score {score:.4f}\")\n",
    "        )\n",
    "        \n",
    "        # Calculate and print training time\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {str(timedelta(seconds=int(training_time)))}\")\n",
    "        \n",
    "        # Save the model\n",
    "        print(f\"Saving model to {save_model_path}\")\n",
    "        self.model.save(save_model_path)\n",
    "        print(\"Model saved successfully!\")\n",
    "        ###########################################################################\n",
    "\n",
    "    # Take a careful look into how the training set is created\n",
    "    def prepare_training_examples(self) -> list[InputExample]:\n",
    "\n",
    "        \"\"\"\n",
    "        Prepares training examples from the training data.\n",
    "        # Inputs:\n",
    "            - None (uses self.train_query_id_to_relevant_doc_ids to create training pairs).\n",
    "\n",
    "         # Output:\n",
    "            Output: - list[InputExample]: A list of training samples containing [anchor, positive] or [anchor, positive, negative].\n",
    "            \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Prepares training examples from the training data.\n",
    "        \"\"\"\n",
    "        train_examples = []\n",
    "        import random\n",
    "        from datetime import timedelta\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        print(\"\\nPreparing training examples...\")\n",
    "        total_queries = len(self.train_query_id_to_relevant_doc_ids)\n",
    "        print(f\"Total queries to process: {total_queries}\")\n",
    "        \n",
    "        # Count total examples that will be created\n",
    "        total_examples = sum(len(doc_ids) for doc_ids in self.train_query_id_to_relevant_doc_ids.values())\n",
    "        print(f\"Expected total training examples: {total_examples}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create progress bar\n",
    "        pbar = tqdm(self.train_query_id_to_relevant_doc_ids.items(), \n",
    "                    total=total_queries,\n",
    "                    desc=\"Processing queries\")\n",
    "        \n",
    "        for qid, doc_ids in pbar:\n",
    "            anchor = self.query_id_to_text[qid]\n",
    "            # Precompute negative candidates for current query using set subtraction for efficiency\n",
    "            relevant_set = set(self.train_query_id_to_relevant_doc_ids.get(qid, []))\n",
    "            negative_candidates = list(set(self.document_ids) - relevant_set)\n",
    "            \n",
    "            for doc_id in doc_ids:\n",
    "                positive = self.document_id_to_text[doc_id]\n",
    "                \n",
    "                # Update progress bar description with current query details\n",
    "                pbar.set_description(f\"Query {qid}: {len(doc_ids)} docs\")\n",
    "                \n",
    "                # Build texts list without an explicit else branch.\n",
    "                texts = [anchor, positive]\n",
    "                if negative_candidates:\n",
    "                    texts.append(self.document_id_to_text[random.choice(negative_candidates)])\n",
    "                train_examples.append(InputExample(texts=texts))\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nTraining examples preparation completed in {timedelta(seconds=int(elapsed_time))}\")\n",
    "        print(f\"Final number of training examples: {len(train_examples)}\")\n",
    "        \n",
    "        return train_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking with sentence_transformer...\n",
      "Mean Average Precision: 0.477390368428073\n",
      "Ranking with glove...\n",
      "Mean Average Precision: 0.08843636869952659\n",
      "Top 10 documents for the query: \"Breast Cancer Cells Feed on Cholesterol\"\n",
      "Rank 1: Document ID: MED-2439, Similarity Score: 0.6946\n",
      "Rank 2: Document ID: MED-2434, Similarity Score: 0.6723\n",
      "Rank 3: Document ID: MED-2440, Similarity Score: 0.6473\n",
      "Rank 4: Document ID: MED-2427, Similarity Score: 0.5877\n",
      "Rank 5: Document ID: MED-2774, Similarity Score: 0.5498\n",
      "Rank 6: Document ID: MED-838, Similarity Score: 0.5406\n",
      "Rank 7: Document ID: MED-2430, Similarity Score: 0.5205\n",
      "Rank 8: Document ID: MED-2102, Similarity Score: 0.5141\n",
      "Rank 9: Document ID: MED-2437, Similarity Score: 0.5081\n",
      "Rank 10: Document ID: MED-5066, Similarity Score: 0.5012\n"
     ]
    }
   ],
   "source": [
    "# Initialize and use the model\n",
    "model = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "\n",
    "# Compare the outputs \n",
    "print(\"Ranking with sentence_transformer...\")\n",
    "model.rank_documents(encoding_method='sentence_transformer')\n",
    "map_score = model.mean_average_precision()\n",
    "print(\"Mean Average Precision:\", map_score)\n",
    "\n",
    "# Compare the outputs \n",
    "print(\"Ranking with glove...\")\n",
    "model.rank_documents(encoding_method='glove')\n",
    "map_score = model.mean_average_precision()\n",
    "print(\"Mean Average Precision:\", map_score)\n",
    "\n",
    "\n",
    "model.show_ranking_documents(\"Breast Cancer Cells Feed on Cholesterol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Preparing training examples...\n",
      "Total queries to process: 2590\n",
      "Expected total training examples: 110575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query PLAIN-3474: 83 docs: 100%|██████████| 2590/2590 [00:51<00:00, 49.99it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples preparation completed in 0:00:51\n",
      "Final number of training examples: 110575\n",
      "Number of training examples: 110575\n",
      "Total parameters: 22,713,216\n",
      "Trainable parameters: 22,713,216\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Trainable parameters:\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c044b695d9e499d9f8c7eeff9691630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34560' max='34560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34560/34560 33:14, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.085800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.628100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.574200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.499200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.321400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.229500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.164200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.122100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.076400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 0:33:17\n",
      "Saving model to finetuned_senBERT_train_v2\n",
      "Model saved successfully!\n",
      "Mean Average Precision: 0.47489604955042447\n"
     ]
    }
   ],
   "source": [
    "# Finetune all-MiniLM-L6-v2 sentence transformer model\n",
    "model.fine_tune_model(batch_size=32, num_epochs=10, save_model_path=\"finetuned_senBERT_train_v2\")  # Adjust batch size and epochs as needed\n",
    "\n",
    "model.rank_documents()\n",
    "map_score = model.mean_average_precision()\n",
    "print(\"Mean Average Precision:\", map_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model before fine-tuning...\n",
      "Initial MAP score (before fine-tuning): 0.477390368428073\n",
      "\n",
      "Fine-tuning with [anchor, positive] strategy...\n",
      "Using device: cuda\n",
      "\n",
      "Preparing training examples with [anchor, positive] strategy...\n",
      "Total queries to process: 2590\n",
      "Expected total training examples: 110575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query PLAIN-3474: 83 docs: 100%|██████████| 2590/2590 [00:54<00:00, 47.53it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples preparation completed in 0:00:54\n",
      "Final number of training examples: 110575\n",
      "Number of training examples: 110575\n",
      "Total parameters: 22,713,216\n",
      "Trainable parameters: 22,713,216\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Trainable parameters:\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fa8c272da24e7fbf1dbf34f32aa169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34560' max='34560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34560/34560 19:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.986200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.946200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.902900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.823900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.800300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.785600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.738100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.737200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.716900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.712300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.684600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.660800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.658700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.647800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.652900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.634400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.629300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.593400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.576500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.577900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.558700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.543100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>2.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.535900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>2.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.504700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.525100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.509800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>2.517300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>2.515100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>2.501500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.488100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>2.479600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.489100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>2.488500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.474100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>2.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>2.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>2.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>2.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>2.466500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>2.459300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>2.453200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 0:19:26\n",
      "Saving model to finetuned_senBERT_ap\n",
      "Model saved successfully!\n",
      "\n",
      "Evaluating model after fine-tuning...\n",
      "\n",
      "Comparison of MAP scores:\n",
      "1. Original model (before fine-tuning): 0.4774\n",
      "2. After fine-tuning with [anchor, positive]: 0.4632\n",
      "3. Previous run with [anchor, positive, negative]: 0.4748\n"
     ]
    }
   ],
   "source": [
    "# Create a new model instance using [anchor, positive] strategy\n",
    "model_ap = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "\n",
    "# First, evaluate the model before fine-tuning\n",
    "print(\"Evaluating model before fine-tuning...\")\n",
    "model_ap.rank_documents()\n",
    "initial_map_score = model_ap.mean_average_precision()\n",
    "print(\"Initial MAP score (before fine-tuning):\", initial_map_score)\n",
    "\n",
    "# Define a new method for preparing training examples with [anchor, positive] pairs\n",
    "def prepare_training_examples_ap(self) -> list[InputExample]:\n",
    "    \"\"\"\n",
    "    Prepares training examples using only [anchor, positive] pairs.\n",
    "    This implementation focuses on direct semantic matching without negative samples.\n",
    "    \n",
    "    # Output:\n",
    "        - list[InputExample]: A list of training samples containing [anchor, positive] pairs.\n",
    "        \n",
    "    Key differences from the original implementation:\n",
    "    1. Uses only positive pairs without negative samples\n",
    "    2. Helps model focus on direct semantic relationships\n",
    "    3. May result in different ranking behavior\n",
    "    \"\"\"\n",
    "    train_examples = []\n",
    "    import random\n",
    "    from datetime import timedelta\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(\"\\nPreparing training examples with [anchor, positive] strategy...\")\n",
    "    total_queries = len(self.train_query_id_to_relevant_doc_ids)\n",
    "    print(f\"Total queries to process: {total_queries}\")\n",
    "    \n",
    "    # Count total examples that will be created\n",
    "    total_examples = sum(len(doc_ids) for doc_ids in self.train_query_id_to_relevant_doc_ids.values())\n",
    "    print(f\"Expected total training examples: {total_examples}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create progress bar for monitoring\n",
    "    pbar = tqdm(self.train_query_id_to_relevant_doc_ids.items(), \n",
    "                total=total_queries,\n",
    "                desc=\"Processing queries\")\n",
    "    \n",
    "    for qid, doc_ids in pbar:\n",
    "        # Get query text as anchor\n",
    "        anchor = self.query_id_to_text[qid]\n",
    "        \n",
    "        for doc_id in doc_ids:\n",
    "            # Get document text as positive example\n",
    "            positive = self.document_id_to_text[doc_id]\n",
    "            \n",
    "            # Update progress information\n",
    "            pbar.set_description(f\"Query {qid}: {len(doc_ids)} docs\")\n",
    "            \n",
    "            # Create training example with only anchor and positive\n",
    "            texts = [anchor, positive]\n",
    "            train_examples.append(InputExample(texts=texts))\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTraining examples preparation completed in {timedelta(seconds=int(elapsed_time))}\")\n",
    "    print(f\"Final number of training examples: {len(train_examples)}\")\n",
    "    \n",
    "    return train_examples\n",
    "\n",
    "# Replace the original method with our new implementation\n",
    "model_ap.prepare_training_examples = prepare_training_examples_ap.__get__(model_ap)\n",
    "\n",
    "# Fine-tune the model with [anchor, positive] strategy\n",
    "print(\"\\nFine-tuning with [anchor, positive] strategy...\")\n",
    "model_ap.fine_tune_model(batch_size=32, num_epochs=10, save_model_path=\"finetuned_senBERT_ap\")\n",
    "\n",
    "# Evaluate the model's performance after fine-tuning\n",
    "print(\"\\nEvaluating model after fine-tuning...\")\n",
    "model_ap.rank_documents()\n",
    "final_map_score = model_ap.mean_average_precision()\n",
    "\n",
    "# Compare all results\n",
    "print(\"\\nComparison of MAP scores:\")\n",
    "print(f\"1. Original model (before fine-tuning): {initial_map_score:.4f}\")\n",
    "print(f\"2. After fine-tuning with [anchor, positive]: {final_map_score:.4f}\")\n",
    "print(f\"3. Previous run with [anchor, positive, negative]: 0.4748\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model before fine-tuning...\n",
      "Initial MAP score (before fine-tuning): 0.477390368428073\n",
      "\n",
      "Fine-tuning with all layers trainable...\n",
      "Using device: cuda\n",
      "\n",
      "Preparing training examples...\n",
      "Total queries to process: 2590\n",
      "Expected total training examples: 110575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query PLAIN-3474: 83 docs: 100%|██████████| 2590/2590 [00:51<00:00, 50.21it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples preparation completed in 0:00:51\n",
      "Final number of training examples: 110575\n",
      "Number of training examples: 110575\n",
      "Total parameters: 22,713,216\n",
      "Trainable parameters: 22,713,216\n",
      "\n",
      "All layers are trainable in this experiment\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44a98a1b0334e64a9c2c0ea95e03000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34560' max='34560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34560/34560 1:07:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.575400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.913500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.825700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.743500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.674800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.561300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.547600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.480300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.432200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.387300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.339400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>2.344100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>2.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>2.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>2.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>2.260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.237900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>2.237900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>2.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.254600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>2.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>2.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>2.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>2.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>2.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>2.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>2.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>2.166100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 1:07:05\n",
      "Saving model to finetuned_senBERT_unfrozen\n",
      "Model saved successfully!\n",
      "\n",
      "Evaluating model after fine-tuning...\n",
      "\n",
      "Comparison of MAP scores:\n",
      "1. Original model (before fine-tuning): 0.4774\n",
      "2. After fine-tuning with all layers trainable: 0.4965\n",
      "3. Previous run with frozen layers except last: 0.4748\n"
     ]
    }
   ],
   "source": [
    "# Create a new model instance for unfrozen experiment\n",
    "model_unfrozen = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "\n",
    "# First, evaluate the model before fine-tuning\n",
    "print(\"Evaluating model before fine-tuning...\")\n",
    "model_unfrozen.rank_documents()\n",
    "initial_map_score = model_unfrozen.mean_average_precision()\n",
    "print(\"Initial MAP score (before fine-tuning):\", initial_map_score)\n",
    "\n",
    "# Define a modified fine_tune_model method without layer freezing\n",
    "def fine_tune_model_unfrozen(self, batch_size: int = 32, num_epochs: int = 10, save_model_path: str = \"finetuned_senBERT\"):\n",
    "    \"\"\"\n",
    "    Fine-tunes the model without freezing any layers.\n",
    "    All parameters will be trainable during fine-tuning.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    import time\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    # Check device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    self.model = self.model.to(device)\n",
    "    \n",
    "    # Prepare training examples\n",
    "    train_examples = self.prepare_training_examples()\n",
    "    print(f\"Number of training examples: {len(train_examples)}\")\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    # Define the loss function\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(self.model)\n",
    "    \n",
    "    # Print model parameters status\n",
    "    total_params = sum(p.numel() for p in self.model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # All parameters are trainable in this version\n",
    "    print(\"\\nAll layers are trainable in this experiment\")\n",
    "    \n",
    "    # Training loop with timing\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStarting training...\")\n",
    "    \n",
    "    # Fine-tune the model with warmup\n",
    "    warmup_steps = int(len(train_dataloader) * 0.1)  # 10% of training data for warmup\n",
    "    self.model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=num_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        show_progress_bar=True,\n",
    "        output_path=save_model_path,\n",
    "        checkpoint_path=f\"{save_model_path}_checkpoint\",\n",
    "        checkpoint_save_steps=len(train_dataloader),\n",
    "        callback=lambda score, epoch, steps: print(f\"\\nEpoch {epoch}: Score {score:.4f}\")\n",
    "    )\n",
    "    \n",
    "    # Calculate and print training time\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {str(timedelta(seconds=int(training_time)))}\")\n",
    "    \n",
    "    # Save the model\n",
    "    print(f\"Saving model to {save_model_path}\")\n",
    "    self.model.save(save_model_path)\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "# Replace the original method with our new implementation\n",
    "model_unfrozen.fine_tune_model = fine_tune_model_unfrozen.__get__(model_unfrozen)\n",
    "\n",
    "# Fine-tune the model without layer freezing\n",
    "print(\"\\nFine-tuning with all layers trainable...\")\n",
    "model_unfrozen.fine_tune_model(batch_size=32, num_epochs=10, save_model_path=\"finetuned_senBERT_unfrozen\")\n",
    "\n",
    "# Evaluate the model's performance after fine-tuning\n",
    "print(\"\\nEvaluating model after fine-tuning...\")\n",
    "model_unfrozen.rank_documents()\n",
    "final_map_score = model_unfrozen.mean_average_precision()\n",
    "\n",
    "# Compare all results\n",
    "print(\"\\nComparison of MAP scores:\")\n",
    "print(f\"1. Original model (before fine-tuning): {initial_map_score:.4f}\")\n",
    "print(f\"2. After fine-tuning with all layers trainable: {final_map_score:.4f}\")\n",
    "print(f\"3. Previous run with frozen layers except last: 0.4748\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment with ContrastiveLoss...\n",
      "Initial MAP score: 0.477390368428073\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb8cffadc7a4d729bcf26ecc7b4e1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69110' max='69110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69110/69110 1:14:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.016400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MAP score after ContrastiveLoss fine-tuning: 0.2926161634244341\n",
      "Experiment with TripletLoss...\n",
      "Initial MAP score: 0.477390368428073\n",
      "\n",
      "Preparing training examples...\n",
      "Total queries to process: 2590\n",
      "Expected total training examples: 110575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query PLAIN-3474: 83 docs: 100%|██████████| 2590/2590 [00:42<00:00, 61.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples preparation completed in 0:00:42\n",
      "Final number of training examples: 110575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bff665bfb34e68ae598af4a6e06eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34560' max='34560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34560/34560 1:05:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.897800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.816300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.773600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.726800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.630400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.517500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>4.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.486600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>4.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.466700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>4.473100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.458600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>4.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.448500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>4.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.438700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>4.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>4.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>4.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>4.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>4.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>4.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>4.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>4.405200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>4.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>4.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>4.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>4.386900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>4.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>4.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>4.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>4.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>4.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>4.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>4.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>4.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>4.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>4.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>4.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>4.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>4.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>4.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>4.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>4.367500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>4.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>4.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>4.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>4.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>4.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>4.357600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>4.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>4.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>4.367000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MAP score after TripletLoss fine-tuning: 0.14230630848830758\n",
      "\n",
      "Loss Function Comparison Results:\n",
      "1. MultipleNegativesRankingLoss (previous run): 0.4774 -> 0.4748\n",
      "2. ContrastiveLoss: 0.4774 -> 0.2926\n",
      "3. TripletLoss: 0.4774 -> 0.1423\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Experiment with ContrastiveLoss\n",
    "print(\"\\nExperiment with ContrastiveLoss...\")\n",
    "model_contrastive = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "# Evaluate the model before fine-tuning\n",
    "model_contrastive.rank_documents()\n",
    "initial_map_contrastive = model_contrastive.mean_average_precision()\n",
    "print(\"Initial MAP score:\", initial_map_contrastive)\n",
    "\n",
    "# Prepare training data\n",
    "def prepare_contrastive_examples(model):\n",
    "    \"\"\"Prepare training samples for ContrastiveLoss\"\"\"\n",
    "    import random\n",
    "    train_examples = []\n",
    "    for qid, doc_ids in model.train_query_id_to_relevant_doc_ids.items():\n",
    "        query = model.query_id_to_text[qid]\n",
    "        for doc_id in doc_ids:\n",
    "            # Positive pair\n",
    "            positive = model.document_id_to_text[doc_id]\n",
    "            train_examples.append(InputExample(texts=[query, positive], label=1.0))\n",
    "            \n",
    "            # Negative pair (randomly select an unrelated document)\n",
    "            negative_candidates = list(set(model.document_ids) - set(doc_ids))\n",
    "            if negative_candidates:\n",
    "                negative = model.document_id_to_text[random.choice(negative_candidates)]\n",
    "                train_examples.append(InputExample(texts=[query, negative], label=0.0))\n",
    "    \n",
    "    return train_examples\n",
    "\n",
    "training_examples_contrastive = prepare_contrastive_examples(model_contrastive)\n",
    "train_dataloader_contrastive = DataLoader(training_examples_contrastive, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create ContrastiveLoss instance\n",
    "contrastive_loss = losses.ContrastiveLoss(model=model_contrastive.model)\n",
    "\n",
    "# Use model.fit directly for fine-tuning\n",
    "model_contrastive.model.fit(\n",
    "    train_objectives=[(train_dataloader_contrastive, contrastive_loss)],\n",
    "    epochs=10,\n",
    "    output_path=\"finetuned_senBERT_contrastive\"\n",
    ")\n",
    "# Evaluate the model after fine-tuning\n",
    "model_contrastive.rank_documents()\n",
    "final_map_contrastive = model_contrastive.mean_average_precision()\n",
    "print(\"Final MAP score after ContrastiveLoss fine-tuning:\", final_map_contrastive)\n",
    "\n",
    "# Experiment with TripletLoss\n",
    "print(\"Experiment with TripletLoss...\")\n",
    "model_triplet = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "# Evaluate the model before fine-tuning\n",
    "model_triplet.rank_documents()\n",
    "initial_map_triplet = model_triplet.mean_average_precision()\n",
    "print(\"Initial MAP score:\", initial_map_triplet)\n",
    "\n",
    "# Prepare training data\n",
    "training_examples_triplet = model_triplet.prepare_training_examples()  \n",
    "train_dataloader_triplet = DataLoader(training_examples_triplet, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create TripletLoss instance\n",
    "triplet_loss = losses.TripletLoss(model=model_triplet.model)\n",
    "\n",
    "model_triplet.model.fit(\n",
    "    train_objectives=[(train_dataloader_triplet, triplet_loss)],\n",
    "    epochs=10,\n",
    "    output_path=\"finetuned_senBERT_triplet\"\n",
    ")\n",
    "# Evaluate the model after fine-tuning\n",
    "model_triplet.rank_documents()\n",
    "final_map_triplet = model_triplet.mean_average_precision()\n",
    "print(\"Final MAP score after TripletLoss fine-tuning:\", final_map_triplet)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Compare all results\n",
    "print(\"\\nLoss Function Comparison Results:\")\n",
    "print(\"1. MultipleNegativesRankingLoss (previous run): 0.4774 -> 0.4748\")\n",
    "print(f\"2. ContrastiveLoss: {initial_map_contrastive:.4f} -> {final_map_contrastive:.4f}\")\n",
    "print(f\"3. TripletLoss: {initial_map_triplet:.4f} -> {final_map_triplet:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with different training durations...\n",
      "\n",
      "Training for 5 epochs...\n",
      "Initial MAP score: 0.4774\n",
      "Using device: cuda\n",
      "\n",
      "Preparing training examples...\n",
      "Total queries to process: 2590\n",
      "Expected total training examples: 110575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query PLAIN-3474: 83 docs: 100%|██████████| 2590/2590 [00:52<00:00, 49.35it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples preparation completed in 0:00:52\n",
      "Final number of training examples: 110575\n",
      "Number of training examples: 110575\n",
      "Total parameters: 22,713,216\n",
      "Trainable parameters: 22,713,216\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Trainable parameters:\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b084b6690e49453393086aa9392362d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17280' max='17280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17280/17280 16:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.571300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.572100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.519700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.464100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.440100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.391600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.388700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.365900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.338500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.335700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.284600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.283400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.291300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 0:16:50\n",
      "Saving model to finetuned_senBERT_5epochs\n",
      "Model saved successfully!\n",
      "Final MAP score after 5 epochs: 0.4767\n",
      "\n",
      "Training for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since BeIR/nfcorpus-qrels couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/sky/.cache/huggingface/datasets/BeIR___nfcorpus-qrels/default/0.0.0/a451b3b26d3ae1358f259c1a3a4dd61fcea35a65 (last modified on Sat Feb  8 18:06:32 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial MAP score: 0.4774\n",
      "Using device: cuda\n",
      "\n",
      "Preparing training examples...\n",
      "Total queries to process: 2590\n",
      "Expected total training examples: 110575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query PLAIN-3474: 83 docs: 100%|██████████| 2590/2590 [00:46<00:00, 55.34it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples preparation completed in 0:00:46\n",
      "Final number of training examples: 110575\n",
      "Number of training examples: 110575\n",
      "Total parameters: 22,713,216\n",
      "Trainable parameters: 22,713,216\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Trainable parameters:\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ff3fc93d09437f94e5c088e597855e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34560' max='34560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34560/34560 31:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.723800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.606300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.533800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.464700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.355800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.309900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.283400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.175700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.066400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 0:31:56\n",
      "Saving model to finetuned_senBERT_10epochs\n",
      "Model saved successfully!\n",
      "Final MAP score after 10 epochs: 0.4769\n",
      "\n",
      "Training for 20 epochs...\n",
      "Initial MAP score: 0.4774\n",
      "Using device: cuda\n",
      "\n",
      "Preparing training examples...\n",
      "Total queries to process: 2590\n",
      "Expected total training examples: 110575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query PLAIN-3474: 83 docs: 100%|██████████| 2590/2590 [00:42<00:00, 61.51it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples preparation completed in 0:00:42\n",
      "Final number of training examples: 110575\n",
      "Number of training examples: 110575\n",
      "Total parameters: 22,713,216\n",
      "Trainable parameters: 22,713,216\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Trainable parameters:\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcb596875e4481599456bfaa46e5f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69120' max='69120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69120/69120 1:03:36, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.654600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.623100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.606300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.043900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>2.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>2.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>2.977700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.969600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>2.959800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.951400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>2.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.946500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>2.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>2.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>2.936300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>2.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>2.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>2.944500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>2.934100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>2.928900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>2.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>2.888500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>2.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.914900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>2.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>2.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>2.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>2.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>2.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>2.884300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>2.882700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>2.894600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>2.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>2.882800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>2.849600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>2.854600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>2.889400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>2.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>2.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>2.854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>2.836600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>2.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>2.847900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>2.846700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>2.844300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>2.852600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>2.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>2.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>2.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>2.818300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>2.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>2.827200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>2.837900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>2.828400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>2.819000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>2.809200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>2.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>2.813500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>2.809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>2.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>2.817700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>2.780500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>2.813700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>2.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>2.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>2.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>2.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>2.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>2.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>2.811900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>2.802100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>2.793700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>2.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>2.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>2.766300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>2.780200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>2.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>2.798100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>2.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>2.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>2.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>2.793000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 1:03:39\n",
      "Saving model to finetuned_senBERT_20epochs\n",
      "Model saved successfully!\n",
      "Final MAP score after 20 epochs: 0.4737\n",
      "\n",
      "Training Duration Comparison Results:\n",
      "Epochs 5: 0.4774 -> 0.4767\n",
      "Epochs 10: 0.4774 -> 0.4769\n",
      "Epochs 20: 0.4774 -> 0.4737\n"
     ]
    }
   ],
   "source": [
    "# Comparison of different training durations\n",
    "print(\"Experiment with different training durations...\")\n",
    "\n",
    "epochs_to_try = [5, 10, 20]\n",
    "duration_results = {}\n",
    "\n",
    "for epochs in epochs_to_try:\n",
    "    print(f\"\\nTraining for {epochs} epochs...\")\n",
    "    model = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "    \n",
    "    # Record initial performance\n",
    "    model.rank_documents()\n",
    "    initial_map = model.mean_average_precision()\n",
    "    print(f\"Initial MAP score: {initial_map:.4f}\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fine_tune_model(batch_size=32, \n",
    "                         num_epochs=epochs, \n",
    "                         save_model_path=f\"finetuned_senBERT_{epochs}epochs\")\n",
    "    \n",
    "    # Evaluate after training\n",
    "    model.rank_documents()\n",
    "    final_map = model.mean_average_precision()\n",
    "    duration_results[epochs] = {\n",
    "        'initial_map': initial_map,\n",
    "        'final_map': final_map,\n",
    "    }\n",
    "    print(f\"Final MAP score after {epochs} epochs: {final_map:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nTraining Duration Comparison Results:\")\n",
    "for epochs, scores in duration_results.items():\n",
    "    print(f\"Epochs {epochs}: {scores['initial_map']:.4f} -> {scores['final_map']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with different numbers of negative samples...\n",
      "\n",
      "Training with 1 negative samples per positive pair...\n",
      "Initial MAP score: 0.4774\n",
      "Using device: cuda\n",
      "\n",
      "Preparing training examples...\n",
      "Total queries to process: 2590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query PLAIN-3474: 83 docs: 100%|██████████| 2590/2590 [00:42<00:00, 60.38it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples preparation completed in 0:00:42\n",
      "Final number of training examples: 110575\n",
      "Number of training examples: 110575\n",
      "Total parameters: 22,713,216\n",
      "Trainable parameters: 22,713,216\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Trainable parameters:\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd234eae1d447f8a271f60728768573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34560' max='34560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34560/34560 31:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.606300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.283700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.245600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.094400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.098500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.103200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.084500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 0:32:03\n",
      "Saving model to finetuned_senBERT_1neg\n",
      "Model saved successfully!\n",
      "Final MAP score with 1 negatives: 0.4743\n",
      "\n",
      "Training with 3 negative samples per positive pair...\n",
      "Initial MAP score: 0.4774\n",
      "Using device: cuda\n",
      "\n",
      "Preparing training examples...\n",
      "Total queries to process: 2590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query PLAIN-3474: 83 docs: 100%|██████████| 2590/2590 [00:43<00:00, 59.67it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples preparation completed in 0:00:43\n",
      "Final number of training examples: 110575\n",
      "Number of training examples: 110575\n",
      "Total parameters: 22,713,216\n",
      "Trainable parameters: 22,713,216\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Trainable parameters:\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8da4dd00dd4da2810be82f4be65478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34560' max='34560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34560/34560 31:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.709300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.453200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.427300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.363300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.335100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.280200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.063800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 0:32:02\n",
      "Saving model to finetuned_senBERT_3neg\n",
      "Model saved successfully!\n",
      "Final MAP score with 3 negatives: 0.4690\n",
      "\n",
      "Training with 5 negative samples per positive pair...\n",
      "Initial MAP score: 0.4774\n",
      "Using device: cuda\n",
      "\n",
      "Preparing training examples...\n",
      "Total queries to process: 2590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query PLAIN-3474: 83 docs: 100%|██████████| 2590/2590 [00:43<00:00, 59.64it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples preparation completed in 0:00:43\n",
      "Final number of training examples: 110575\n",
      "Number of training examples: 110575\n",
      "Total parameters: 22,713,216\n",
      "Trainable parameters: 22,713,216\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "Unfreezing: 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Trainable parameters:\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.dense.weight\n",
      "- 0.auto_model.encoder.layer.5.output.dense.bias\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "- 0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420193aa9a3e40e7bb7c6f0703fc831b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34560' max='34560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34560/34560 31:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.709300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.453200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.427300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.363300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.335100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.280200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.063800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 0:32:01\n",
      "Saving model to finetuned_senBERT_5neg\n",
      "Model saved successfully!\n",
      "Final MAP score with 5 negatives: 0.4690\n",
      "\n",
      "Negative Samples Comparison Results:\n",
      "1 negatives: 0.4774 -> 0.4743\n",
      "3 negatives: 0.4774 -> 0.4690\n",
      "5 negatives: 0.4774 -> 0.4690\n"
     ]
    }
   ],
   "source": [
    "# Comparison of different numbers of negative samples\n",
    "print(\"Experiment with different numbers of negative samples...\")\n",
    "\n",
    "# Modify prepare_training_examples to support multiple negatives\n",
    "def prepare_training_examples_with_n_negatives(self, n_negatives=1):\n",
    "    \"\"\"\n",
    "    Prepares training examples with specified number of negative samples per positive pair\n",
    "    \"\"\"\n",
    "    train_examples = []\n",
    "    import random\n",
    "    from datetime import timedelta\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(\"\\nPreparing training examples...\")\n",
    "    total_queries = len(self.train_query_id_to_relevant_doc_ids)\n",
    "    print(f\"Total queries to process: {total_queries}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(self.train_query_id_to_relevant_doc_ids.items(), \n",
    "                total=total_queries,\n",
    "                desc=\"Processing queries\")\n",
    "    \n",
    "    for qid, doc_ids in pbar:\n",
    "        anchor = self.query_id_to_text[qid]\n",
    "        relevant_set = set(doc_ids)\n",
    "        negative_candidates = list(set(self.document_ids) - relevant_set)\n",
    "        \n",
    "        for pos_id in doc_ids:\n",
    "            positive = self.document_id_to_text[pos_id]\n",
    "            \n",
    "            # Sample n negative documents\n",
    "            if negative_candidates:\n",
    "                negatives = random.sample(negative_candidates, \n",
    "                                       min(n_negatives, len(negative_candidates)))\n",
    "                texts = [anchor, positive] + [self.document_id_to_text[neg] for neg in negatives]\n",
    "            else:\n",
    "                texts = [anchor, positive]\n",
    "            \n",
    "            train_examples.append(InputExample(texts=texts))\n",
    "            \n",
    "            pbar.set_description(f\"Query {qid}: {len(doc_ids)} docs\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTraining examples preparation completed in {timedelta(seconds=int(elapsed_time))}\")\n",
    "    print(f\"Final number of training examples: {len(train_examples)}\")\n",
    "    \n",
    "    return train_examples\n",
    "\n",
    "# Test different numbers of negative samples\n",
    "n_negatives_to_try = [1, 3, 5]\n",
    "negative_results = {}\n",
    "\n",
    "for n_neg in n_negatives_to_try:\n",
    "    print(f\"\\nTraining with {n_neg} negative samples per positive pair...\")\n",
    "    model = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "    \n",
    "    # Record initial performance\n",
    "    model.rank_documents()\n",
    "    initial_map = model.mean_average_precision()\n",
    "    print(f\"Initial MAP score: {initial_map:.4f}\")\n",
    "    \n",
    "    # Replace prepare_training_examples method\n",
    "    model.prepare_training_examples = prepare_training_examples_with_n_negatives.__get__(model)\n",
    "    model.n_negatives = n_neg\n",
    "    \n",
    "    # Train the model\n",
    "    model.fine_tune_model(batch_size=32, \n",
    "                         num_epochs=10, \n",
    "                         save_model_path=f\"finetuned_senBERT_{n_neg}neg\")\n",
    "    \n",
    "    # Evaluate after training\n",
    "    model.rank_documents()\n",
    "    final_map = model.mean_average_precision()\n",
    "    negative_results[n_neg] = {\n",
    "        'initial_map': initial_map,\n",
    "        'final_map': final_map,\n",
    "    }\n",
    "    print(f\"Final MAP score with {n_neg} negatives: {final_map:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nNegative Samples Comparison Results:\")\n",
    "for n_neg, scores in negative_results.items():\n",
    "    print(f\"{n_neg} negatives: {scores['initial_map']:.4f} -> {scores['final_map']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 documents for the query: \"Do Cholesterol Statin Drugs Cause Breast Cancer?\"\n",
      "Rank 1: Document ID: MED-2429, Similarity Score: 0.7492\n",
      "Rank 2: Document ID: MED-10, Similarity Score: 0.7311\n",
      "Rank 3: Document ID: MED-2431, Similarity Score: 0.7285\n",
      "Rank 4: Document ID: MED-14, Similarity Score: 0.7175\n",
      "Rank 5: Document ID: MED-2428, Similarity Score: 0.6996\n",
      "Rank 6: Document ID: MED-1193, Similarity Score: 0.6248\n",
      "Rank 7: Document ID: MED-1429, Similarity Score: 0.6172\n",
      "Rank 8: Document ID: MED-4827, Similarity Score: 0.6069\n",
      "Rank 9: Document ID: MED-1486, Similarity Score: 0.6046\n",
      "Rank 10: Document ID: MED-2525, Similarity Score: 0.5948\n",
      "\n",
      "Ranking Analysis:\n",
      "Query: Do Cholesterol Statin Drugs Cause Breast Cancer?\n",
      "Number of truly relevant documents: 24\n",
      "\n",
      "Relevant documents in top 10:\n",
      "Rank 1: MED-2429 - Relevant\n",
      "Rank 2: MED-10 - Relevant\n",
      "Rank 3: MED-2431 - Relevant\n",
      "Rank 4: MED-14 - Relevant\n",
      "Rank 5: MED-2428 - Relevant\n",
      "Rank 6: MED-1193 - Not Relevant\n",
      "Rank 7: MED-1429 - Not Relevant\n",
      "Rank 8: MED-1486 - Not Relevant\n",
      "Rank 9: MED-2525 - Not Relevant\n",
      "Rank 10: MED-2440 - Relevant\n",
      "\n",
      "Relevant documents not in top 10: 18\n",
      "Examples: ['MED-2427', 'MED-2430', 'MED-2432']\n"
     ]
    }
   ],
   "source": [
    "def analyze_ranking_quality(self, query_example=None):\n",
    "    \"\"\"Analyze ranking quality by comparing model rankings with true relevance\"\"\"\n",
    "    # If no query is provided, use the first query from the test set\n",
    "    if query_example is None:\n",
    "        query_id = self.test_query_ids[0]\n",
    "        query_example = self.query_id_to_text[query_id]\n",
    "    else:\n",
    "        # Find the query in the test set\n",
    "        query_id = None\n",
    "        for qid in self.test_query_ids:\n",
    "            if self.query_id_to_text[qid] == query_example:\n",
    "                query_id = qid\n",
    "                break\n",
    "        \n",
    "        if query_id is None:\n",
    "            # If the query is not found in the test set, use the first query from the test set\n",
    "            query_id = self.test_query_ids[0]\n",
    "            query_example = self.query_id_to_text[query_id]\n",
    "            print(f\"Query not found in test set. Using first test query instead: {query_example}\")\n",
    "    \n",
    "    # Get relevant documents and analyze ranking\n",
    "    relevant_docs = self.test_query_id_to_relevant_doc_ids.get(query_id, [])\n",
    "    self.show_ranking_documents(query_example)\n",
    "    ranked_docs = self.query_id_to_ranked_doc_ids[query_id]\n",
    "    \n",
    "    print(\"\\nRanking Analysis:\")\n",
    "    print(f\"Query: {query_example}\")\n",
    "    print(f\"Number of truly relevant documents: {len(relevant_docs)}\")\n",
    "    print(\"\\nRelevant documents in top 10:\")\n",
    "    for i, doc_id in enumerate(ranked_docs[:10]):\n",
    "        is_relevant = doc_id in relevant_docs\n",
    "        print(f\"Rank {i+1}: {doc_id} - {'Relevant' if is_relevant else 'Not Relevant'}\")\n",
    "        \n",
    "    missed_relevant = [doc for doc in relevant_docs if doc not in ranked_docs[:10]]\n",
    "    print(f\"\\nRelevant documents not in top 10: {len(missed_relevant)}\")\n",
    "    if missed_relevant:\n",
    "        print(\"Examples:\", missed_relevant[:3])\n",
    "    \n",
    "analyze_ranking_quality(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Do Cholesterol Statin Drugs Cause Breast Cancer?\n",
      "Number of truly relevant documents: 24\n",
      "\n",
      "Top 10 Documents Comparison:\n",
      "\n",
      "Cosine Similarity:\n",
      "Rank 1: MED-2429 (score: 0.7492) - Relevant\n",
      "Rank 2: MED-10 (score: 0.7311) - Relevant\n",
      "Rank 3: MED-2431 (score: 0.7285) - Relevant\n",
      "Rank 4: MED-14 (score: 0.7175) - Relevant\n",
      "Rank 5: MED-2428 (score: 0.6996) - Relevant\n",
      "Rank 6: MED-1193 (score: 0.6248) - Not Relevant\n",
      "Rank 7: MED-1429 (score: 0.6172) - Not Relevant\n",
      "Rank 8: MED-4827 (score: 0.6069) - Not Relevant\n",
      "Rank 9: MED-1486 (score: 0.6046) - Not Relevant\n",
      "Rank 10: MED-2525 (score: 0.5948) - Not Relevant\n",
      "Relevant documents in top 10: 5/10\n",
      "\n",
      "Euclidean Distance:\n",
      "Rank 1: MED-2429 (distance: 0.7083) - Relevant\n",
      "Rank 2: MED-10 (distance: 0.7333) - Relevant\n",
      "Rank 3: MED-2431 (distance: 0.7369) - Relevant\n",
      "Rank 4: MED-14 (distance: 0.7517) - Relevant\n",
      "Rank 5: MED-2428 (distance: 0.7751) - Relevant\n",
      "Rank 6: MED-1193 (distance: 0.8662) - Not Relevant\n",
      "Rank 7: MED-1429 (distance: 0.8750) - Not Relevant\n",
      "Rank 8: MED-4827 (distance: 0.8866) - Not Relevant\n",
      "Rank 9: MED-1486 (distance: 0.8893) - Not Relevant\n",
      "Rank 10: MED-2525 (distance: 0.9002) - Not Relevant\n",
      "Relevant documents in top 10: 5/10\n",
      "\n",
      "Manhattan Distance:\n",
      "Rank 1: MED-2429 (distance: 11.1150) - Relevant\n",
      "Rank 2: MED-2431 (distance: 11.4851) - Relevant\n",
      "Rank 3: MED-10 (distance: 11.5920) - Relevant\n",
      "Rank 4: MED-14 (distance: 11.9211) - Relevant\n",
      "Rank 5: MED-2428 (distance: 11.9382) - Relevant\n",
      "Rank 6: MED-1193 (distance: 13.2146) - Not Relevant\n",
      "Rank 7: MED-1429 (distance: 13.4823) - Not Relevant\n",
      "Rank 8: MED-4559 (distance: 13.6319) - Relevant\n",
      "Rank 9: MED-1486 (distance: 13.6785) - Not Relevant\n",
      "Rank 10: MED-4827 (distance: 13.9932) - Not Relevant\n",
      "Relevant documents in top 10: 6/10\n"
     ]
    }
   ],
   "source": [
    "def compare_distance_metrics(self, query_example=None):\n",
    "    \"\"\"Compare the effectiveness of different distance metrics\"\"\"\n",
    "    from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances\n",
    "    \n",
    "    # If no query is provided, use the first query from the test set\n",
    "    if query_example is None:\n",
    "        query_id = self.test_query_ids[0]\n",
    "        query_example = self.query_id_to_text[query_id]\n",
    "    else:\n",
    "        # Find the query in the test set\n",
    "        query_id = None\n",
    "        for qid in self.test_query_ids:\n",
    "            if self.query_id_to_text[qid] == query_example:\n",
    "                query_id = qid\n",
    "                break\n",
    "        \n",
    "        if query_id is None:\n",
    "            query_id = self.test_query_ids[0]\n",
    "            query_example = self.query_id_to_text[query_id]\n",
    "            print(f\"Query not found in test set. Using first test query instead: {query_example}\")\n",
    "    \n",
    "    # Get ground truth relevant documents\n",
    "    relevant_docs = self.test_query_id_to_relevant_doc_ids.get(query_id, [])\n",
    "    print(f\"Query: {query_example}\")\n",
    "    print(f\"Number of truly relevant documents: {len(relevant_docs)}\\n\")\n",
    "    \n",
    "    # Encode the query and documents\n",
    "    query_embedding = self.model.encode([query_example])\n",
    "    doc_embeddings = self.model.encode(self.documents)\n",
    "    \n",
    "    # Calculate different distance metrics\n",
    "    cosine_sim = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "    euclidean_dist = euclidean_distances(query_embedding, doc_embeddings)[0]\n",
    "    manhattan_dist = manhattan_distances(query_embedding, doc_embeddings)[0]\n",
    "    \n",
    "    # Get the top 10 documents for each metric\n",
    "    top_10_cosine = np.argsort(cosine_sim)[::-1][:10]\n",
    "    top_10_euclidean = np.argsort(euclidean_dist)[:10]\n",
    "    top_10_manhattan = np.argsort(manhattan_dist)[:10]\n",
    "    \n",
    "    print(\"Top 10 Documents Comparison:\")\n",
    "    \n",
    "    def print_ranking_with_relevance(name, scores, indices, reverse=True):\n",
    "        print(f\"\\n{name}:\")\n",
    "        relevant_count = 0\n",
    "        for i, idx in enumerate(indices):\n",
    "            doc_id = self.document_ids[idx]\n",
    "            is_relevant = doc_id in relevant_docs\n",
    "            if is_relevant:\n",
    "                relevant_count += 1\n",
    "            score_str = f\"score: {scores[idx]:.4f}\" if reverse else f\"distance: {scores[idx]:.4f}\"\n",
    "            relevance_str = \"Relevant\" if is_relevant else \"Not Relevant\"\n",
    "            print(f\"Rank {i+1}: {doc_id} ({score_str}) - {relevance_str}\")\n",
    "        print(f\"Relevant documents in top 10: {relevant_count}/10\")\n",
    "    \n",
    "    print_ranking_with_relevance(\"Cosine Similarity\", cosine_sim, top_10_cosine)\n",
    "    print_ranking_with_relevance(\"Euclidean Distance\", euclidean_dist, top_10_euclidean, reverse=False)\n",
    "    print_ranking_with_relevance(\"Manhattan Distance\", manhattan_dist, top_10_manhattan, reverse=False)\n",
    "\n",
    "compare_distance_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Do Cholesterol Statin Drugs Cause Breast Cancer?\n",
      "Number of truly relevant documents: 24\n",
      "\n",
      "\n",
      "Original Query Results:\n",
      "Query used: Do Cholesterol Statin Drugs Cause Breast Cancer?\n",
      "Top 10 documents for the query: \"Do Cholesterol Statin Drugs Cause Breast Cancer?\"\n",
      "Rank 1: Document ID: MED-2429, Similarity Score: 0.7492\n",
      "Rank 2: Document ID: MED-10, Similarity Score: 0.7311\n",
      "Rank 3: Document ID: MED-2431, Similarity Score: 0.7285\n",
      "Rank 4: Document ID: MED-14, Similarity Score: 0.7175\n",
      "Rank 5: Document ID: MED-2428, Similarity Score: 0.6996\n",
      "Rank 6: Document ID: MED-1193, Similarity Score: 0.6248\n",
      "Rank 7: Document ID: MED-1429, Similarity Score: 0.6172\n",
      "Rank 8: Document ID: MED-4827, Similarity Score: 0.6069\n",
      "Rank 9: Document ID: MED-1486, Similarity Score: 0.6046\n",
      "Rank 10: Document ID: MED-2525, Similarity Score: 0.5948\n",
      "\n",
      "Relevance Analysis:\n",
      "Rank 1: MED-2429 - Relevant\n",
      "Rank 2: MED-10 - Relevant\n",
      "Rank 3: MED-2431 - Relevant\n",
      "Rank 4: MED-14 - Relevant\n",
      "Rank 5: MED-2428 - Relevant\n",
      "Rank 6: MED-1193 - Not Relevant\n",
      "Rank 7: MED-1429 - Not Relevant\n",
      "Rank 8: MED-1486 - Not Relevant\n",
      "Rank 9: MED-2525 - Not Relevant\n",
      "Rank 10: MED-2440 - Relevant\n",
      "Relevant documents in top 10: 6/10\n",
      "Relevant documents not in top 10: 18\n",
      "Examples: ['MED-2427', 'MED-2430', 'MED-2432']\n",
      "\n",
      "Preprocessed Query Results:\n",
      "Query used: do cholesterol statin drugs cause breast cancer\n",
      "Top 10 documents for the query: \"do cholesterol statin drugs cause breast cancer\"\n",
      "Rank 1: Document ID: MED-2429, Similarity Score: 0.7977\n",
      "Rank 2: Document ID: MED-10, Similarity Score: 0.7798\n",
      "Rank 3: Document ID: MED-2431, Similarity Score: 0.7770\n",
      "Rank 4: Document ID: MED-14, Similarity Score: 0.7650\n",
      "Rank 5: Document ID: MED-2428, Similarity Score: 0.7352\n",
      "Rank 6: Document ID: MED-1193, Similarity Score: 0.6806\n",
      "Rank 7: Document ID: MED-4559, Similarity Score: 0.6549\n",
      "Rank 8: Document ID: MED-1429, Similarity Score: 0.6548\n",
      "Rank 9: Document ID: MED-4827, Similarity Score: 0.6409\n",
      "Rank 10: Document ID: MED-1887, Similarity Score: 0.6238\n",
      "\n",
      "Relevance Analysis:\n",
      "Rank 1: MED-2429 - Relevant\n",
      "Rank 2: MED-10 - Relevant\n",
      "Rank 3: MED-2431 - Relevant\n",
      "Rank 4: MED-14 - Relevant\n",
      "Rank 5: MED-2428 - Relevant\n",
      "Rank 6: MED-1193 - Not Relevant\n",
      "Rank 7: MED-1429 - Not Relevant\n",
      "Rank 8: MED-1486 - Not Relevant\n",
      "Rank 9: MED-2525 - Not Relevant\n",
      "Rank 10: MED-2440 - Relevant\n",
      "Relevant documents in top 10: 6/10\n",
      "Relevant documents not in top 10: 18\n",
      "Examples: ['MED-2427', 'MED-2430', 'MED-2432']\n",
      "\n",
      "Preprocessing Impact Summary:\n",
      "Original query: Do Cholesterol Statin Drugs Cause Breast Cancer?\n",
      "Preprocessed query: do cholesterol statin drugs cause breast cancer\n"
     ]
    }
   ],
   "source": [
    "def analyze_preprocessing_impact(self, query_example=None):\n",
    "    \"\"\"Analyze the impact of preprocessing on ranking results\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # If no query is provided, use the first query from the test set\n",
    "    if query_example is None:\n",
    "        query_id = self.test_query_ids[0]\n",
    "        query_example = self.query_id_to_text[query_id]\n",
    "    else:\n",
    "        # Find the query in the test set\n",
    "        query_id = None\n",
    "        for qid in self.test_query_ids:\n",
    "            if self.query_id_to_text[qid] == query_example:\n",
    "                query_id = qid\n",
    "                break\n",
    "        \n",
    "        if query_id is None:\n",
    "            query_id = self.test_query_ids[0]\n",
    "            query_example = self.query_id_to_text[query_id]\n",
    "            print(f\"Query not found in test set. Using first test query instead: {query_example}\")\n",
    "    \n",
    "    # Get ground truth relevant documents\n",
    "    relevant_docs = self.test_query_id_to_relevant_doc_ids.get(query_id, [])\n",
    "    print(f\"Query: {query_example}\")\n",
    "    print(f\"Number of truly relevant documents: {len(relevant_docs)}\\n\")\n",
    "    \n",
    "    def preprocess_text(text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove common stop words\n",
    "        stop_words = {'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', \n",
    "                     'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', \n",
    "                     'to', 'was', 'were', 'will', 'with'}\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def print_ranking_with_relevance(title, query):\n",
    "        print(f\"\\n{title}:\")\n",
    "        print(f\"Query used: {query}\")\n",
    "        self.show_ranking_documents(query)\n",
    "        \n",
    "        # Get ranked documents and analyze relevance\n",
    "        ranked_docs = self.query_id_to_ranked_doc_ids[query_id][:10]\n",
    "        relevant_count = sum(1 for doc_id in ranked_docs if doc_id in relevant_docs)\n",
    "        \n",
    "        print(\"\\nRelevance Analysis:\")\n",
    "        for i, doc_id in enumerate(ranked_docs):\n",
    "            is_relevant = doc_id in relevant_docs\n",
    "            print(f\"Rank {i+1}: {doc_id} - {'Relevant' if is_relevant else 'Not Relevant'}\")\n",
    "        print(f\"Relevant documents in top 10: {relevant_count}/10\")\n",
    "        \n",
    "        # Show missed relevant documents\n",
    "        missed_relevant = [doc for doc in relevant_docs if doc not in ranked_docs]\n",
    "        print(f\"Relevant documents not in top 10: {len(missed_relevant)}\")\n",
    "        if missed_relevant:\n",
    "            print(\"Examples:\", missed_relevant[:3])\n",
    "    \n",
    "    # Original query results\n",
    "    print_ranking_with_relevance(\"Original Query Results\", query_example)\n",
    "    \n",
    "    # Preprocessed query results\n",
    "    processed_query = preprocess_text(query_example)\n",
    "    print_ranking_with_relevance(\"Preprocessed Query Results\", processed_query)\n",
    "    \n",
    "    # Compare the rankings\n",
    "    print(\"\\nPreprocessing Impact Summary:\")\n",
    "    print(\"Original query:\", query_example)\n",
    "    print(\"Preprocessed query:\", processed_query)\n",
    "\n",
    "analyze_preprocessing_impact(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm596",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
