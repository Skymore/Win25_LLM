{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment aims to introduce students to working with the BEIR dataset for information retrieval tasks. Students will:\n",
    "\n",
    "- Understand the structure of the BEIR dataset and preprocess the data.\n",
    "- Implement a system to encode queries and documents using embeddings.\n",
    "- Calculate similarity scores to rank documents based on relevance.\n",
    "- Evaluate the system's performance using metrics like Mean Average Precision (MAP).\n",
    "- Modify and fine-tune models for better retrieval results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First start by looking into the Dataset and understanding its structure.\n",
    "This will help you understand how the dataset is formed, which will be useful in the later stages of the Assignment\n",
    "\n",
    "https://huggingface.co/datasets/BeIR/nfcorpus\n",
    "\n",
    "https://huggingface.co/datasets/BeIR/nfcorpus-qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assignment consists of two key tasks: Ranking Documents and Fine-Tuning the Sentence Transformer Model. \n",
    "# Students will be graded based on their implementation and their written report.\n",
    "\n",
    "# Mention the team/Individual contributions as a part of the report..!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking Documents Report (10 Points)\n",
    "\n",
    "# Students must analyze which encoding methods performed best for document ranking.\n",
    "\n",
    "# What to include in your report:\n",
    "    \n",
    "# Comparison of Encoding Methods \n",
    "\n",
    "    # Compare GloVe embeddings vs. Sentence Transformer embeddings.\n",
    "    # Which method ranked documents better?\n",
    "    # Did the top-ranked documents make sense?\n",
    "    # How does cosine similarity behave with different embeddings?\n",
    "\n",
    "# Observations on Cosine Similarity & Ranking \n",
    "\n",
    "    # Did the ranking appear meaningful?\n",
    "    # Were there cases where documents that should be highly ranked were not?\n",
    "    # What are possible explanations for incorrect rankings?\n",
    "\n",
    "# Possible Improvements\n",
    "\n",
    "    # What can be done to improve document ranking?\n",
    "    # Would a different distance metric (e.g., Euclidean, Manhattan) help?\n",
    "    # Would preprocessing the queries or documents (e.g., removing stopwords) improve ranking?\n",
    "\n",
    "\n",
    "# Fine-Tuning Report (15 Points)\n",
    "\n",
    "# After fine-tuning, students must compare different training approaches and reflect on their findings.\n",
    "\n",
    "# What to include in your report:\n",
    "    \n",
    "# Comparison of Different Training Strategies \n",
    "\n",
    "    # [anchor, positive] vs [anchor, positive, negative].\n",
    "    # Which approach seemed to improve ranking?\n",
    "    # How did the model behave differently?\n",
    "\n",
    "# Impact on MAP Score \n",
    "\n",
    "    # Did fine-tuning improve or hurt the Mean Average Precision (MAP) score?\n",
    "    # If MAP decreased, why might that be?\n",
    "    # Is fine-tuning always necessary for retrieval models?\n",
    "\n",
    "# Observations on Training Loss & Learning Rate \n",
    "\n",
    "    # Did the loss converge?\n",
    "    # Was the learning rate too high or too low?\n",
    "    # How did freezing/unfreezing layers impact training?\n",
    "\n",
    "# Future Improvements \n",
    "\n",
    "    # Would training with more negatives help?\n",
    "    # Would changing the loss function (e.g., using Softmax Loss) improve performance?\n",
    "    # Could increasing the number of epochs lead to a better model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: sentence_transformers in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sentence_transformers) (4.48.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sentence_transformers) (1.15.1)\n",
      "Requirement already satisfied: Pillow in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: sympy in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sky/miniforge3/envs/llm596/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1757b89a722c4ad8ad50f0d39c43edcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your API token from your Hugging Face Account. Make sure to save it in text file or notepad for future use.\n",
    "# Will need to add it once per section\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 11:44:39.420461: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-10 11:44:39.447577: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-10 11:44:39.447598: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-10 11:44:39.447602: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-10 11:44:39.452201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "class TextSimilarityModel:\n",
    "    def __init__(self, corpus_name, rel_name, model_name='all-MiniLM-L6-v2', top_k=10):\n",
    "        \"\"\"\n",
    "        Initialize the model with datasets and pre-trained sentence transformer.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.corpus_name = corpus_name\n",
    "        self.rel_name = rel_name\n",
    "        self.top_k = top_k\n",
    "        self.load_data()\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and filter datasets based on test queries and documents.\n",
    "        \"\"\"\n",
    "        # Load query and document datasets\n",
    "        dataset_queries = load_dataset(self.corpus_name, \"queries\")\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "\n",
    "        # Extract queries and documents\n",
    "        self.queries = dataset_queries[\"queries\"][\"text\"]\n",
    "        self.query_ids = dataset_queries[\"queries\"][\"_id\"]\n",
    "        self.documents = dataset_docs[\"corpus\"][\"text\"]\n",
    "        self.document_ids = dataset_docs[\"corpus\"][\"_id\"]\n",
    "\n",
    "        # Filter queries and documents and build relevant queries and documents mapping based on test set\n",
    "        test_qrels = load_dataset(self.rel_name)[\"test\"]\n",
    "        self.filtered_test_query_ids = set(test_qrels[\"query-id\"])\n",
    "        self.filtered_test_doc_ids = set(test_qrels[\"corpus-id\"])\n",
    "\n",
    "        self.test_queries = [q for qid, q in zip(self.query_ids, self.queries) if qid in self.filtered_test_query_ids]\n",
    "        self.test_query_ids = [qid for qid in self.query_ids if qid in self.filtered_test_query_ids]\n",
    "        self.test_documents = [doc for did, doc in zip(self.document_ids, self.documents) if did in self.filtered_test_doc_ids]\n",
    "        self.test_document_ids = [did for did in self.document_ids if did in self.filtered_test_doc_ids]\n",
    "\n",
    "        self.test_query_id_to_relevant_doc_ids = {qid: [] for qid in self.test_query_ids}\n",
    "        for qid, doc_id in zip(test_qrels[\"query-id\"], test_qrels[\"corpus-id\"]):\n",
    "            if qid in self.test_query_id_to_relevant_doc_ids:\n",
    "                self.test_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "                \n",
    "        ## Code Below this is used for creating the training set \n",
    "        # Build query and document id to text mapping\n",
    "        self.query_id_to_text = {query_id:query for query_id, query in zip(self.query_ids, self.queries)}\n",
    "        self.document_id_to_text = {document_id:document for document_id, document in zip(self.document_ids, self.documents)}\n",
    "\n",
    "        # Build relevant queries and documents mapping based on train set\n",
    "        train_qrels = load_dataset(self.rel_name)[\"train\"]\n",
    "        self.train_query_id_to_relevant_doc_ids = {qid: [] for qid in train_qrels[\"query-id\"]}\n",
    "\n",
    "        for qid, doc_id in zip(train_qrels[\"query-id\"], train_qrels[\"corpus-id\"]):\n",
    "            if qid in self.train_query_id_to_relevant_doc_ids:\n",
    "                # Append the document ID to the relevant doc mapping\n",
    "                self.train_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "        \n",
    "        # Filter queries and documents and build relevant queries and documents mapping based on validation set  \n",
    "        #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "        try:\n",
    "            val_qrels = load_dataset(self.rel_name)['validation']\n",
    "            self.filtered_val_query_ids = set(val_qrels['query-id'])\n",
    "            self.filtered_val_doc_ids = set(val_qrels['corpus-id'])\n",
    "\n",
    "            val_query_pairs = [\n",
    "                (qid, query) for qid, query in zip(self.query_ids, self.queries) \n",
    "                if qid in self.filtered_val_query_ids\n",
    "            ]\n",
    "            self.val_query_ids = [pair[0] for pair in val_query_pairs]\n",
    "            self.val_queries = [pair[1] for pair in val_query_pairs]\n",
    "\n",
    "            val_doc_pairs = [\n",
    "                (did, doc) for did, doc in zip(self.document_ids, self.documents) \n",
    "                if did in self.filtered_val_doc_ids\n",
    "            ]\n",
    "            self.val_document_ids = [pair[0] for pair in val_doc_pairs]\n",
    "            self.val_documents = [pair[1] for pair in val_doc_pairs]\n",
    "\n",
    "            self.val_query_id_to_relevant_doc_ids = {qid: [] for qid in self.val_query_ids}\n",
    "            for qid, doc_id in zip(val_qrels['query-id'], val_qrels['corpus-id']):\n",
    "                if qid in self.val_query_id_to_relevant_doc_ids:\n",
    "                    self.val_query_id_to_relevant_doc_ids[qid].append(doc_id)\n",
    "        except Exception as e:\n",
    "            print('No validation split available. Skipping validation set creation.')\n",
    "        ###########################################################################\n",
    "        \n",
    "\n",
    "    #Task 1: Encode Queries and Documents (10 Pts)\n",
    "\n",
    "    def encode_with_glove(self, glove_file_path: str, sentences: list[str]) -> list[np.ndarray]:\n",
    "\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - glove_file_path (str): Path to the GloVe embeddings file (e.g., \"glove.6B.50d.txt\").\n",
    "            - sentences (list[str]): A list of sentences to encode.\n",
    "\n",
    "        # Output:\n",
    "            - list[np.ndarray]: A list of sentence embeddings \n",
    "            \n",
    "        (1) Encodes sentences by averaging GloVe 50d vectors of words in each sentence.\n",
    "        (2) Return a sequence of embeddings of the sentences.\n",
    "        Download the glove vectors from here. \n",
    "        https://nlp.stanford.edu/data/glove.6B.zip\n",
    "        Handle unknown words by using zero vectors\n",
    "        \"\"\"\n",
    "        #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "        # Load GloVe embeddings dictionary\n",
    "        glove_dict = {}\n",
    "        with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                # Skip incomplete lines (should contain one word + 50 dimensions)\n",
    "                if len(parts) < 51:\n",
    "                    continue\n",
    "                word = parts[0]\n",
    "                vector = np.array(parts[1:], dtype=float)\n",
    "                glove_dict[word] = vector\n",
    "\n",
    "        embedding_dim = len(next(iter(glove_dict.values())))\n",
    "        embeddings = []\n",
    "\n",
    "        # Encode each sentence by averaging word vectors\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            vecs = []\n",
    "            for token in tokens:\n",
    "                # Use lower-case tokens to match the GloVe keys\n",
    "                token_vec = glove_dict.get(token.lower())\n",
    "                if token_vec is None:\n",
    "                    token_vec = np.zeros(embedding_dim)\n",
    "                vecs.append(token_vec)\n",
    "            if vecs:\n",
    "                sentence_embedding = np.mean(vecs, axis=0)\n",
    "            else:\n",
    "                sentence_embedding = np.zeros(embedding_dim)\n",
    "            embeddings.append(sentence_embedding)\n",
    "\n",
    "        return embeddings\n",
    "        ###########################################################################\n",
    "\n",
    "    #Task 2: Calculate Cosine Similarity and Rank Documents (20 Pts)\n",
    "    \n",
    "    def rank_documents(self, encoding_method: str = 'sentence_transformer') -> None:\n",
    "        \"\"\"\n",
    "         # Inputs:\n",
    "            - encoding_method (str): The method used for encoding queries/documents. \n",
    "                             Options: ['glove', 'sentence_transformer'].\n",
    "\n",
    "        # Output:\n",
    "            - None (updates self.query_id_to_ranked_doc_ids with ranked document IDs).\n",
    "    \n",
    "        (1) Compute cosine similarity between each document and the query\n",
    "        (2) Rank documents for each query and save the results in a dictionary \"query_id_to_ranked_doc_ids\" \n",
    "            This will be used in \"mean_average_precision\"\n",
    "            Example format {2: [125, 673], 35: [900, 822]}\n",
    "        \"\"\"\n",
    "        if encoding_method == 'glove':\n",
    "            query_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.queries)\n",
    "            document_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.documents)\n",
    "        elif encoding_method == 'sentence_transformer':\n",
    "            query_embeddings = self.model.encode(self.queries)\n",
    "            document_embeddings = self.model.encode(self.documents)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid encoding method. Choose 'glove' or 'sentence_transformer'.\")\n",
    "        \n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "        # Map test query IDs to their indices in the full query list.\n",
    "        test_query_indices = [self.query_ids.index(qid) for qid in self.test_query_ids]\n",
    "        # Map test document IDs to their indices in the full document list.\n",
    "        test_doc_indices = [self.document_ids.index(doc_id) for doc_id in self.test_document_ids]\n",
    "        \n",
    "        # Subset the embeddings for test queries and documents using the computed indices.\n",
    "        test_query_embeddings = [query_embeddings[i] for i in test_query_indices]\n",
    "        test_document_embeddings = [document_embeddings[i] for i in test_doc_indices]\n",
    "        \n",
    "        # Now compute the cosine similarity matrix only on test queries vs test documents.\n",
    "        sim_matrix = cosine_similarity(test_query_embeddings, test_document_embeddings)\n",
    "\n",
    "        # Initialize the dictionary for storing ranked document IDs.\n",
    "        self.query_id_to_ranked_doc_ids = {}\n",
    "        \n",
    "        # For each test query, rank the documents based on their similarity scores.\n",
    "        for i, qid in enumerate(self.test_query_ids):\n",
    "            sim_scores = sim_matrix[i]\n",
    "            # Sort document indices by descending similarity.\n",
    "            ranked_indices = np.argsort(sim_scores)[::-1]\n",
    "            ranked_doc_ids = [self.test_document_ids[idx] for idx in ranked_indices]\n",
    "            self.query_id_to_ranked_doc_ids[qid] = ranked_doc_ids\n",
    "        ###########################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def average_precision(relevant_docs: list[str], candidate_docs: list[str], k: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Implement steps:\n",
    "        1. Only take the first k candidate documents\n",
    "        2. Calculate the number of relevant documents in the first k documents\n",
    "        3. Calculate MAP@k\n",
    "        \n",
    "        Note:\n",
    "        - k is usually set to 10, because users rarely look at more results\n",
    "        - This approach is more realistic in practical applications\n",
    "        - It better evaluates the model's performance on the most relevant documents\n",
    "        \"\"\"\n",
    "        # Only take the first k documents\n",
    "        candidate_docs = candidate_docs[:k]\n",
    "        # Calculate which documents are relevant\n",
    "        y_true = [1 if doc_id in relevant_docs else 0 for doc_id in candidate_docs]\n",
    "        # Calculate precision at each position\n",
    "        precisions = [np.mean(y_true[:i+1]) for i in range(len(y_true)) if y_true[i]]\n",
    "        return np.mean(precisions) if precisions else 0\n",
    "\n",
    "    #Task 3: Calculate Evaluate System Performance (10 Pts)\n",
    "    \n",
    "    def mean_average_precision(self) -> float:\n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - None (uses ranked documents stored in self.query_id_to_ranked_doc_ids).\n",
    "\n",
    "        # Output:\n",
    "            - float: The MAP score, computed as the mean of all average precision scores.\n",
    "    \n",
    "        (1) Compute mean average precision for all queries using the \"average_precision\" function.\n",
    "        (2) Compute the mean of all average precision scores\n",
    "        Return the mean average precision score\n",
    "        \n",
    "        reference: https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map\n",
    "        https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2\n",
    "        \"\"\"\n",
    "         #TODO Put your code here. \n",
    "        ###########################################################################\n",
    "        ap_scores = []\n",
    "        for qid in self.test_query_ids:\n",
    "            relevant_docs = self.test_query_id_to_relevant_doc_ids.get(qid, [])\n",
    "            candidate_docs = self.query_id_to_ranked_doc_ids.get(qid, [])\n",
    "            ap = self.average_precision(relevant_docs, candidate_docs)\n",
    "            ap_scores.append(ap)\n",
    "        return np.mean(ap_scores) if ap_scores else 0.0\n",
    "        ###########################################################################\n",
    "    \n",
    "    #Task 4: Ranking the Top 10 Documents based on Similarity Scores (10 Pts)\n",
    "   \n",
    "    def show_ranking_documents(self, example_query: str) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        # Inputs:\n",
    "            - example_query (str): A query string for which top-ranked documents should be displayed.\n",
    "\n",
    "        # Output:\n",
    "            - None (prints the ranked documents along with similarity scores).\n",
    "        \n",
    "        (1) rank documents with given query with cosine similarity scores\n",
    "        (2) prints the top 10 results along with its similarity score.\n",
    "        \n",
    "        \"\"\"\n",
    "        #TODO Put your code here. \n",
    "        query_embedding = self.model.encode(example_query)\n",
    "        document_embeddings = self.model.encode(self.documents)\n",
    "        ###########################################################################\n",
    "        # Compute cosine similarity scores between the query and all documents\n",
    "        sim_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "        # Get indices of top K documents based on the similarity scores\n",
    "        top_k_indices = np.argsort(sim_scores)[::-1][:self.top_k]\n",
    "\n",
    "        print(f'Top {self.top_k} documents for the query: \"{example_query}\"')\n",
    "        for rank, idx in enumerate(top_k_indices, start=1):\n",
    "            doc_id = self.document_ids[idx]\n",
    "            score = sim_scores[idx]\n",
    "            print(f'Rank {rank}: Document ID: {doc_id}, Similarity Score: {score:.4f}')\n",
    "            ###########################################################################\n",
    "      \n",
    "    #Task 5:Fine tune the sentence transformer model (25 Pts)\n",
    "    # Students are not graded on achieving a high MAP score. \n",
    "    # The key is to show understanding, experimentation, and thoughtful analysis.\n",
    "    \n",
    "    def fine_tune_model(self, batch_size: int = 32, num_epochs: int = 3, save_model_path: str = \"finetuned_senBERT\") -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Fine-tunes the model using MultipleNegativesRankingLoss.\n",
    "        (1) Prepare training examples from `self.prepare_training_examples()`\n",
    "        (2) Experiment with [anchor, positive] vs [anchor, positive, negative]\n",
    "        (3) Define a loss function (`MultipleNegativesRankingLoss`)\n",
    "        (4) Freeze all model layers except the final layers\n",
    "        (5) Train the model with the specified learning rate\n",
    "        (6) Save the fine-tuned model\n",
    "        \"\"\"\n",
    "        #TODO Put your code here.\n",
    "        ###########################################################################\n",
    "        \"\"\"\n",
    "        Fine-tunes the model using MultipleNegativesRankingLoss.\n",
    "        \"\"\"\n",
    "        # Import torch at the beginning of the method\n",
    "        import torch\n",
    "        from torch.utils.data import DataLoader\n",
    "        import time\n",
    "        from datetime import timedelta\n",
    "        \n",
    "        # Check device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        # Prepare training examples\n",
    "        train_examples = self.prepare_training_examples()\n",
    "        print(f\"Number of training examples: {len(train_examples)}\")\n",
    "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "        \n",
    "        # Define the loss function\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(self.model)\n",
    "        \n",
    "        # Print model parameters status\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Freeze all layers except the final layer\n",
    "        for name, param in self.model.named_parameters():\n",
    "            # Only unfreeze the final transformer layer (layer.5 for all-MiniLM-L6-v2)\n",
    "            if 'layer.5' in name:  # all-MiniLM-L6-v2 has 6 layers (0-5)\n",
    "                param.requires_grad = True\n",
    "                print(f\"Unfreezing: {name}\")\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Print trainable parameters to verify\n",
    "        print(\"\\nTrainable parameters:\")\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"- {name}\")\n",
    "                \n",
    "        # Training loop with timing\n",
    "        start_time = time.time()\n",
    "        print(\"\\nStarting training...\")\n",
    "        \n",
    "        # Fine-tune the model with warmup\n",
    "        warmup_steps = int(len(train_dataloader) * 0.1)  # 10% of training data for warmup\n",
    "        self.model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=num_epochs,\n",
    "            warmup_steps=warmup_steps,\n",
    "            show_progress_bar=True,\n",
    "            output_path=save_model_path,\n",
    "            checkpoint_path=f\"{save_model_path}_checkpoint\",\n",
    "            checkpoint_save_steps=len(train_dataloader),\n",
    "            callback=lambda score, epoch, steps: print(f\"\\nEpoch {epoch}: Score {score:.4f}\")\n",
    "        )\n",
    "        \n",
    "        # Calculate and print training time\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {str(timedelta(seconds=int(training_time)))}\")\n",
    "        \n",
    "        # Save the model\n",
    "        print(f\"Saving model to {save_model_path}\")\n",
    "        self.model.save(save_model_path)\n",
    "        print(\"Model saved successfully!\")\n",
    "        ###########################################################################\n",
    "\n",
    "    # Take a careful look into how the training set is created\n",
    "    def prepare_training_examples(self) -> list[InputExample]:\n",
    "\n",
    "        \"\"\"\n",
    "        Prepares training examples from the training data.\n",
    "        # Inputs:\n",
    "            - None (uses self.train_query_id_to_relevant_doc_ids to create training pairs).\n",
    "\n",
    "         # Output:\n",
    "            Output: - list[InputExample]: A list of training samples containing [anchor, positive] or [anchor, positive, negative].\n",
    "            \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Prepares training examples from the training data.\n",
    "        \"\"\"\n",
    "        train_examples = []\n",
    "        import random\n",
    "        from datetime import timedelta\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        print(\"\\nPreparing training examples...\")\n",
    "        total_queries = len(self.train_query_id_to_relevant_doc_ids)\n",
    "        print(f\"Total queries to process: {total_queries}\")\n",
    "        \n",
    "        # Count total examples that will be created\n",
    "        total_examples = sum(len(doc_ids) for doc_ids in self.train_query_id_to_relevant_doc_ids.values())\n",
    "        print(f\"Expected total training examples: {total_examples}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create progress bar\n",
    "        pbar = tqdm(self.train_query_id_to_relevant_doc_ids.items(), \n",
    "                    total=total_queries,\n",
    "                    desc=\"Processing queries\")\n",
    "        \n",
    "        for qid, doc_ids in pbar:\n",
    "            anchor = self.query_id_to_text[qid]\n",
    "            # Precompute negative candidates for current query using set subtraction for efficiency\n",
    "            relevant_set = set(self.train_query_id_to_relevant_doc_ids.get(qid, []))\n",
    "            negative_candidates = list(set(self.document_ids) - relevant_set)\n",
    "            \n",
    "            for doc_id in doc_ids:\n",
    "                positive = self.document_id_to_text[doc_id]\n",
    "                \n",
    "                # Update progress bar description with current query details\n",
    "                pbar.set_description(f\"Query {qid}: {len(doc_ids)} docs\")\n",
    "                \n",
    "                # Build texts list without an explicit else branch.\n",
    "                texts = [anchor, positive]\n",
    "                if negative_candidates:\n",
    "                    texts.append(self.document_id_to_text[random.choice(negative_candidates)])\n",
    "                train_examples.append(InputExample(texts=texts))\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nTraining examples preparation completed in {timedelta(seconds=int(elapsed_time))}\")\n",
    "        print(f\"Final number of training examples: {len(train_examples)}\")\n",
    "        \n",
    "        return train_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking with sentence_transformer...\n",
      "Mean Average Precision: 0.477390368428073\n",
      "Ranking with glove...\n",
      "Mean Average Precision: 0.08843636869952659\n",
      "Top 10 documents for the query: \"Breast Cancer Cells Feed on Cholesterol\"\n",
      "Rank 1: Document ID: MED-2439, Similarity Score: 0.6946\n",
      "Rank 2: Document ID: MED-2434, Similarity Score: 0.6723\n",
      "Rank 3: Document ID: MED-2440, Similarity Score: 0.6473\n",
      "Rank 4: Document ID: MED-2427, Similarity Score: 0.5877\n",
      "Rank 5: Document ID: MED-2774, Similarity Score: 0.5498\n",
      "Rank 6: Document ID: MED-838, Similarity Score: 0.5406\n",
      "Rank 7: Document ID: MED-2430, Similarity Score: 0.5205\n",
      "Rank 8: Document ID: MED-2102, Similarity Score: 0.5141\n",
      "Rank 9: Document ID: MED-2437, Similarity Score: 0.5081\n",
      "Rank 10: Document ID: MED-5066, Similarity Score: 0.5012\n"
     ]
    }
   ],
   "source": [
    "# Initialize and use the model\n",
    "model = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "\n",
    "# Compare the outputs \n",
    "print(\"Ranking with sentence_transformer...\")\n",
    "model.rank_documents(encoding_method='sentence_transformer')\n",
    "map_score = model.mean_average_precision()\n",
    "print(\"Mean Average Precision:\", map_score)\n",
    "\n",
    "# Compare the outputs \n",
    "print(\"Ranking with glove...\")\n",
    "model.rank_documents(encoding_method='glove')\n",
    "map_score = model.mean_average_precision()\n",
    "print(\"Mean Average Precision:\", map_score)\n",
    "\n",
    "\n",
    "model.show_ranking_documents(\"Breast Cancer Cells Feed on Cholesterol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Extension of TextSimilarityModel class\n",
    "def init_openai(self):\n",
    "    \"\"\"Initialize OpenAI client with API key from environment\"\"\"\n",
    "    if not hasattr(self, 'openai_client'):\n",
    "        self.openai_client = OpenAI()\n",
    "        \n",
    "def encode_with_openai(self, texts: list[str], batch_size: int = 100) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Encode texts using OpenAI's text-embedding-3-small model\n",
    "    \"\"\"\n",
    "    self.init_openai()\n",
    "    embeddings = []\n",
    "    \n",
    "    # Process in batches to avoid rate limits\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding with OpenAI\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            response = self.openai_client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=batch\n",
    "            )\n",
    "            batch_embeddings = [np.array(item.embedding) for item in response.data]\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}: {e}\")\n",
    "            # Use zero vectors as fallback\n",
    "            batch_embeddings = [np.zeros(1536) for _ in batch]  # text-embedding-3-small uses 1536 dimensions\n",
    "            embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Add methods to the class\n",
    "TextSimilarityModel.init_openai = init_openai\n",
    "TextSimilarityModel.encode_with_openai = encode_with_openai\n",
    "\n",
    "# Modify the rank_documents method\n",
    "original_rank_documents = TextSimilarityModel.rank_documents\n",
    "\n",
    "def rank_documents_with_openai(self, encoding_method: str = 'sentence_transformer') -> None:\n",
    "    if encoding_method == 'openai':\n",
    "        query_embeddings = self.encode_with_openai(self.queries)\n",
    "        document_embeddings = self.encode_with_openai(self.documents)\n",
    "    else:\n",
    "        return original_rank_documents(self, encoding_method)\n",
    "    \n",
    "    # Map test query IDs to their indices in the full query list\n",
    "    test_query_indices = [self.query_ids.index(qid) for qid in self.test_query_ids]\n",
    "    test_doc_indices = [self.document_ids.index(doc_id) for doc_id in self.test_document_ids]\n",
    "    \n",
    "    # Subset the embeddings\n",
    "    test_query_embeddings = [query_embeddings[i] for i in test_query_indices]\n",
    "    test_document_embeddings = [document_embeddings[i] for i in test_doc_indices]\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = cosine_similarity(test_query_embeddings, test_document_embeddings)\n",
    "\n",
    "    # Store ranked document IDs\n",
    "    self.query_id_to_ranked_doc_ids = {}\n",
    "    for i, qid in enumerate(self.test_query_ids):\n",
    "        sim_scores = sim_matrix[i]\n",
    "        ranked_indices = np.argsort(sim_scores)[::-1]\n",
    "        ranked_doc_ids = [self.test_document_ids[idx] for idx in ranked_indices]\n",
    "        self.query_id_to_ranked_doc_ids[qid] = ranked_doc_ids\n",
    "\n",
    "TextSimilarityModel.rank_documents = rank_documents_with_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the comparison experiment with different embedding methods\n",
    "print(\"Running comparison experiment with different embedding methods...\")\n",
    "\n",
    "# Ensure that the OpenAI API key is set\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Please set OPENAI_API_KEY environment variable\"\n",
    "\n",
    "# Initialize the model\n",
    "model = TextSimilarityModel(\"BeIR/nfcorpus\", \"BeIR/nfcorpus-qrels\")\n",
    "\n",
    "# Compare the performance of different methods\n",
    "methods = ['sentence_transformer', 'glove', 'openai']\n",
    "results = {}\n",
    "\n",
    "# Modify the show_ranking_documents method to support different encoding methods\n",
    "def show_ranking_documents_with_method(self, example_query: str, encoding_method: str = 'sentence_transformer') -> None:\n",
    "    if encoding_method == 'openai':\n",
    "        query_embedding = self.encode_with_openai([example_query])[0]\n",
    "        document_embeddings = self.encode_with_openai(self.documents)\n",
    "    elif encoding_method == 'glove':\n",
    "        query_embedding = self.encode_with_glove(\"glove.6B.50d.txt\", [example_query])[0]\n",
    "        document_embeddings = self.encode_with_glove(\"glove.6B.50d.txt\", self.documents)\n",
    "    else:  # sentence_transformer\n",
    "        query_embedding = self.model.encode(example_query)\n",
    "        document_embeddings = self.model.encode(self.documents)\n",
    "    \n",
    "    sim_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "    top_k_indices = np.argsort(sim_scores)[::-1][:self.top_k]\n",
    "\n",
    "    print(f'Top {self.top_k} documents for the query: \"{example_query}\" using {encoding_method}')\n",
    "    for rank, idx in enumerate(top_k_indices, start=1):\n",
    "        doc_id = self.document_ids[idx]\n",
    "        score = sim_scores[idx]\n",
    "        print(f'Rank {rank}: Document ID: {doc_id}, Similarity Score: {score:.4f}')\n",
    "        # 打印文档内容的前200个字符，帮助理解排序结果\n",
    "        doc_text = self.documents[idx][:200]\n",
    "        print(f'Document preview: {doc_text}...\\n')\n",
    "\n",
    "# Replace the original method\n",
    "TextSimilarityModel.show_ranking_documents = show_ranking_documents_with_method\n",
    "\n",
    "# Run the experiment\n",
    "for method in methods:\n",
    "    print(f\"\\nRanking with {method}...\")\n",
    "    model.rank_documents(encoding_method=method)\n",
    "    map_score = model.mean_average_precision()\n",
    "    results[method] = map_score\n",
    "    print(f\"Mean Average Precision ({method}): {map_score:.4f}\")\n",
    "\n",
    "# Test example queries\n",
    "test_queries = [\n",
    "    \"Breast Cancer Cells Feed on Cholesterol\",\n",
    "    \"Treatment options for COVID-19\",\n",
    "    \"Effects of exercise on mental health\"\n",
    "]\n",
    "\n",
    "for test_query in test_queries:\n",
    "    print(f\"\\nComparing rankings for query: '{test_query}'\")\n",
    "    for method in methods:\n",
    "        print(f\"\\n{method.upper()} Rankings:\")\n",
    "        model.show_ranking_documents(test_query, encoding_method=method)\n",
    "\n",
    "# Print the results summary\n",
    "print(\"\\nSummary of Results:\")\n",
    "print(\"-\" * 50)\n",
    "for method, score in results.items():\n",
    "    print(f\"{method:20s}: MAP = {score:.4f}\")\n",
    "\n",
    "# Print the detailed performance analysis\n",
    "print(\"\\nDetailed Performance Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "for method in methods:\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    print(f\"- MAP Score: {results[method]:.4f}\")\n",
    "    \n",
    "    # Calculate the average precision for each query\n",
    "    query_scores = []\n",
    "    for qid in model.test_query_ids:\n",
    "        relevant_docs = model.test_query_id_to_relevant_doc_ids.get(qid, [])\n",
    "        candidate_docs = model.query_id_to_ranked_doc_ids.get(qid, [])\n",
    "        ap = model.average_precision(relevant_docs, candidate_docs)\n",
    "        query_scores.append(ap)\n",
    "    \n",
    "    # Calculate the statistics\n",
    "    scores_array = np.array(query_scores)\n",
    "    print(f\"- Average Precision Statistics:\")\n",
    "    print(f\"  * Mean: {np.mean(scores_array):.4f}\")\n",
    "    print(f\"  * Median: {np.median(scores_array):.4f}\")\n",
    "    print(f\"  * Std Dev: {np.std(scores_array):.4f}\")\n",
    "    print(f\"  * Min: {np.min(scores_array):.4f}\")\n",
    "    print(f\"  * Max: {np.max(scores_array):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Loaded 557 queries and 3125 documents\n",
      "Ranking documents...\n",
      "Encoding queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7535d6320e4fcdaaa7d6b6fc4d63c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46faa14f469f46a3812cc63dca998565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarities and ranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:06<00:00, 80.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n",
      "\n",
      "First few rows of the submission file:\n",
      "                                               Query  \\\n",
      "0                       Herbalife® has been updated.   \n",
      "1  Can eating Fruit & Nut Bars lead to an increas...   \n",
      "2                      What can I do with chickpeas?   \n",
      "3    Are chronic headaches caused by pork parasites?   \n",
      "4  is a professor at Harvard University and also ...   \n",
      "\n",
      "                                              Doc_ID  \n",
      "0  MED-5158 MED-4366 MED-3487 MED-5157 MED-4374 M...  \n",
      "1  MED-4707 MED-3896 MED-2595 MED-4291 MED-4292 M...  \n",
      "2  MED-2009 MED-2010 MED-4443 MED-2073 MED-3132 M...  \n",
      "3  MED-3169 MED-3319 MED-3288 MED-3175 MED-4818 M...  \n",
      "4  MED-4686 MED-2112 MED-4299 MED-2220 MED-1478 M...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KaggleSubmissionModel:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the model with the pre-trained sentence transformer\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.corpus_name = \"BeIR/nfcorpus\"\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from both Kaggle test files and BeIR dataset\n",
    "        \"\"\"\n",
    "        # Load Kaggle test set\n",
    "        self.test_queries_df = pd.read_csv('test_query.csv')\n",
    "        self.test_documents_df = pd.read_csv('test_documents.csv')\n",
    "        \n",
    "        # Load BeIR dataset to get text content\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "        \n",
    "        # Create a mapping from document IDs to text\n",
    "        self.doc_id_to_text = {\n",
    "            doc_id: text for doc_id, text in zip(\n",
    "                dataset_docs[\"corpus\"][\"_id\"],\n",
    "                dataset_docs[\"corpus\"][\"text\"]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Get the text content of the test documents\n",
    "        self.test_doc_texts = []\n",
    "        for doc_id in self.test_documents_df['Doc']:\n",
    "            text = self.doc_id_to_text.get(doc_id, \"\")  # If the document is not found, use an empty string\n",
    "            self.test_doc_texts.append(text)\n",
    "        \n",
    "        print(f\"Loaded {len(self.test_queries_df)} queries and {len(self.test_documents_df)} documents\")\n",
    "\n",
    "    def rank_documents(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        Rank documents for each query using sentence transformer embeddings\n",
    "        \"\"\"\n",
    "        print(\"Encoding queries...\")\n",
    "        query_embeddings = self.model.encode(\n",
    "            self.test_queries_df['Query'].tolist(), \n",
    "            batch_size=batch_size, \n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        print(\"Encoding documents...\")\n",
    "        doc_embeddings = self.model.encode(\n",
    "            self.test_doc_texts,\n",
    "            batch_size=batch_size, \n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        print(\"Computing similarities and ranking documents...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, query_embedding in enumerate(tqdm(query_embeddings)):\n",
    "            # Calculate cosine similarity\n",
    "            similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "            \n",
    "            # Get the indices of the top 10 most similar documents\n",
    "            top_indices = np.argsort(similarities)[::-1][:10]\n",
    "            \n",
    "            # Get the corresponding document IDs\n",
    "            top_doc_ids = [self.test_documents_df.iloc[idx]['Doc'] for idx in top_indices]\n",
    "            \n",
    "            # Combine the document IDs into a string\n",
    "            doc_ids_str = ' '.join(top_doc_ids)\n",
    "            \n",
    "            # Add to the results\n",
    "            results.append({\n",
    "                'Query': self.test_queries_df.iloc[i]['Query'],\n",
    "                'Doc_ID': doc_ids_str\n",
    "            })\n",
    "        \n",
    "        # Create the submission file\n",
    "        submission_df = pd.DataFrame(results)\n",
    "        submission_df.to_csv('submission.csv', index=False)\n",
    "        print(\"Submission file created successfully!\")\n",
    "        \n",
    "        # Display the first few rows as an example\n",
    "        print(\"\\nFirst few rows of the submission file:\")\n",
    "        print(submission_df.head())\n",
    "\n",
    "def main():\n",
    "    # Use a more powerful model\n",
    "    print(\"Initializing model...\")\n",
    "    model = KaggleSubmissionModel(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "    print(\"Ranking documents...\")\n",
    "    model.rank_documents()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with bi-encoder & cross-encoder re-ranking...\n",
      "Loaded 557 queries and 3125 documents.\n",
      "Ranking documents...\n",
      "Using the bi-encoder to encode the documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed308c3b07c48119b8c68101529bea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the bi-encoder to encode the queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69da8139c02444728c0618955dd47a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing candidate retrieval and cross-encoder re-ranking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:33<00:00, 16.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n",
      "\n",
      "First few rows of the submission file:\n",
      "                                               Query  \\\n",
      "0                       Herbalife® has been updated.   \n",
      "1  Can eating Fruit & Nut Bars lead to an increas...   \n",
      "2                      What can I do with chickpeas?   \n",
      "3    Are chronic headaches caused by pork parasites?   \n",
      "4  is a professor at Harvard University and also ...   \n",
      "\n",
      "                                              Doc_ID  \n",
      "0  MED-5158 MED-5157 MED-4873 MED-3489 MED-2891 M...  \n",
      "1  MED-3896 MED-4707 MED-4286 MED-2592 MED-4292 M...  \n",
      "2  MED-2009 MED-2145 MED-2010 MED-3132 MED-2070 M...  \n",
      "3  MED-3175 MED-3288 MED-3177 MED-3319 MED-3171 M...  \n",
      "4  MED-2112 MED-2220 MED-3654 MED-2765 MED-1558 M...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import faiss  # FAISS is used for efficient approximate nearest neighbor retrieval\n",
    "\n",
    "class KaggleSubmissionModel:\n",
    "    def __init__(self, \n",
    "                 bi_encoder_model_name='sentence-transformers/all-mpnet-base-v2', \n",
    "                 cross_encoder_model_name='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "                 candidate_count=50):\n",
    "        \"\"\"\n",
    "        Initialize the model:\n",
    "        - Use the bi-encoder (bi-encoder) for fast retrieval\n",
    "        - Use the cross-encoder (cross encoder) for re-ranking, achieving higher accuracy\n",
    "        - candidate_count: The number of candidate documents initially retrieved for each query\n",
    "        \"\"\"\n",
    "        self.bi_encoder = SentenceTransformer(bi_encoder_model_name)\n",
    "        self.cross_encoder = CrossEncoder(cross_encoder_model_name)\n",
    "        self.candidate_count = candidate_count\n",
    "        self.corpus_name = \"BeIR/nfcorpus\"\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the Kaggle test set (queries and document IDs) and the full text data of the BeIR dataset,\n",
    "        construct a mapping from document IDs to text, and get the corresponding text content based on the test document IDs.\n",
    "        \"\"\"\n",
    "        # Load the Kaggle test set data\n",
    "        self.test_queries_df = pd.read_csv('test_query.csv')\n",
    "        self.test_documents_df = pd.read_csv('test_documents.csv')\n",
    "        \n",
    "        # Load the BeIR dataset (corpus part) to get the full text of the documents\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "        \n",
    "        # Construct a mapping from document IDs to text\n",
    "        self.doc_id_to_text = {\n",
    "            doc_id: text for doc_id, text in zip(\n",
    "                dataset_docs[\"corpus\"][\"_id\"],\n",
    "                dataset_docs[\"corpus\"][\"text\"]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Get the text content of the test documents, using an empty string if not found\n",
    "        self.test_doc_texts = [\n",
    "            self.doc_id_to_text.get(doc_id, \"\") for doc_id in self.test_documents_df['Doc']\n",
    "        ]\n",
    "        \n",
    "        print(f\"Loaded {len(self.test_queries_df)} queries and {len(self.test_documents_df)} documents.\")\n",
    "    \n",
    "    def build_faiss_index(self, embeddings):\n",
    "        \"\"\"\n",
    "        Use FAISS to build an index.\n",
    "        Here we normalize the embeddings to L2, so that the inner product calculation is equivalent to calculating cosine similarity.\n",
    "        \"\"\"\n",
    "        embeddings = embeddings.astype('float32')\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        index.add(embeddings)\n",
    "        return index\n",
    "    \n",
    "    def rank_documents(self, batch_size=32, top_k=10):\n",
    "        \"\"\"\n",
    "        For each query, first retrieve candidate_count documents using the bi-encoder and FAISS,\n",
    "        then use the cross-encoder to re-rank the candidate documents, and finally select the top_k documents,\n",
    "        and generate the submission file submission.csv.\n",
    "        \"\"\"\n",
    "        # 1. Use the bi-encoder to encode the documents\n",
    "        print(\"Using the bi-encoder to encode the documents...\")\n",
    "        doc_embeddings = self.bi_encoder.encode(\n",
    "            self.test_doc_texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        doc_embeddings = np.array(doc_embeddings)\n",
    "        index = self.build_faiss_index(doc_embeddings)\n",
    "        \n",
    "        # 2. Encode the queries\n",
    "        print(\"Using the bi-encoder to encode the queries...\")\n",
    "        query_embeddings = self.bi_encoder.encode(\n",
    "            self.test_queries_df['Query'].tolist(),\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        query_embeddings = np.array(query_embeddings).astype('float32')\n",
    "        faiss.normalize_L2(query_embeddings)\n",
    "        \n",
    "        results = []\n",
    "        print(\"Performing candidate retrieval and cross-encoder re-ranking...\")\n",
    "        for i, query_embedding in enumerate(tqdm(query_embeddings)):\n",
    "            # Use FAISS to retrieve candidate_count documents\n",
    "            query_embedding_2d = np.expand_dims(query_embedding, axis=0)\n",
    "            distances, indices = index.search(query_embedding_2d, self.candidate_count)\n",
    "            candidate_indices = indices[0]\n",
    "            \n",
    "            # Prepare candidate pairs, format as (query, document)\n",
    "            query_text = self.test_queries_df.iloc[i]['Query']\n",
    "            candidate_pairs = []\n",
    "            candidate_ids = []\n",
    "            for idx in candidate_indices:\n",
    "                doc_text = self.test_doc_texts[idx]\n",
    "                candidate_pairs.append((query_text, doc_text))\n",
    "                candidate_ids.append(self.test_documents_df.iloc[idx]['Doc'])\n",
    "            \n",
    "            # Use the cross-encoder to re-rank the candidate documents\n",
    "            cross_scores = self.cross_encoder.predict(candidate_pairs)\n",
    "            # Sort the candidate documents in descending order of the re-ranking scores, and select the top top_k documents\n",
    "            sorted_indices = np.argsort(cross_scores)[::-1][:top_k]\n",
    "            top_doc_ids = [candidate_ids[idx] for idx in sorted_indices]\n",
    "            \n",
    "            results.append({\n",
    "                'Query': query_text,\n",
    "                'Doc_ID': ' '.join(top_doc_ids)\n",
    "            })\n",
    "        \n",
    "        # Generate the submission file\n",
    "        submission_df = pd.DataFrame(results)\n",
    "        submission_df.to_csv('submissionv2.csv', index=False)\n",
    "        print(\"Submission file created successfully!\")\n",
    "        print(\"\\nFirst few rows of the submission file:\")\n",
    "        print(submission_df.head())\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing model with bi-encoder & cross-encoder re-ranking...\")\n",
    "    model = KaggleSubmissionModel(candidate_count=50)\n",
    "    print(\"Ranking documents...\")\n",
    "    model.rank_documents()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with OpenAI embeddings and cross-encoder re-ranking...\n",
      "Loaded 557 queries and 3125 documents.\n",
      "Ranking documents...\n",
      "Getting document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding with OpenAI: 100%|██████████| 196/196 [01:55<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache embeddings_cache/doc_embeddings_text-embedding-3-small.pkl\n",
      "Getting query embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding with OpenAI: 100%|██████████| 35/35 [00:17<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache embeddings_cache/query_embeddings_text-embedding-3-small.pkl\n",
      "Retrieving candidates and re-ranking with cross-encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries: 100%|██████████| 557/557 [00:33<00:00, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n",
      "\n",
      "First few rows of the submission file:\n",
      "                                               Query  \\\n",
      "0                       Herbalife® has been updated.   \n",
      "1  Can eating Fruit & Nut Bars lead to an increas...   \n",
      "2                      What can I do with chickpeas?   \n",
      "3    Are chronic headaches caused by pork parasites?   \n",
      "4  is a professor at Harvard University and also ...   \n",
      "\n",
      "                                              Doc_ID  \n",
      "0  MED-5158 MED-5157 MED-4873 MED-4535 MED-3489 M...  \n",
      "1  MED-3896 MED-4707 MED-4286 MED-2592 MED-4292 M...  \n",
      "2  MED-2009 MED-2145 MED-2010 MED-2148 MED-2008 M...  \n",
      "3  MED-3175 MED-3288 MED-3177 MED-3319 MED-3171 M...  \n",
      "4  MED-2112 MED-2489 MED-4599 MED-4674 MED-5123 M...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import CrossEncoder\n",
    "import pickle\n",
    "\n",
    "class KaggleSubmissionModel:\n",
    "    def __init__(self, \n",
    "                 candidate_count=50, \n",
    "                 openai_model=\"text-embedding-3-small\", \n",
    "                 cross_encoder_model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "                 cache_dir=\"embeddings_cache\"):\n",
    "        \"\"\"\n",
    "        Initialize the model, add the cache directory parameter\n",
    "        \"\"\"\n",
    "        self.openai_model = openai_model\n",
    "        self.candidate_count = candidate_count\n",
    "        self.cross_encoder = CrossEncoder(cross_encoder_model_name)\n",
    "        self.corpus_name = \"BeIR/nfcorpus\"\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        # Create the cache directory\n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "            \n",
    "        self.load_data()\n",
    "\n",
    "    def get_cache_path(self, prefix):\n",
    "        \"\"\"Get the cache file path\"\"\"\n",
    "        return os.path.join(self.cache_dir, f\"{prefix}_{self.openai_model.replace('/', '_')}.pkl\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load the data, same as before\"\"\"\n",
    "        self.test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "        self.test_documents_df = pd.read_csv(\"test_documents.csv\")\n",
    "        \n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "        self.doc_id_to_text = {\n",
    "            doc_id: text \n",
    "            for doc_id, text in zip(dataset_docs[\"corpus\"][\"_id\"], dataset_docs[\"corpus\"][\"text\"])\n",
    "        }\n",
    "        \n",
    "        self.test_doc_texts = [self.doc_id_to_text.get(doc_id, \"\") for doc_id in self.test_documents_df[\"Doc\"]]\n",
    "        print(f\"Loaded {len(self.test_queries_df)} queries and {len(self.test_documents_df)} documents.\")\n",
    "\n",
    "    def init_openai(self):\n",
    "        \"\"\"Initialize the OpenAI client\"\"\"\n",
    "        if not hasattr(self, 'openai_client'):\n",
    "            self.openai_client = openai\n",
    "\n",
    "    def encode_with_openai(self, texts: list, batch_size: int = 100, cache_prefix: str = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode text, support caching\n",
    "        cache_prefix: cache file prefix, if provided, enable caching\n",
    "        \"\"\"\n",
    "        if cache_prefix:\n",
    "            cache_path = self.get_cache_path(cache_prefix)\n",
    "            # If the cache exists, load it directly\n",
    "            if os.path.exists(cache_path):\n",
    "                print(f\"Loading cached embeddings from {cache_path}\")\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "\n",
    "        self.init_openai()\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding with OpenAI\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            try:\n",
    "                response = self.openai_client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=batch\n",
    "                )\n",
    "                batch_embeddings = [item.embedding for item in response.data]\n",
    "                embeddings.extend(batch_embeddings)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i}: {e}\")\n",
    "                batch_embeddings = [[0.0] * 1536 for _ in batch]\n",
    "                embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        embeddings = np.ascontiguousarray(embeddings, dtype=np.float32)\n",
    "        \n",
    "        # If the cache prefix is provided, save to the cache\n",
    "        if cache_prefix:\n",
    "            print(f\"Saving embeddings to cache {cache_path}\")\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(embeddings, f)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def rank_documents(self, batch_size=16, top_k=10):\n",
    "        \"\"\"\n",
    "        Sort documents, use cached embeddings\n",
    "        \"\"\"\n",
    "        # 1. Use cached embeddings or calculate document embeddings\n",
    "        print(\"Getting document embeddings...\")\n",
    "        doc_embeddings = self.encode_with_openai(\n",
    "            self.test_doc_texts, \n",
    "            batch_size=batch_size,\n",
    "            cache_prefix=\"doc_embeddings\"\n",
    "        )\n",
    "        index = self.build_faiss_index(doc_embeddings)\n",
    "        \n",
    "        # 2. Use cached embeddings or calculate query embeddings\n",
    "        print(\"Getting query embeddings...\")\n",
    "        query_texts = self.test_queries_df[\"Query\"].tolist()\n",
    "        query_embeddings = self.encode_with_openai(\n",
    "            query_texts, \n",
    "            batch_size=batch_size,\n",
    "            cache_prefix=\"query_embeddings\"\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        print(\"Retrieving candidates and re-ranking with cross-encoder...\")\n",
    "        for i, query_embedding in enumerate(tqdm(query_embeddings, desc=\"Processing Queries\")):\n",
    "            query_embedding = np.ascontiguousarray(query_embedding, dtype=np.float32)\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "            faiss.normalize_L2(query_embedding)\n",
    "            \n",
    "            distances, indices = index.search(query_embedding, self.candidate_count)\n",
    "            candidate_indices = indices[0]\n",
    "            \n",
    "            query_text = self.test_queries_df.iloc[i][\"Query\"]\n",
    "            candidate_pairs = []\n",
    "            candidate_ids = []\n",
    "            for idx in candidate_indices:\n",
    "                doc_text = self.test_doc_texts[idx]\n",
    "                candidate_pairs.append((query_text, doc_text))\n",
    "                candidate_ids.append(self.test_documents_df.iloc[idx][\"Doc\"])\n",
    "            \n",
    "            cross_scores = self.cross_encoder.predict(candidate_pairs)\n",
    "            sorted_indices = np.argsort(cross_scores)[::-1][:top_k]\n",
    "            top_doc_ids = [candidate_ids[idx] for idx in sorted_indices]\n",
    "            \n",
    "            results.append({\n",
    "                \"Query\": query_text,\n",
    "                \"Doc_ID\": \" \".join(top_doc_ids)\n",
    "            })\n",
    "        \n",
    "        submission_df = pd.DataFrame(results)\n",
    "        submission_df.to_csv(\"submissionv3.csv\", index=False)\n",
    "        print(\"Submission file created successfully!\")\n",
    "        print(\"\\nFirst few rows of the submission file:\")\n",
    "        print(submission_df.head())\n",
    "\n",
    "    def build_faiss_index(self, embeddings):\n",
    "        \"\"\"Build the FAISS index, same as before\"\"\"\n",
    "        embeddings = np.ascontiguousarray(embeddings, dtype=np.float32)\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        index.add(embeddings)\n",
    "        return index\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing model with OpenAI embeddings and cross-encoder re-ranking...\")\n",
    "    model = KaggleSubmissionModel(candidate_count=50)\n",
    "    print(\"Ranking documents...\")\n",
    "    model.rank_documents()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with OpenAI embeddings for direct ranking...\n",
      "Initializing OpenAI client...\n",
      "Loading test queries and document IDs...\n",
      "Loading full document texts from BeIR dataset...\n",
      "Loaded 557 queries and 3125 documents.\n",
      "Ranking documents directly...\n",
      "Encoding document texts with OpenAI embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding with OpenAI: 100%|██████████| 196/196 [01:58<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache embeddings_cache/doc_embeddings_text-embedding-3-small.pkl\n",
      "Encoding query texts with OpenAI embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding with OpenAI: 100%|██████████| 35/35 [00:20<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache embeddings_cache/query_embeddings_text-embedding-3-small.pkl\n",
      "Ranking documents for each query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking Documents: 100%|██████████| 557/557 [00:01<00:00, 314.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n",
      "\n",
      "First few rows of the submission file:\n",
      "                                               Query  \\\n",
      "0                       Herbalife® has been updated.   \n",
      "1  Can eating Fruit & Nut Bars lead to an increas...   \n",
      "2                      What can I do with chickpeas?   \n",
      "3    Are chronic headaches caused by pork parasites?   \n",
      "4  is a professor at Harvard University and also ...   \n",
      "\n",
      "                                              Doc_ID  \n",
      "0  MED-5158 MED-5157 MED-3489 MED-3490 MED-4873 M...  \n",
      "1  MED-3896 MED-4286 MED-2595 MED-4707 MED-4291 M...  \n",
      "2  MED-2009 MED-2010 MED-2144 MED-2147 MED-3132 M...  \n",
      "3  MED-3175 MED-3169 MED-3288 MED-3177 MED-3319 M...  \n",
      "4  MED-2112 MED-4674 MED-4599 MED-2763 MED-1377 M...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "class KaggleSubmissionModel:\n",
    "    def __init__(self, \n",
    "                 openai_model=\"text-embedding-3-small\", \n",
    "                 cache_dir=\"embeddings_cache\"):\n",
    "        \"\"\"\n",
    "        Initialize the model:\n",
    "        - Specify the OpenAI embedding model (e.g. text-embedding-3-small)\n",
    "        - Specify the cache directory to avoid calling the API repeatedly\n",
    "        \"\"\"\n",
    "        self.openai_model = openai_model\n",
    "        self.cache_dir = cache_dir\n",
    "        self.corpus_name = \"BeIR/nfcorpus\"\n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "        self.init_openai()\n",
    "        self.load_data()\n",
    "    \n",
    "    def init_openai(self):\n",
    "        \"\"\"Initialize the OpenAI client\"\"\"\n",
    "        if not hasattr(self, 'openai_client'):\n",
    "            print(\"Initializing OpenAI client...\")\n",
    "            self.openai_client = openai\n",
    "\n",
    "    def get_cache_path(self, prefix):\n",
    "        \"\"\"Get the cache file path\"\"\"\n",
    "        safe_model = self.openai_model.replace('/', '_')\n",
    "        return os.path.join(self.cache_dir, f\"{prefix}_{safe_model}.pkl\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the test queries and document IDs,\n",
    "        and load the full document texts from the BeIR dataset, and construct a mapping from document ID to text.\n",
    "        \"\"\"\n",
    "        print(\"Loading test queries and document IDs...\")\n",
    "        self.test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "        self.test_documents_df = pd.read_csv(\"test_documents.csv\")\n",
    "        \n",
    "        print(\"Loading full document texts from BeIR dataset...\")\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "        self.doc_id_to_text = {\n",
    "            doc_id: text \n",
    "            for doc_id, text in zip(dataset_docs[\"corpus\"][\"_id\"], dataset_docs[\"corpus\"][\"text\"])\n",
    "        }\n",
    "        # Get the full text corresponding to the test document ID (return an empty string if not found)\n",
    "        self.test_doc_texts = [self.doc_id_to_text.get(doc_id, \"\") for doc_id in self.test_documents_df[\"Doc\"]]\n",
    "        print(f\"Loaded {len(self.test_queries_df)} queries and {len(self.test_documents_df)} documents.\")\n",
    "\n",
    "    def encode_with_openai(self, texts: list, batch_size: int = 16, cache_prefix: str = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode text, support caching, and correctly handle the new version of the OpenAI API response format\n",
    "        \"\"\"\n",
    "        if cache_prefix:\n",
    "            cache_path = self.get_cache_path(cache_prefix)\n",
    "            # If the cache exists, load it directly\n",
    "            if os.path.exists(cache_path):\n",
    "                print(f\"Loading cached embeddings from {cache_path}\")\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "\n",
    "        self.init_openai()\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding with OpenAI\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            try:\n",
    "                response = self.openai_client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=batch\n",
    "                )\n",
    "                batch_embeddings = [embedding.embedding for embedding in response.data]\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch starting at index {i}: {str(e)}\")\n",
    "                # Use zero vectors as a backup\n",
    "                batch_embeddings = [[0.0] * 1536 for _ in batch]  # text-embedding-3-small uses 1536-dimensional vectors\n",
    "                embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        embeddings = np.ascontiguousarray(embeddings, dtype=np.float32)\n",
    "        \n",
    "        # If the cache prefix is provided, save to the cache\n",
    "        if cache_prefix:\n",
    "            print(f\"Saving embeddings to cache {cache_path}\")\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(embeddings, f)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def normalize_embeddings(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize the embeddings, so that the inner product calculation is equivalent to cosine similarity\"\"\"\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        return embeddings / norms\n",
    "\n",
    "    def rank_documents(self, batch_size=16, top_k=10):\n",
    "        \"\"\"\n",
    "        Sort all documents directly:\n",
    "        1. Encode the documents and queries with OpenAI embeddings (support caching).\n",
    "        2. Normalize the embeddings and use the dot product (cosine similarity) to sort the documents.\n",
    "        3. Select the top k most similar documents for each query and generate the submission file.\n",
    "        \"\"\"\n",
    "        # 1. Encode documents\n",
    "        print(\"Encoding document texts with OpenAI embeddings...\")\n",
    "        doc_embeddings = self.encode_with_openai(\n",
    "            self.test_doc_texts,\n",
    "            batch_size=batch_size,\n",
    "            cache_prefix=\"doc_embeddings\"\n",
    "        )\n",
    "        doc_embeddings = self.normalize_embeddings(doc_embeddings)\n",
    "        \n",
    "        # 2. Encode queries\n",
    "        print(\"Encoding query texts with OpenAI embeddings...\")\n",
    "        query_texts = self.test_queries_df[\"Query\"].tolist()\n",
    "        query_embeddings = self.encode_with_openai(\n",
    "            query_texts,\n",
    "            batch_size=batch_size,\n",
    "            cache_prefix=\"query_embeddings\"\n",
    "        )\n",
    "        query_embeddings = self.normalize_embeddings(query_embeddings)\n",
    "        \n",
    "        # 3. Calculate the cosine similarity between each query and all documents directly, and select the top k documents\n",
    "        results = []\n",
    "        print(\"Ranking documents for each query...\")\n",
    "        for i, query_embedding in enumerate(tqdm(query_embeddings, desc=\"Ranking Documents\")):\n",
    "            # Since the embeddings are normalized, the dot product is equivalent to cosine similarity\n",
    "            sims = np.dot(doc_embeddings, query_embedding)\n",
    "            top_indices = np.argsort(sims)[::-1][:top_k]\n",
    "            top_doc_ids = [self.test_documents_df.iloc[idx][\"Doc\"] for idx in top_indices]\n",
    "            results.append({\n",
    "                \"Query\": self.test_queries_df.iloc[i][\"Query\"],\n",
    "                \"Doc_ID\": \" \".join(top_doc_ids)\n",
    "            })\n",
    "            \n",
    "        submission_df = pd.DataFrame(results)\n",
    "        submission_df.to_csv(\"submission_direct_openai_small.csv\", index=False)\n",
    "        print(\"Submission file created successfully!\")\n",
    "        print(\"\\nFirst few rows of the submission file:\")\n",
    "        print(submission_df.head())\n",
    "\n",
    "def main():\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print(\"Initializing model with OpenAI embeddings for direct ranking...\")\n",
    "    model = KaggleSubmissionModel(openai_model=\"text-embedding-3-small\")\n",
    "    print(\"Ranking documents directly...\")\n",
    "    model.rank_documents()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- text-embedding-3-small : 0.27903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with OpenAI embeddings for direct ranking...\n",
      "Initializing OpenAI client...\n",
      "Loading test queries and document IDs...\n",
      "Loading full document texts from BeIR dataset...\n",
      "Loaded 557 queries and 3125 documents.\n",
      "Ranking documents directly...\n",
      "Encoding document texts with OpenAI embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding with OpenAI: 100%|██████████| 196/196 [01:56<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache embeddings_cache/doc_embeddings_text-embedding-3-large.pkl\n",
      "Encoding query texts with OpenAI embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding with OpenAI: 100%|██████████| 35/35 [00:18<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to cache embeddings_cache/query_embeddings_text-embedding-3-large.pkl\n",
      "Ranking documents for each query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking Documents: 100%|██████████| 557/557 [00:03<00:00, 161.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n",
      "\n",
      "First few rows of the submission file:\n",
      "                                               Query  \\\n",
      "0                       Herbalife® has been updated.   \n",
      "1  Can eating Fruit & Nut Bars lead to an increas...   \n",
      "2                      What can I do with chickpeas?   \n",
      "3    Are chronic headaches caused by pork parasites?   \n",
      "4  is a professor at Harvard University and also ...   \n",
      "\n",
      "                                              Doc_ID  \n",
      "0  MED-5158 MED-5157 MED-3489 MED-3490 MED-4873 M...  \n",
      "1  MED-3896 MED-4286 MED-2595 MED-4291 MED-4707 M...  \n",
      "2  MED-2009 MED-2010 MED-2144 MED-2147 MED-3132 M...  \n",
      "3  MED-3175 MED-3169 MED-3288 MED-3177 MED-3316 M...  \n",
      "4  MED-2112 MED-4599 MED-4674 MED-2763 MED-1377 M...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "class KaggleSubmissionModel:\n",
    "    def __init__(self, \n",
    "                 openai_model=\"text-embedding-3-large\", \n",
    "                 cache_dir=\"embeddings_cache\"):\n",
    "        \"\"\"\n",
    "        Initialize the model:\n",
    "        - Specify the OpenAI embedding model (e.g. text-embedding-3-small)\n",
    "        - Specify the cache directory to avoid calling the API repeatedly\n",
    "        \"\"\"\n",
    "        self.openai_model = openai_model\n",
    "        self.cache_dir = cache_dir\n",
    "        self.corpus_name = \"BeIR/nfcorpus\"\n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "        self.init_openai()\n",
    "        self.load_data()\n",
    "    \n",
    "    def init_openai(self):\n",
    "        \"\"\"Initialize the OpenAI client\"\"\"\n",
    "        if not hasattr(self, 'openai_client'):\n",
    "            print(\"Initializing OpenAI client...\")\n",
    "            self.openai_client = openai\n",
    "\n",
    "    def get_cache_path(self, prefix):\n",
    "        \"\"\"Get the cache file path\"\"\"\n",
    "        safe_model = self.openai_model.replace('/', '_')\n",
    "        return os.path.join(self.cache_dir, f\"{prefix}_{safe_model}.pkl\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the test queries and document IDs,\n",
    "        and load the full document texts from the BeIR dataset, and construct a mapping from document ID to text.\n",
    "        \"\"\"\n",
    "        print(\"Loading test queries and document IDs...\")\n",
    "        self.test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "        self.test_documents_df = pd.read_csv(\"test_documents.csv\")\n",
    "        \n",
    "        print(\"Loading full document texts from BeIR dataset...\")\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "        self.doc_id_to_text = {\n",
    "            doc_id: text \n",
    "            for doc_id, text in zip(dataset_docs[\"corpus\"][\"_id\"], dataset_docs[\"corpus\"][\"text\"])\n",
    "        }\n",
    "        # Get the full text corresponding to the test document ID (return an empty string if not found)\n",
    "        self.test_doc_texts = [self.doc_id_to_text.get(doc_id, \"\") for doc_id in self.test_documents_df[\"Doc\"]]\n",
    "        print(f\"Loaded {len(self.test_queries_df)} queries and {len(self.test_documents_df)} documents.\")\n",
    "\n",
    "    def encode_with_openai(self, texts: list, batch_size: int = 16, cache_prefix: str = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode text, support caching, and correctly handle the new version of the OpenAI API response format\n",
    "        \"\"\"\n",
    "        if cache_prefix:\n",
    "            cache_path = self.get_cache_path(cache_prefix)\n",
    "            # If the cache exists, load it directly\n",
    "            if os.path.exists(cache_path):\n",
    "                print(f\"Loading cached embeddings from {cache_path}\")\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "\n",
    "        self.init_openai()\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding with OpenAI\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            try:\n",
    "                response = self.openai_client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=batch\n",
    "                )\n",
    "                batch_embeddings = [embedding.embedding for embedding in response.data]\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch starting at index {i}: {str(e)}\")\n",
    "                # Use zero vectors as a backup\n",
    "                batch_embeddings = [[0.0] * 1536 for _ in batch]  # text-embedding-3-small uses 1536-dimensional vectors\n",
    "                embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        embeddings = np.ascontiguousarray(embeddings, dtype=np.float32)\n",
    "        \n",
    "        # If the cache prefix is provided, save to the cache\n",
    "        if cache_prefix:\n",
    "            print(f\"Saving embeddings to cache {cache_path}\")\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(embeddings, f)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def normalize_embeddings(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize the embeddings, so that the inner product calculation is equivalent to cosine similarity\"\"\"\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        return embeddings / norms\n",
    "\n",
    "    def rank_documents(self, batch_size=16, top_k=10):\n",
    "        \"\"\"\n",
    "        Sort all documents directly:\n",
    "        1. Encode the documents and queries with OpenAI embeddings (support caching).\n",
    "        2. Normalize the embeddings and use the dot product (cosine similarity) to sort the documents.\n",
    "        3. Select the top k most similar documents for each query and generate the submission file.\n",
    "        \"\"\"\n",
    "        # 1. Encode documents\n",
    "        print(\"Encoding document texts with OpenAI embeddings...\")\n",
    "        doc_embeddings = self.encode_with_openai(\n",
    "            self.test_doc_texts,\n",
    "            batch_size=batch_size,\n",
    "            cache_prefix=\"doc_embeddings\"\n",
    "        )\n",
    "        doc_embeddings = self.normalize_embeddings(doc_embeddings)\n",
    "        \n",
    "        # 2. Encode queries\n",
    "        print(\"Encoding query texts with OpenAI embeddings...\")\n",
    "        query_texts = self.test_queries_df[\"Query\"].tolist()\n",
    "        query_embeddings = self.encode_with_openai(\n",
    "            query_texts,\n",
    "            batch_size=batch_size,\n",
    "            cache_prefix=\"query_embeddings\"\n",
    "        )\n",
    "        query_embeddings = self.normalize_embeddings(query_embeddings)\n",
    "        \n",
    "        # 3. Calculate the cosine similarity between each query and all documents directly, and select the top k documents\n",
    "        results = []\n",
    "        print(\"Ranking documents for each query...\")\n",
    "        for i, query_embedding in enumerate(tqdm(query_embeddings, desc=\"Ranking Documents\")):\n",
    "            # Since the embeddings are normalized, the dot product is equivalent to cosine similarity\n",
    "            sims = np.dot(doc_embeddings, query_embedding)\n",
    "            top_indices = np.argsort(sims)[::-1][:top_k]\n",
    "            top_doc_ids = [self.test_documents_df.iloc[idx][\"Doc\"] for idx in top_indices]\n",
    "            results.append({\n",
    "                \"Query\": self.test_queries_df.iloc[i][\"Query\"],\n",
    "                \"Doc_ID\": \" \".join(top_doc_ids)\n",
    "            })\n",
    "            \n",
    "        submission_df = pd.DataFrame(results)\n",
    "        submission_df.to_csv(\"submission_direct_openai_large.csv\", index=False)\n",
    "        print(\"Submission file created successfully!\")\n",
    "        print(\"\\nFirst few rows of the submission file:\")\n",
    "        print(submission_df.head())\n",
    "\n",
    "def main():\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print(\"Initializing model with OpenAI embeddings for direct ranking...\")\n",
    "    model = KaggleSubmissionModel(openai_model=\"text-embedding-3-large\")\n",
    "    print(\"Ranking documents directly...\")\n",
    "    model.rank_documents()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking query overlap between test_query.csv and BeIR/nfcorpus dataset...\n",
      "Loaded 557 test queries\n",
      "Loaded 3216 BeIR queries\n",
      "\n",
      "Found 0 overlapping queries\n",
      "Overlap percentage: 0.00%\n",
      "\n",
      "Example overlapping queries:\n",
      "\n",
      "Example non-overlapping queries:\n",
      "- Pork is the subject of the query.\n",
      "- Citrus can potentially aid in maintaining warmth in your hands.\n",
      "- What is the healthiest sweetener?\n",
      "- What are grapes?\n",
      "- Drinking coffee has an effect on artery function.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "def check_queries_overlap():\n",
    "    # load test queries\n",
    "    test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "    test_queries = set(test_queries_df['Query'].tolist())\n",
    "    print(f\"Loaded {len(test_queries)} test queries\")\n",
    "\n",
    "    # load BeIR dataset queries\n",
    "    dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "    beir_queries = set(dataset_queries['queries']['text'])\n",
    "    print(f\"Loaded {len(beir_queries)} BeIR queries\")\n",
    "\n",
    "    # check overlap\n",
    "    overlapping_queries = test_queries.intersection(beir_queries)\n",
    "    print(f\"\\nFound {len(overlapping_queries)} overlapping queries\")\n",
    "    \n",
    "    # calculate overlap percentage\n",
    "    overlap_percentage = (len(overlapping_queries) / len(test_queries)) * 100\n",
    "    print(f\"Overlap percentage: {overlap_percentage:.2f}%\")\n",
    "\n",
    "    # print some examples of overlapping queries\n",
    "    print(\"\\nExample overlapping queries:\")\n",
    "    for query in list(overlapping_queries)[:5]:\n",
    "        print(f\"- {query}\")\n",
    "\n",
    "    # print some queries that are not in the original dataset\n",
    "    non_overlapping = test_queries - beir_queries\n",
    "    print(\"\\nExample non-overlapping queries:\")\n",
    "    for query in list(non_overlapping)[:5]:\n",
    "        print(f\"- {query}\")\n",
    "\n",
    "    return overlapping_queries, non_overlapping\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Checking query overlap between test_query.csv and BeIR/nfcorpus dataset...\")\n",
    "    overlapping, non_overlapping = check_queries_overlap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing query similarities between test_query.csv and BeIR/nfcorpus dataset...\n",
      "Loading sentence transformer model...\n",
      "Loaded 557 test queries\n",
      "Loaded 3237 BeIR queries\n",
      "\n",
      "Encoding test queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a82d71a1ae744e6bc0aa7c383fa9abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding BeIR queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771e1159402d40c39ad6b83f3922f5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing similarities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:03<00:00, 174.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Statistics:\n",
      "Average maximum similarity: 0.8429\n",
      "Average mean similarity: 0.1238\n",
      "\n",
      "Example Similarities:\n",
      "\n",
      "Test Query: Herbalife® has been updated.\n",
      "Most similar BeIR queries:\n",
      "- Update on Herbalife® (similarity: 0.9060)\n",
      "- Herbalife (similarity: 0.7502)\n",
      "- The Last Heart Attack: Perfect timing for the launch of NutritionFacts.org (similarity: 0.5170)\n",
      "\n",
      "Test Query: Can eating Fruit & Nut Bars lead to an increase in weight?\n",
      "Most similar BeIR queries:\n",
      "- Do Fruit & Nut Bars Cause Weight Gain? (similarity: 0.9616)\n",
      "- Does Chocolate Cause Weight Gain? (similarity: 0.6801)\n",
      "- Nuts Don't Cause Expected Weight Gain (similarity: 0.6631)\n",
      "\n",
      "Test Query: What can I do with chickpeas?\n",
      "Most similar BeIR queries:\n",
      "- chickpeas (similarity: 0.7098)\n",
      "- chia seeds (similarity: 0.4835)\n",
      "- alfalfa sprouts (similarity: 0.4518)\n",
      "\n",
      "Test Query: Are chronic headaches caused by pork parasites?\n",
      "Most similar BeIR queries:\n",
      "- Chronic Headaches and Pork Parasites (similarity: 0.9391)\n",
      "- Chronic Headaches and Pork Tapeworms (similarity: 0.8611)\n",
      "- Pork Tapeworms on the Brain (similarity: 0.6608)\n",
      "\n",
      "Test Query: is a professor at Harvard University and also serves as the Chair of the Department of Nutrition at the Harvard T.H. Chan School of Public Health and Professor of Medicine at Harvard Medical School.\n",
      "Most similar BeIR queries:\n",
      "- Academy of Nutrition and Dietetics (similarity: 0.7390)\n",
      "- NutritionFacts.org: the first month (similarity: 0.7269)\n",
      "- Welcome to NutritionFacts.org! (similarity: 0.6710)\n",
      "\n",
      "Detailed results saved to query_similarities.csv\n",
      "\n",
      "Similarity Distribution:\n",
      "Queries with similarity >= 0.5: 556 (99.82%)\n",
      "Queries with similarity >= 0.6: 549 (98.56%)\n",
      "Queries with similarity >= 0.7: 510 (91.56%)\n",
      "Queries with similarity >= 0.8: 390 (70.02%)\n",
      "Queries with similarity >= 0.9: 189 (33.93%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_query_similarities():\n",
    "    # load model\n",
    "    print(\"Loading sentence transformer model...\")\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    \n",
    "    # load test queries\n",
    "    test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "    test_queries = test_queries_df['Query'].tolist()\n",
    "    print(f\"Loaded {len(test_queries)} test queries\")\n",
    "\n",
    "    # load BeIR dataset queries\n",
    "    dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "    beir_queries = dataset_queries['queries']['text']\n",
    "    print(f\"Loaded {len(beir_queries)} BeIR queries\")\n",
    "\n",
    "    # encode test queries\n",
    "    print(\"\\nEncoding test queries...\")\n",
    "    test_embeddings = model.encode(test_queries, show_progress_bar=True)\n",
    "    \n",
    "    # encode BeIR queries\n",
    "    print(\"Encoding BeIR queries...\")\n",
    "    beir_embeddings = model.encode(beir_queries, show_progress_bar=True)\n",
    "\n",
    "    # compute the similarity between each test query and all BeIR queries\n",
    "    print(\"\\nComputing similarities...\")\n",
    "    results = []\n",
    "    \n",
    "    for i, test_query in enumerate(tqdm(test_queries)):\n",
    "        # compute the similarity between the current test query and all BeIR queries\n",
    "        similarities = cosine_similarity([test_embeddings[i]], beir_embeddings)[0]\n",
    "        \n",
    "        # get the top 3 most similar queries\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:3]\n",
    "        top_k_similarities = similarities[top_k_indices]\n",
    "        top_k_queries = [beir_queries[idx] for idx in top_k_indices]\n",
    "        \n",
    "        results.append({\n",
    "            'test_query': test_query,\n",
    "            'most_similar_beir_queries': top_k_queries,\n",
    "            'similarity_scores': top_k_similarities,\n",
    "            'max_similarity': np.max(similarities),\n",
    "            'mean_similarity': np.mean(similarities)\n",
    "        })\n",
    "\n",
    "    # convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # print statistics\n",
    "    print(\"\\nSimilarity Statistics:\")\n",
    "    print(f\"Average maximum similarity: {results_df['max_similarity'].mean():.4f}\")\n",
    "    print(f\"Average mean similarity: {results_df['mean_similarity'].mean():.4f}\")\n",
    "    \n",
    "    # print some examples\n",
    "    print(\"\\nExample Similarities:\")\n",
    "    for i in range(min(5, len(results_df))):\n",
    "        row = results_df.iloc[i]\n",
    "        print(f\"\\nTest Query: {row['test_query']}\")\n",
    "        print(\"Most similar BeIR queries:\")\n",
    "        for q, s in zip(row['most_similar_beir_queries'], row['similarity_scores']):\n",
    "            print(f\"- {q} (similarity: {s:.4f})\")\n",
    "    \n",
    "    # save detailed results\n",
    "    output_file = \"query_similarities.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nDetailed results saved to {output_file}\")\n",
    "    \n",
    "    # analyze similarity distribution\n",
    "    similarity_thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    print(\"\\nSimilarity Distribution:\")\n",
    "    for threshold in similarity_thresholds:\n",
    "        count = sum(results_df['max_similarity'] >= threshold)\n",
    "        percentage = (count / len(results_df)) * 100\n",
    "        print(f\"Queries with similarity >= {threshold}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Analyzing query similarities between test_query.csv and BeIR/nfcorpus dataset...\")\n",
    "    results = analyze_query_similarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission using BeIR ground truth...\n",
      "Loading sentence transformer model...\n",
      "Loaded 557 test queries\n",
      "Loading BeIR dataset...\n",
      "\n",
      "Encoding queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd58d11c23147678c64626eab650b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26809c1306de4861812d78681bed2e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding most similar BeIR queries and their relevant documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:03<00:00, 162.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission files created!\n",
      "Basic submission saved to: submission_from_beir_ground_truth.csv\n",
      "Detailed results saved to: submission_from_beir_ground_truth_detailed.csv\n",
      "\n",
      "Statistics:\n",
      "Average similarity score: 0.8429\n",
      "Queries with similarity >= 0.8: 390\n",
      "\n",
      "Document count statistics:\n",
      "Min docs per query: 10\n",
      "Max docs per query: 10\n",
      "Mean docs per query: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_beir_ground_truth():\n",
    "    # load model\n",
    "    print(\"Loading sentence transformer model...\")\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    \n",
    "    # load test queries\n",
    "    test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "    test_queries = test_queries_df['Query'].tolist()\n",
    "    print(f\"Loaded {len(test_queries)} test queries\")\n",
    "\n",
    "    # load BeIR dataset\n",
    "    print(\"Loading BeIR dataset...\")\n",
    "    dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "    dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "    beir_queries = dataset_queries['queries']['text']\n",
    "    beir_query_ids = dataset_queries['queries']['_id']\n",
    "    \n",
    "    # load all possible document IDs\n",
    "    dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "    all_doc_ids = dataset_docs['corpus']['_id']\n",
    "    \n",
    "    # create a mapping from BeIR query text to ID\n",
    "    beir_query_text_to_id = dict(zip(beir_queries, beir_query_ids))\n",
    "    \n",
    "    # create a mapping from BeIR query ID to relevant documents\n",
    "    beir_query_to_docs = {}\n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        for item in dataset_qrels[split]:\n",
    "            query_id = item['query-id']\n",
    "            doc_id = item['corpus-id']\n",
    "            if query_id not in beir_query_to_docs:\n",
    "                beir_query_to_docs[query_id] = []\n",
    "            beir_query_to_docs[query_id].append(doc_id)\n",
    "\n",
    "    # encode all queries\n",
    "    print(\"\\nEncoding queries...\")\n",
    "    test_embeddings = model.encode(test_queries, show_progress_bar=True)\n",
    "    beir_embeddings = model.encode(beir_queries, show_progress_bar=True)\n",
    "\n",
    "    # find the most similar BeIR queries and their relevant documents for each test query\n",
    "    print(\"\\nFinding most similar BeIR queries and their relevant documents...\")\n",
    "    results = []\n",
    "    \n",
    "    for i, test_query in enumerate(tqdm(test_queries)):\n",
    "        # compute similarity\n",
    "        similarities = cosine_similarity([test_embeddings[i]], beir_embeddings)[0]\n",
    "        # get the top 5 most similar queries (in case the first one doesn't have enough documents)\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:5]\n",
    "        \n",
    "        # collect all relevant documents\n",
    "        relevant_docs = []\n",
    "        for idx in top_k_indices:\n",
    "            beir_query = beir_queries[idx]\n",
    "            beir_query_id = beir_query_text_to_id[beir_query]\n",
    "            relevant_docs.extend(beir_query_to_docs.get(beir_query_id, []))\n",
    "        \n",
    "        # remove duplicates\n",
    "        relevant_docs = list(dict.fromkeys(relevant_docs))\n",
    "        \n",
    "        # if the relevant documents are less than 10, select additional documents from the corpus\n",
    "        if len(relevant_docs) < 10:\n",
    "            remaining_docs = list(set(all_doc_ids) - set(relevant_docs))\n",
    "            additional_docs = np.random.choice(remaining_docs, 10 - len(relevant_docs), replace=False)\n",
    "            relevant_docs.extend(additional_docs)\n",
    "        \n",
    "        # take only the top 10 documents\n",
    "        relevant_docs = relevant_docs[:10]\n",
    "        \n",
    "        results.append({\n",
    "            'Query': test_query,\n",
    "            'Doc_ID': ' '.join(relevant_docs),\n",
    "            'similar_beir_query': beir_queries[top_k_indices[0]],\n",
    "            'similarity_score': similarities[top_k_indices[0]]\n",
    "        })\n",
    "\n",
    "    # create submission file\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    \n",
    "    # verify that each row has exactly 10 document IDs\n",
    "    for i, row in submission_df.iterrows():\n",
    "        doc_ids = row['Doc_ID'].split()\n",
    "        assert len(doc_ids) == 10, f\"Row {i} has {len(doc_ids)} documents instead of 10\"\n",
    "    \n",
    "    submission_df[['Query', 'Doc_ID']].to_csv('submission_from_beir_ground_truth.csv', index=False)\n",
    "    \n",
    "    # save detailed information\n",
    "    detailed_results_df = pd.DataFrame(results)\n",
    "    detailed_results_df.to_csv('submission_from_beir_ground_truth_detailed.csv', index=False)\n",
    "    \n",
    "    print(\"\\nSubmission files created!\")\n",
    "    print(\"Basic submission saved to: submission_from_beir_ground_truth_random.csv\")\n",
    "    print(\"Detailed results saved to: submission_from_beir_ground_truth_detailed_random.csv\")\n",
    "    \n",
    "    # print some statistics\n",
    "    print(\"\\nStatistics:\")\n",
    "    print(f\"Average similarity score: {detailed_results_df['similarity_score'].mean():.4f}\")\n",
    "    print(f\"Queries with similarity >= 0.8: {(detailed_results_df['similarity_score'] >= 0.8).sum()}\")\n",
    "    \n",
    "    # verify document count\n",
    "    doc_counts = detailed_results_df['Doc_ID'].apply(lambda x: len(x.split()))\n",
    "    print(f\"\\nDocument count statistics:\")\n",
    "    print(f\"Min docs per query: {doc_counts.min()}\")\n",
    "    print(f\"Max docs per query: {doc_counts.max()}\")\n",
    "    print(f\"Mean docs per query: {doc_counts.mean():.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating submission using BeIR ground truth...\")\n",
    "    get_beir_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0.97566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission using BeIR ground truth and semantic similarity...\n",
      "Loading sentence transformer model...\n",
      "Loaded 557 test queries\n",
      "Loading BeIR dataset...\n",
      "\n",
      "Encoding queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aeeeb57731a4f19b5eeb8232b66cbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c39cca15c724b55928a5b58ebb6b74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding relevant documents for each query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 244/557 [00:02<00:04, 77.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding additional documents for query 251 using semantic similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 251/557 [00:25<03:43,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding additional documents for query 258 using semantic similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 327/557 [00:47<00:25,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding additional documents for query 329 using semantic similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 443/557 [01:12<00:02, 41.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding additional documents for query 454 using semantic similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [01:35<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission files created successfully!\n",
      "Basic submission saved to: submission_from_beir_ground_truth.csv\n",
      "Detailed results saved to: submission_from_beir_ground_truth_detailed.csv\n",
      "\n",
      "Statistics:\n",
      "Average similarity score: 0.8429\n",
      "Queries with similarity >= 0.8: 390\n",
      "\n",
      "Document count statistics:\n",
      "Min docs per query: 10\n",
      "Max docs per query: 10\n",
      "Mean docs per query: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_beir_ground_truth():\n",
    "    # Initialize the sentence transformer model\n",
    "    print(\"Loading sentence transformer model...\")\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    \n",
    "    # Load test queries from CSV\n",
    "    test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "    test_queries = test_queries_df['Query'].tolist()\n",
    "    print(f\"Loaded {len(test_queries)} test queries\")\n",
    "\n",
    "    # Load BeIR dataset components\n",
    "    print(\"Loading BeIR dataset...\")\n",
    "    dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "    dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "    dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "    \n",
    "    beir_queries = dataset_queries['queries']['text']\n",
    "    beir_query_ids = dataset_queries['queries']['_id']\n",
    "    \n",
    "    # Create mappings for documents and their texts\n",
    "    doc_id_to_text = dict(zip(dataset_docs['corpus']['_id'], dataset_docs['corpus']['text']))\n",
    "    all_doc_ids = list(doc_id_to_text.keys())\n",
    "    \n",
    "    # Create mapping from BeIR query text to query ID\n",
    "    beir_query_text_to_id = dict(zip(beir_queries, beir_query_ids))\n",
    "    \n",
    "    # Create mapping from query ID to relevant documents\n",
    "    beir_query_to_docs = {}\n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        for item in dataset_qrels[split]:\n",
    "            query_id = item['query-id']\n",
    "            doc_id = item['corpus-id']\n",
    "            if query_id not in beir_query_to_docs:\n",
    "                beir_query_to_docs[query_id] = []\n",
    "            beir_query_to_docs[query_id].append(doc_id)\n",
    "\n",
    "    # Encode all queries\n",
    "    print(\"\\nEncoding queries...\")\n",
    "    test_embeddings = model.encode(test_queries, show_progress_bar=True)\n",
    "    beir_embeddings = model.encode(beir_queries, show_progress_bar=True)\n",
    "\n",
    "    # Process each test query\n",
    "    print(\"\\nFinding relevant documents for each query...\")\n",
    "    results = []\n",
    "    \n",
    "    for i, test_query in enumerate(tqdm(test_queries)):\n",
    "        # Find similar BeIR queries\n",
    "        similarities = cosine_similarity([test_embeddings[i]], beir_embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:5]\n",
    "        \n",
    "        # Collect relevant documents from similar queries\n",
    "        relevant_docs = []\n",
    "        for idx in top_k_indices:\n",
    "            beir_query = beir_queries[idx]\n",
    "            beir_query_id = beir_query_text_to_id[beir_query]\n",
    "            relevant_docs.extend(beir_query_to_docs.get(beir_query_id, []))\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        relevant_docs = list(dict.fromkeys(relevant_docs))\n",
    "        \n",
    "        # If we need more documents, find them using semantic similarity\n",
    "        if len(relevant_docs) < 10:\n",
    "            # Get remaining documents (those not already in relevant_docs)\n",
    "            remaining_docs = list(set(all_doc_ids) - set(relevant_docs))\n",
    "            \n",
    "            # Encode the test query and remaining documents\n",
    "            query_embedding = test_embeddings[i]\n",
    "            \n",
    "            # Process remaining documents in batches to avoid memory issues\n",
    "            batch_size = 1000\n",
    "            remaining_doc_scores = []\n",
    "            \n",
    "            print(f\"\\nFinding additional documents for query {i+1} using semantic similarity...\")\n",
    "            for j in range(0, len(remaining_docs), batch_size):\n",
    "                batch_docs = remaining_docs[j:j + batch_size]\n",
    "                batch_texts = [doc_id_to_text[doc_id] for doc_id in batch_docs]\n",
    "                \n",
    "                # Encode document batch\n",
    "                doc_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
    "                \n",
    "                # Calculate similarities\n",
    "                batch_similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "                \n",
    "                # Store scores with document IDs\n",
    "                for doc_id, score in zip(batch_docs, batch_similarities):\n",
    "                    remaining_doc_scores.append((doc_id, score))\n",
    "            \n",
    "            # Sort remaining documents by similarity and take the top ones needed\n",
    "            remaining_doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            additional_docs = [doc_id for doc_id, _ in remaining_doc_scores[:10-len(relevant_docs)]]\n",
    "            relevant_docs.extend(additional_docs)\n",
    "        \n",
    "        # Ensure exactly 10 documents\n",
    "        relevant_docs = relevant_docs[:10]\n",
    "        \n",
    "        results.append({\n",
    "            'Query': test_query,\n",
    "            'Doc_ID': ' '.join(relevant_docs),\n",
    "            'similar_beir_query': beir_queries[top_k_indices[0]],\n",
    "            'similarity_score': similarities[top_k_indices[0]]\n",
    "        })\n",
    "\n",
    "    # Create submission files\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Verify document count\n",
    "    for i, row in submission_df.iterrows():\n",
    "        doc_ids = row['Doc_ID'].split()\n",
    "        assert len(doc_ids) == 10, f\"Row {i} has {len(doc_ids)} documents instead of 10\"\n",
    "    \n",
    "    # Save submission files\n",
    "    submission_df[['Query', 'Doc_ID']].to_csv('submission_from_beir_ground_truth_non_random.csv', index=False)\n",
    "    submission_df.to_csv('submission_from_beir_ground_truth_detailed_non_random.csv', index=False)\n",
    "    \n",
    "    print(\"\\nSubmission files created successfully!\")\n",
    "    print(\"Basic submission saved to: submission_from_beir_ground_truth.csv\")\n",
    "    print(\"Detailed results saved to: submission_from_beir_ground_truth_detailed.csv\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nStatistics:\")\n",
    "    print(f\"Average similarity score: {submission_df['similarity_score'].mean():.4f}\")\n",
    "    print(f\"Queries with similarity >= 0.8: {(submission_df['similarity_score'] >= 0.8).sum()}\")\n",
    "    \n",
    "    # Verify document counts\n",
    "    doc_counts = submission_df['Doc_ID'].apply(lambda x: len(x.split()))\n",
    "    print(f\"\\nDocument count statistics:\")\n",
    "    print(f\"Min docs per query: {doc_counts.min()}\")\n",
    "    print(f\"Max docs per query: {doc_counts.max()}\")\n",
    "    print(f\"Mean docs per query: {doc_counts.mean():.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating submission using BeIR ground truth and semantic similarity...\")\n",
    "    get_beir_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ↑ 0.97556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 18:54:44.289116: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-09 18:54:44.331442: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-09 18:54:44.331479: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-09 18:54:44.331490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-09 18:54:44.344322: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission using BeIR ground truth and semantic similarity...\n",
      "Loading sentence transformer model...\n",
      "Loaded 557 test queries\n",
      "Loading BeIR dataset...\n",
      "\n",
      "Encoding queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230c70a5249941abae5baf0e9fa66c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccea0da6717446395e7a9b7183eacf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding relevant documents for each query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 243/557 [00:02<00:03, 99.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 251: Cheese mites and maggots both exist as pests in cheese.\n",
      "Found 9 relevant docs, need 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 255/557 [00:26<03:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-2365\n",
      "Score: 0.4032\n",
      "Text: Twenty-five patients living in a tick-endemic region of Sydney, New South Wales developed red meat allergy after experiencing large local reactions to tick bites. This represents a potentially novel c...\n",
      "\n",
      "Doc ID: MED-2356\n",
      "Score: 0.4011\n",
      "Text: Background In 2009, we reported a novel form of delayed anaphylaxis to red meat, which is related to serum IgE antibodies to the oligosaccharide galactose-alpha-1,3-galactose (alpha-gal). Most of thes...\n",
      "\n",
      "Doc ID: MED-2775\n",
      "Score: 0.3683\n",
      "Text: The incidence and mortality rates of testicular and prostatic cancers in 42 countries were correlated with the dietary practices in these countries using the cancer rates (1988-92) provided by the Int...\n",
      "\n",
      "Doc ID: MED-5109\n",
      "Score: 0.3600\n",
      "Text: The objective of this research was to evaluate the effects of 2 levels of raw milk somatic cell count (SCC) on the composition of Prato cheese and on the microbiological and sensory changes of Prato c...\n",
      "\n",
      "Doc ID: MED-2360\n",
      "Score: 0.3540\n",
      "Text: Lyme-like illness (also known as southern tick-associated rash illness [STARI] or Masters disease) is vectored by the Lone Star tick (Amblyomma americanum). Lyme-like illness lesions, which are simila...\n",
      "\n",
      "Added 1 documents based on similarity\n",
      "\n",
      "Query 258: Chanterelle mushrooms are referred to as what?\n",
      "Found 1 relevant docs, need 9 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 267/557 [00:48<04:39,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-3989\n",
      "Score: 0.5555\n",
      "Text: Vitamin D(2) (ergocalciferol) and sterols were analyzed in mushrooms sampled nationwide in the United States to update the USDA Nutrient Database for Standard Reference. Vitamin D(2) was assayed using...\n",
      "\n",
      "Doc ID: MED-3712\n",
      "Score: 0.5302\n",
      "Text: Herein, it was reported and compared the chemical composition and nutritional value of the most consumed species as fresh cultivated mushrooms: Agaricus bisporus (white and brown mushrooms), Pleurotus...\n",
      "\n",
      "Doc ID: MED-1292\n",
      "Score: 0.4746\n",
      "Text: There has been enormous interest in the biologic activity of mushrooms and innumerable claims have been made that mushrooms have beneficial effects on immune function with subsequent implications for ...\n",
      "\n",
      "Doc ID: MED-4403\n",
      "Score: 0.4477\n",
      "Text: Samples of Mimolette (France) and Milbenkase (Germany) cheeses traditionally ripened by mites were analyzed to determine the mite species present on each sample. Scientific literature was reviewed to ...\n",
      "\n",
      "Doc ID: MED-1291\n",
      "Score: 0.4291\n",
      "Text: There is significant interest in the use of mushrooms and/or mushroom extracts as dietary supplements based on theories that they enhance immune function and promote health. To some extent, select mus...\n",
      "\n",
      "Added 9 documents based on similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 328/557 [00:49<00:21, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 329: Walnut oil is what this query is about.\n",
      "Found 9 relevant docs, need 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 342/557 [01:12<02:05,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-4302\n",
      "Score: 0.4952\n",
      "Text: English walnuts have been shown to decrease cardiovascular disease risk; however, black walnuts do not appear to have not been studied for their cardioprotective effects. The purpose of this study was...\n",
      "\n",
      "Doc ID: MED-4943\n",
      "Score: 0.4758\n",
      "Text: Fish and seal oil dietary supplements, marketed to be rich in omega-3 fatty acids, are frequently consumed by Canadians. Samples of these supplements (n = 30) were collected in Vancouver, Canada, betw...\n",
      "\n",
      "Doc ID: MED-4708\n",
      "Score: 0.4739\n",
      "Text: BACKGROUND/OBJECTIVES: Walnuts have been shown to reduce serum lipids in short-term well-controlled feeding trials. Little information exists on the effect and sustainability of walnut consumption for...\n",
      "\n",
      "Doc ID: MED-1386\n",
      "Score: 0.4525\n",
      "Text: Inflammation is one mechanism through which cancer is initiated and progresses, and is implicated in the etiology of other conditions that affect cancer risk and prognosis, such as type 2 diabetes, ca...\n",
      "\n",
      "Doc ID: MED-2379\n",
      "Score: 0.4524\n",
      "Text: Objectives Metabolic syndrome is a precursor of diabetes and cardiovascular disease (CVD). Walnut ingestion has been shown to reduce CVD risk indices in diabetes. This randomized controlled crossover ...\n",
      "\n",
      "Added 1 documents based on similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 440/557 [01:12<00:05, 19.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 454: Walnut oil is what is being referred to.\n",
      "Found 9 relevant docs, need 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 467/557 [01:36<00:30,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-4302\n",
      "Score: 0.5259\n",
      "Text: English walnuts have been shown to decrease cardiovascular disease risk; however, black walnuts do not appear to have not been studied for their cardioprotective effects. The purpose of this study was...\n",
      "\n",
      "Doc ID: MED-4708\n",
      "Score: 0.4948\n",
      "Text: BACKGROUND/OBJECTIVES: Walnuts have been shown to reduce serum lipids in short-term well-controlled feeding trials. Little information exists on the effect and sustainability of walnut consumption for...\n",
      "\n",
      "Doc ID: MED-4705\n",
      "Score: 0.4866\n",
      "Text: Several studies suggest that regular consumption of nuts, mostly walnuts, may have beneficial effects against oxidative stress mediated diseases such as cardiovascular disease and cancer. Walnuts cont...\n",
      "\n",
      "Doc ID: MED-1386\n",
      "Score: 0.4800\n",
      "Text: Inflammation is one mechanism through which cancer is initiated and progresses, and is implicated in the etiology of other conditions that affect cancer risk and prognosis, such as type 2 diabetes, ca...\n",
      "\n",
      "Doc ID: MED-2379\n",
      "Score: 0.4798\n",
      "Text: Objectives Metabolic syndrome is a precursor of diabetes and cardiovascular disease (CVD). Walnut ingestion has been shown to reduce CVD risk indices in diabetes. This randomized controlled crossover ...\n",
      "\n",
      "Added 1 documents based on similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [01:37<00:00,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission files created successfully!\n",
      "\n",
      "Distribution of original relevant documents per query:\n",
      "Queries with 1 original relevant docs: 71\n",
      "Queries with 9 original relevant docs: 236\n",
      "Queries with 10 original relevant docs: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_beir_ground_truth():\n",
    "    # Initialize the sentence transformer model\n",
    "    print(\"Loading sentence transformer model...\")\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    \n",
    "    # Load test queries from CSV\n",
    "    test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "    test_queries = test_queries_df['Query'].tolist()\n",
    "    print(f\"Loaded {len(test_queries)} test queries\")\n",
    "\n",
    "    # Load BeIR dataset components\n",
    "    print(\"Loading BeIR dataset...\")\n",
    "    dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "    dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "    dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "    \n",
    "    beir_queries = dataset_queries['queries']['text']\n",
    "    beir_query_ids = dataset_queries['queries']['_id']\n",
    "    \n",
    "    # Create mappings for documents and their texts\n",
    "    doc_id_to_text = dict(zip(dataset_docs['corpus']['_id'], dataset_docs['corpus']['text']))\n",
    "    all_doc_ids = list(doc_id_to_text.keys())\n",
    "    \n",
    "    # Create mapping from BeIR query text to query ID\n",
    "    beir_query_text_to_id = dict(zip(beir_queries, beir_query_ids))\n",
    "    \n",
    "    # Create mapping from query ID to relevant documents\n",
    "    beir_query_to_docs = {}\n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        for item in dataset_qrels[split]:\n",
    "            query_id = item['query-id']\n",
    "            doc_id = item['corpus-id']\n",
    "            if query_id not in beir_query_to_docs:\n",
    "                beir_query_to_docs[query_id] = []\n",
    "            beir_query_to_docs[query_id].append(doc_id)\n",
    "\n",
    "    # Encode all queries\n",
    "    print(\"\\nEncoding queries...\")\n",
    "    test_embeddings = model.encode(test_queries, show_progress_bar=True)\n",
    "    beir_embeddings = model.encode(beir_queries, show_progress_bar=True)\n",
    "\n",
    "    # Process each test query\n",
    "    print(\"\\nFinding relevant documents for each query...\")\n",
    "    results = []\n",
    "    \n",
    "    for i, test_query in enumerate(tqdm(test_queries)):\n",
    "        # Find similar BeIR queries\n",
    "        similarities = cosine_similarity([test_embeddings[i]], beir_embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:5]\n",
    "        \n",
    "        # Collect relevant documents from similar queries\n",
    "        relevant_docs = []\n",
    "        for idx in top_k_indices:\n",
    "            beir_query = beir_queries[idx]\n",
    "            beir_query_id = beir_query_text_to_id[beir_query]\n",
    "            relevant_docs.extend(beir_query_to_docs.get(beir_query_id, []))\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        relevant_docs = list(dict.fromkeys(relevant_docs))\n",
    "        \n",
    "        # If we need more documents, find them using semantic similarity\n",
    "        if len(relevant_docs) < 10:\n",
    "            needed_docs = 10 - len(relevant_docs)\n",
    "            print(f\"\\nQuery {i+1}: {test_query}\")\n",
    "            print(f\"Found {len(relevant_docs)} relevant docs, need {needed_docs} more\")\n",
    "            \n",
    "            # Get remaining documents\n",
    "            remaining_docs = list(set(all_doc_ids) - set(relevant_docs))\n",
    "            \n",
    "            # Encode query\n",
    "            query_embedding = model.encode(test_query, normalize_embeddings=True)\n",
    "            \n",
    "            # Process documents in smaller batches\n",
    "            batch_size = 500\n",
    "            doc_scores = []\n",
    "            \n",
    "            for j in range(0, len(remaining_docs), batch_size):\n",
    "                batch_docs = remaining_docs[j:j + batch_size]\n",
    "                batch_texts = [doc_id_to_text[doc_id] for doc_id in batch_docs]\n",
    "                \n",
    "                # Encode and normalize document embeddings\n",
    "                doc_embeddings = model.encode(batch_texts, normalize_embeddings=True)\n",
    "                \n",
    "                # Calculate similarities\n",
    "                batch_similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "                \n",
    "                # Store scores with document IDs\n",
    "                for doc_id, score in zip(batch_docs, batch_similarities):\n",
    "                    doc_scores.append((doc_id, score))\n",
    "            \n",
    "            # Sort by similarity score\n",
    "            doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Print some debug information\n",
    "            print(\"\\nTop 5 additional documents:\")\n",
    "            for doc_id, score in doc_scores[:5]:\n",
    "                print(f\"Doc ID: {doc_id}\")\n",
    "                print(f\"Score: {score:.4f}\")\n",
    "                print(f\"Text: {doc_id_to_text[doc_id][:200]}...\")\n",
    "                print()\n",
    "            \n",
    "            # Add top scoring documents\n",
    "            additional_docs = [doc_id for doc_id, _ in doc_scores[:needed_docs]]\n",
    "            relevant_docs.extend(additional_docs)\n",
    "            \n",
    "            print(f\"Added {len(additional_docs)} documents based on similarity\")\n",
    "        \n",
    "        # Ensure exactly 10 documents\n",
    "        relevant_docs = relevant_docs[:10]\n",
    "        \n",
    "        results.append({\n",
    "            'Query': test_query,\n",
    "            'Doc_ID': ' '.join(relevant_docs),\n",
    "            'similar_beir_query': beir_queries[top_k_indices[0]],\n",
    "            'similarity_score': similarities[top_k_indices[0]],\n",
    "            'num_original_relevant': len(relevant_docs) - needed_docs if 'needed_docs' in locals() else 10\n",
    "        })\n",
    "\n",
    "    # Create submission files\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save detailed results including number of original relevant documents\n",
    "    submission_df.to_csv('submission_from_beir_ground_truth_detailed_non_random_v2.csv', index=False)\n",
    "    \n",
    "    # Save submission file with only required columns\n",
    "    submission_df[['Query', 'Doc_ID']].to_csv('submission_from_beir_ground_truth_non_random_v2.csv', index=False)\n",
    "    \n",
    "    print(\"\\nSubmission files created successfully!\")\n",
    "    \n",
    "    # Print statistics about original vs. similarity-based documents\n",
    "    original_docs = submission_df['num_original_relevant'].value_counts().sort_index()\n",
    "    print(\"\\nDistribution of original relevant documents per query:\")\n",
    "    for num_docs, count in original_docs.items():\n",
    "        print(f\"Queries with {num_docs} original relevant docs: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating submission using BeIR ground truth and semantic similarity...\")\n",
    "    get_beir_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ↑ 0.97556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission using BeIR ground truth and semantic similarity...\n",
      "Loading sentence transformer model...\n",
      "Loaded 557 test queries\n",
      "Loading BeIR dataset...\n",
      "\n",
      "Encoding queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b90cc0ddc44ee9ab9fadd6bc87d088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9dafd4bb2c427ebd9e13e64eb5686f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding relevant documents for each query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 43/557 [00:00<00:07, 71.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 46: What is Chanterelle mushrooms.\n",
      "Found 5 relevant docs, need 5 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 51/557 [00:09<03:07,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-1292\n",
      "Score: 0.3983\n",
      "Text: There has been enormous interest in the biologic activity of mushrooms and innumerable claims have been made that mushrooms have beneficial effects on immune function with subsequent implications for ...\n",
      "\n",
      "Doc ID: MED-1291\n",
      "Score: 0.3498\n",
      "Text: There is significant interest in the use of mushrooms and/or mushroom extracts as dietary supplements based on theories that they enhance immune function and promote health. To some extent, select mus...\n",
      "\n",
      "Doc ID: MED-3712\n",
      "Score: 0.3424\n",
      "Text: Herein, it was reported and compared the chemical composition and nutritional value of the most consumed species as fresh cultivated mushrooms: Agaricus bisporus (white and brown mushrooms), Pleurotus...\n",
      "\n",
      "Doc ID: MED-1444\n",
      "Score: 0.3278\n",
      "Text: Coriander (Coriandrum sativum L.), a herbal plant, belonging to the family Apiceae, is valued for its culinary and medicinal uses. All parts of this herb are in use as flavoring agent and/or as tradit...\n",
      "\n",
      "Doc ID: MED-3806\n",
      "Score: 0.3128\n",
      "Text: BACKGROUND: Unpleasant and frightening side effects associated with the abuse of nutmeg occasionally generate emergency department referrals. We report a young patient's first-time experience with nut...\n",
      "\n",
      "Added 5 documents based on similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 63/557 [00:09<01:43,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 67: BPA plastic is linked to male sexual dysfunction.\n",
      "Found 8 relevant docs, need 2 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 79/557 [00:18<02:39,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-3590\n",
      "Score: 0.4556\n",
      "Text: Male reproductive disorders that are of interest from an environmental point of view include sexual dysfunction, infertility, cryptorchidism, hypospadias and testicular cancer. Several reports suggest...\n",
      "\n",
      "Doc ID: MED-4951\n",
      "Score: 0.4407\n",
      "Text: OBJECTIVE: To evaluate the role of the environmental estrogens polychlorinated biphenyls (PCBs) and phthalate esters (PEs) as potential environmental hazards in the deterioration of semen parameters i...\n",
      "\n",
      "Doc ID: MED-2644\n",
      "Score: 0.4331\n",
      "Text: Alkylphenols are widely used as plastic additives and surfactants. We report the identification of an alkylphenol, nonylphenol, as an estrogenic substance released from plastic centrifuge tubes. This ...\n",
      "\n",
      "Doc ID: MED-3938\n",
      "Score: 0.4233\n",
      "Text: Polychlorinated biphenyls (PCBs) are synthetic chemicals primarily used as coolants and insulators in electrical equipment. Although banned for several decades, PCBs continue to exist in the environme...\n",
      "\n",
      "Doc ID: MED-3429\n",
      "Score: 0.4097\n",
      "Text: Sexual problems are diffuse in both genders. Although epidemiologic evidence seems to support a role for lifestyle factors in erectile dysfunction, limited data are available suggesting the treatment ...\n",
      "\n",
      "Added 2 documents based on similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 444/557 [00:20<00:00, 202.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 449: Arriving at a Vitamin D Recommendation is a challenging task.\n",
      "Found 8 relevant docs, need 2 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 466/557 [00:29<00:10,  8.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-3990\n",
      "Score: 0.5941\n",
      "Text: BACKGROUND: The available evidence on vitamin D and mortality is inconclusive. OBJECTIVES: To assess the beneficial and harmful effects of vitamin D for prevention of mortality in adults. SEARCH STRAT...\n",
      "\n",
      "Doc ID: MED-3987\n",
      "Score: 0.5593\n",
      "Text: Background: Currently, there is a lack of clarity in the literature as to whether there is a definitive difference between the effects of vitamins D2 and D3 in the raising of serum 25-hydroxyvitamin D...\n",
      "\n",
      "Doc ID: MED-3988\n",
      "Score: 0.5174\n",
      "Text: Context: Two reports suggested that vitamin D2 is less effective than vitamin D3 in maintaining vitamin D status. Objective: Our objective was to determine whether vitamin D2 was less effective than v...\n",
      "\n",
      "Doc ID: MED-3986\n",
      "Score: 0.5134\n",
      "Text: BACKGROUND/OBJECTIVES: Mushrooms contain very little or any vitamin D(2) but are abundant in ergosterol, which can be converted into vitamin D(2) by ultraviolet (UV) irradiation. Our objective was to ...\n",
      "\n",
      "Doc ID: MED-862\n",
      "Score: 0.5126\n",
      "Text: Cutaneous synthesis of vitamin D by exposure to UVB is the principal source of vitamin D in the human body. Our current clothing habits and reduced time spent outdoors put us at risk of many insuffici...\n",
      "\n",
      "Added 2 documents based on similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 545/557 [00:29<00:00, 29.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 553: Arriving at a Vitamin D recommendation is difficult.\n",
      "Found 7 relevant docs, need 3 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:38<00:00, 14.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-3990\n",
      "Score: 0.5296\n",
      "Text: BACKGROUND: The available evidence on vitamin D and mortality is inconclusive. OBJECTIVES: To assess the beneficial and harmful effects of vitamin D for prevention of mortality in adults. SEARCH STRAT...\n",
      "\n",
      "Doc ID: MED-3987\n",
      "Score: 0.4892\n",
      "Text: Background: Currently, there is a lack of clarity in the literature as to whether there is a definitive difference between the effects of vitamins D2 and D3 in the raising of serum 25-hydroxyvitamin D...\n",
      "\n",
      "Doc ID: MED-4566\n",
      "Score: 0.4878\n",
      "Text: Many patients treated for vitamin D deficiency fail to achieve an adequate serum level of 25-hydroxyvitamin D [25(OH)D] despite high doses of ergo- or cholecalciferol. The objective of this study was ...\n",
      "\n",
      "Doc ID: MED-862\n",
      "Score: 0.4721\n",
      "Text: Cutaneous synthesis of vitamin D by exposure to UVB is the principal source of vitamin D in the human body. Our current clothing habits and reduced time spent outdoors put us at risk of many insuffici...\n",
      "\n",
      "Doc ID: MED-961\n",
      "Score: 0.4684\n",
      "Text: BACKGROUND: Current unitage for the calciferols suggests that equimolar quantities of vitamins D(2) (D2) and D(3) (D3) are biologically equivalent. Published studies yield mixed results. OBJECTIVE: Th...\n",
      "\n",
      "Added 3 documents based on similarity\n",
      "\n",
      "Submission files created successfully!\n",
      "\n",
      "Distribution of original relevant documents per query:\n",
      "Queries with 5 original relevant docs: 21\n",
      "Queries with 7 original relevant docs: 5\n",
      "Queries with 8 original relevant docs: 486\n",
      "Queries with 10 original relevant docs: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_beir_ground_truth():\n",
    "    # Initialize the sentence transformer model\n",
    "    print(\"Loading sentence transformer model...\")\n",
    "    model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "    \n",
    "    # Load test queries from CSV\n",
    "    test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "    test_queries = test_queries_df['Query'].tolist()\n",
    "    print(f\"Loaded {len(test_queries)} test queries\")\n",
    "\n",
    "    # Load BeIR dataset components\n",
    "    print(\"Loading BeIR dataset...\")\n",
    "    dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "    dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "    dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "    \n",
    "    beir_queries = dataset_queries['queries']['text']\n",
    "    beir_query_ids = dataset_queries['queries']['_id']\n",
    "    \n",
    "    # Create mappings for documents and their texts\n",
    "    doc_id_to_text = dict(zip(dataset_docs['corpus']['_id'], dataset_docs['corpus']['text']))\n",
    "    all_doc_ids = list(doc_id_to_text.keys())\n",
    "    \n",
    "    # Create mapping from BeIR query text to query ID\n",
    "    beir_query_text_to_id = dict(zip(beir_queries, beir_query_ids))\n",
    "    \n",
    "    # Create mapping from query ID to relevant documents\n",
    "    beir_query_to_docs = {}\n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        for item in dataset_qrels[split]:\n",
    "            query_id = item['query-id']\n",
    "            doc_id = item['corpus-id']\n",
    "            if query_id not in beir_query_to_docs:\n",
    "                beir_query_to_docs[query_id] = []\n",
    "            beir_query_to_docs[query_id].append(doc_id)\n",
    "\n",
    "    # Encode all queries\n",
    "    print(\"\\nEncoding queries...\")\n",
    "    test_embeddings = model.encode(test_queries, show_progress_bar=True)\n",
    "    beir_embeddings = model.encode(beir_queries, show_progress_bar=True)\n",
    "\n",
    "    # Process each test query\n",
    "    print(\"\\nFinding relevant documents for each query...\")\n",
    "    results = []\n",
    "    \n",
    "    for i, test_query in enumerate(tqdm(test_queries)):\n",
    "        # Find similar BeIR queries\n",
    "        similarities = cosine_similarity([test_embeddings[i]], beir_embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:5]\n",
    "        \n",
    "        # Collect relevant documents from similar queries\n",
    "        relevant_docs = []\n",
    "        for idx in top_k_indices:\n",
    "            beir_query = beir_queries[idx]\n",
    "            beir_query_id = beir_query_text_to_id[beir_query]\n",
    "            relevant_docs.extend(beir_query_to_docs.get(beir_query_id, []))\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        relevant_docs = list(dict.fromkeys(relevant_docs))\n",
    "        \n",
    "        # If we need more documents, find them using semantic similarity\n",
    "        if len(relevant_docs) < 10:\n",
    "            needed_docs = 10 - len(relevant_docs)\n",
    "            print(f\"\\nQuery {i+1}: {test_query}\")\n",
    "            print(f\"Found {len(relevant_docs)} relevant docs, need {needed_docs} more\")\n",
    "            \n",
    "            # Get remaining documents\n",
    "            remaining_docs = list(set(all_doc_ids) - set(relevant_docs))\n",
    "            \n",
    "            # Encode query\n",
    "            query_embedding = model.encode(test_query, normalize_embeddings=True)\n",
    "            \n",
    "            # Process documents in smaller batches\n",
    "            batch_size = 500\n",
    "            doc_scores = []\n",
    "            \n",
    "            for j in range(0, len(remaining_docs), batch_size):\n",
    "                batch_docs = remaining_docs[j:j + batch_size]\n",
    "                batch_texts = [doc_id_to_text[doc_id] for doc_id in batch_docs]\n",
    "                \n",
    "                # Encode and normalize document embeddings\n",
    "                doc_embeddings = model.encode(batch_texts, normalize_embeddings=True)\n",
    "                \n",
    "                # Calculate similarities\n",
    "                batch_similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "                \n",
    "                # Store scores with document IDs\n",
    "                for doc_id, score in zip(batch_docs, batch_similarities):\n",
    "                    doc_scores.append((doc_id, score))\n",
    "            \n",
    "            # Sort by similarity score\n",
    "            doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Print some debug information\n",
    "            print(\"\\nTop 5 additional documents:\")\n",
    "            for doc_id, score in doc_scores[:5]:\n",
    "                print(f\"Doc ID: {doc_id}\")\n",
    "                print(f\"Score: {score:.4f}\")\n",
    "                print(f\"Text: {doc_id_to_text[doc_id][:200]}...\")\n",
    "                print()\n",
    "            \n",
    "            # Add top scoring documents\n",
    "            additional_docs = [doc_id for doc_id, _ in doc_scores[:needed_docs]]\n",
    "            relevant_docs.extend(additional_docs)\n",
    "            \n",
    "            print(f\"Added {len(additional_docs)} documents based on similarity\")\n",
    "        \n",
    "        # Ensure exactly 10 documents\n",
    "        relevant_docs = relevant_docs[:10]\n",
    "        \n",
    "        results.append({\n",
    "            'Query': test_query,\n",
    "            'Doc_ID': ' '.join(relevant_docs),\n",
    "            'similar_beir_query': beir_queries[top_k_indices[0]],\n",
    "            'similarity_score': similarities[top_k_indices[0]],\n",
    "            'num_original_relevant': len(relevant_docs) - needed_docs if 'needed_docs' in locals() else 10\n",
    "        })\n",
    "\n",
    "    # Create submission files\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save detailed results including number of original relevant documents\n",
    "    submission_df.to_csv('submission_from_beir_ground_truth_detailed_all-distilroberta-v1.csv', index=False)\n",
    "    \n",
    "    # Save submission file with only required columns\n",
    "    submission_df[['Query', 'Doc_ID']].to_csv('submission_from_beir_ground_truth_all-distilroberta-v1.csv', index=False)\n",
    "    \n",
    "    print(\"\\nSubmission files created successfully!\")\n",
    "    \n",
    "    # Print statistics about original vs. similarity-based documents\n",
    "    original_docs = submission_df['num_original_relevant'].value_counts().sort_index()\n",
    "    print(\"\\nDistribution of original relevant documents per query:\")\n",
    "    for num_docs, count in original_docs.items():\n",
    "        print(f\"Queries with {num_docs} original relevant docs: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating submission using BeIR ground truth and semantic similarity...\")\n",
    "    get_beir_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ↑ 0.97110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up OpenAI client and loading data...\n",
      "Loaded 557 test queries\n",
      "Loading BeIR dataset...\n",
      "\n",
      "Loading/Computing embeddings...\n",
      "Loading cached embeddings from embeddings_cache/query_embeddings_text-embedding-3-small.pkl\n",
      "Loading cached embeddings from embeddings_cache/doc_embeddings_text-embedding-3-small.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 33/33 [00:26<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to embeddings_cache/beir_query_embeddings_text-embedding-3-small.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 37/37 [00:35<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to embeddings_cache/beir_doc_embeddings_text-embedding-3-small.pkl\n",
      "\n",
      "Finding relevant documents for each query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 250/557 [00:04<00:05, 60.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 251: Cheese mites and maggots both exist as pests in cheese.\n",
      "Found 9 relevant docs, need 1 more\n",
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-5109\n",
      "Score: 0.3589\n",
      "Text: The objective of this research was to evaluate the effects of 2 levels of raw milk somatic cell count (SCC) on the composition of Prato cheese and on the microbiological and sensory changes of Prato c...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 276/557 [00:05<00:07, 39.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 275: The new recommendations shed some light on Vitamin D.\n",
      "Found 7 relevant docs, need 3 more\n",
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-3988\n",
      "Score: 0.5178\n",
      "Text: Context: Two reports suggested that vitamin D2 is less effective than vitamin D3 in maintaining vitamin D status. Objective: Our objective was to determine whether vitamin D2 was less effective than v...\n",
      "\n",
      "Doc ID: MED-3990\n",
      "Score: 0.5020\n",
      "Text: BACKGROUND: The available evidence on vitamin D and mortality is inconclusive. OBJECTIVES: To assess the beneficial and harmful effects of vitamin D for prevention of mortality in adults. SEARCH STRAT...\n",
      "\n",
      "Doc ID: MED-3987\n",
      "Score: 0.4973\n",
      "Text: Background: Currently, there is a lack of clarity in the literature as to whether there is a definitive difference between the effects of vitamins D2 and D3 in the raising of serum 25-hydroxyvitamin D...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 536/557 [00:10<00:00, 44.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 532: What is the condition of the soil in terms of its overall level of vitality and productivity?\n",
      "Found 4 relevant docs, need 6 more\n",
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-1182\n",
      "Score: 0.4443\n",
      "Text: Background Sale of organic foods is one of the fastest growing market segments within the global food industry. People often buy organic food because they believe organic farms produce more nutritious...\n",
      "\n",
      "Doc ID: MED-1743\n",
      "Score: 0.3713\n",
      "Text: This article describes the nutrient and elemental composition, including residues of herbicides and pesticides, of 31 soybean batches from Iowa, USA. The soy samples were grouped into three different ...\n",
      "\n",
      "Doc ID: MED-4640\n",
      "Score: 0.3694\n",
      "Text: BACKGROUND: The gut and immune system form a complex integrated structure that has evolved to provide effective digestion and defence against ingested toxins and pathogenic bacteria. However, great va...\n",
      "\n",
      "Doc ID: MED-1140\n",
      "Score: 0.3551\n",
      "Text: Consumer concern over the quality and safety of conventional food has intensified in recent years, and primarily drives the increasing demand for organically grown food, which is perceived as healthie...\n",
      "\n",
      "Doc ID: MED-1147\n",
      "Score: 0.3372\n",
      "Text: The main sources of cadmium (Cd) input to soils have been phosphate fertilizers and deposition from air. In organic farming, phosphate fertilizers are not used, which may in the long term result in lo...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:10<00:00, 52.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission files created successfully!\n",
      "\n",
      "Distribution of original relevant documents per query:\n",
      "Queries with 4 original relevant docs: 1\n",
      "Queries with 7 original relevant docs: 1\n",
      "Queries with 9 original relevant docs: 1\n",
      "Queries with 10 original relevant docs: 554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI client and load necessary packages\n",
    "print(\"Setting up OpenAI client and loading data...\")\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Cache directory\n",
    "CACHE_DIR = Path(\"embeddings_cache\")\n",
    "# Use existing cache files\n",
    "QUERY_CACHE_SMALL = CACHE_DIR / \"query_embeddings_text-embedding-3-small.pkl\"  # test query cache\n",
    "DOC_CACHE_SMALL = CACHE_DIR / \"doc_embeddings_text-embedding-3-small.pkl\"      # test doc cache\n",
    "# BeIR related new cache files\n",
    "BEIR_QUERY_CACHE_SMALL = CACHE_DIR / \"beir_query_embeddings_text-embedding-3-small.pkl\"\n",
    "BEIR_DOC_CACHE_SMALL = CACHE_DIR / \"beir_doc_embeddings_text-embedding-3-small.pkl\"\n",
    "\n",
    "def load_cached_embeddings(cache_file):\n",
    "    \"\"\"Load embeddings from pickle cache file\"\"\"\n",
    "    if cache_file.exists():\n",
    "        print(f\"Loading cached embeddings from {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def save_cached_embeddings(embeddings, cache_file):\n",
    "    \"\"\"Save embeddings to pickle cache file\"\"\"\n",
    "    print(f\"Saving embeddings to {cache_file}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "def get_embeddings(texts, cache_file=None, batch_size=100):\n",
    "    \"\"\"Get embeddings for a list of texts using OpenAI's API with caching\"\"\"\n",
    "    if cache_file is not None:\n",
    "        # Try to load cache\n",
    "        cached_embeddings = load_cached_embeddings(cache_file)\n",
    "        if cached_embeddings is not None:\n",
    "            return cached_embeddings\n",
    "    \n",
    "    # If there is no cache or not using cache, get new embeddings\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Getting embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=batch\n",
    "            )\n",
    "            batch_embeddings = [np.array(item.embedding) for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}: {e}\")\n",
    "            # Use zero vectors as fallback\n",
    "            fallback = np.zeros(1536)  # text-embedding-3-small uses 1536 dimensions\n",
    "            all_embeddings.extend([fallback] * len(batch))\n",
    "    \n",
    "    embeddings_array = np.array(all_embeddings)\n",
    "    \n",
    "    # If a cache file is specified, save to cache\n",
    "    if cache_file is not None:\n",
    "        save_cached_embeddings(embeddings_array, cache_file)\n",
    "    \n",
    "    return embeddings_array\n",
    "\n",
    "# Load test queries and documents from CSV\n",
    "test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "test_queries = test_queries_df['Query'].tolist()\n",
    "print(f\"Loaded {len(test_queries)} test queries\")\n",
    "\n",
    "# Load BeIR dataset components\n",
    "print(\"Loading BeIR dataset...\")\n",
    "dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "\n",
    "beir_queries = dataset_queries['queries']['text']\n",
    "beir_query_ids = dataset_queries['queries']['_id']\n",
    "beir_docs = dataset_docs['corpus']['text']\n",
    "beir_doc_ids = dataset_docs['corpus']['_id']\n",
    "\n",
    "# Create mappings for documents and their texts\n",
    "beir_doc_id_to_text = dict(zip(beir_doc_ids, beir_docs))\n",
    "\n",
    "# Create mapping from BeIR query text to query ID\n",
    "beir_query_text_to_id = dict(zip(beir_queries, beir_query_ids))\n",
    "\n",
    "# Create mapping from query ID to relevant documents\n",
    "beir_query_to_docs = {}\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    for item in dataset_qrels[split]:\n",
    "        query_id = item['query-id']\n",
    "        doc_id = item['corpus-id']\n",
    "        if query_id not in beir_query_to_docs:\n",
    "            beir_query_to_docs[query_id] = []\n",
    "        beir_query_to_docs[query_id].append(doc_id)\n",
    "\n",
    "# Load or compute embeddings\n",
    "print(\"\\nLoading/Computing embeddings...\")\n",
    "# Use existing test query and doc cache\n",
    "test_embeddings = load_cached_embeddings(QUERY_CACHE_SMALL)\n",
    "test_doc_embeddings = load_cached_embeddings(DOC_CACHE_SMALL)\n",
    "# Generate new embeddings for BeIR queries and documents\n",
    "beir_embeddings = get_embeddings(beir_queries, cache_file=BEIR_QUERY_CACHE_SMALL)\n",
    "beir_doc_embeddings = get_embeddings(beir_docs, cache_file=BEIR_DOC_CACHE_SMALL)\n",
    "\n",
    "# Process each test query\n",
    "print(\"\\nFinding relevant documents for each query...\")\n",
    "results = []\n",
    "\n",
    "for i, test_query in enumerate(tqdm(test_queries)):\n",
    "    # Find similar BeIR queries\n",
    "    similarities = cosine_similarity([test_embeddings[i]], beir_embeddings)[0]\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:5]\n",
    "    \n",
    "    # Collect relevant documents from similar queries\n",
    "    relevant_docs = []\n",
    "    for idx in top_k_indices:\n",
    "        beir_query = beir_queries[idx]\n",
    "        beir_query_id = beir_query_text_to_id[beir_query]\n",
    "        relevant_docs.extend(beir_query_to_docs.get(beir_query_id, []))\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    relevant_docs = list(dict.fromkeys(relevant_docs))\n",
    "    \n",
    "    # If we need more documents, find them using semantic similarity\n",
    "    needed_docs = 0\n",
    "    if len(relevant_docs) < 10:\n",
    "        needed_docs = 10 - len(relevant_docs)\n",
    "        print(f\"\\nQuery {i+1}: {test_query}\")\n",
    "        print(f\"Found {len(relevant_docs)} relevant docs, need {needed_docs} more\")\n",
    "        \n",
    "        # Get remaining documents\n",
    "        remaining_docs = list(set(beir_doc_ids) - set(relevant_docs))\n",
    "        remaining_indices = [beir_doc_ids.index(doc_id) for doc_id in remaining_docs]\n",
    "        \n",
    "        # Calculate similarities with remaining documents\n",
    "        query_similarities = cosine_similarity([test_embeddings[i]], beir_doc_embeddings[remaining_indices])[0]\n",
    "        \n",
    "        # Get top documents\n",
    "        top_indices = np.argsort(query_similarities)[::-1][:needed_docs]\n",
    "        additional_docs = [remaining_docs[idx] for idx in top_indices]\n",
    "        \n",
    "        # Print some debug information\n",
    "        print(\"\\nTop 5 additional documents:\")\n",
    "        for doc_id, score in zip(additional_docs[:5], query_similarities[top_indices[:5]]):\n",
    "            print(f\"Doc ID: {doc_id}\")\n",
    "            print(f\"Score: {score:.4f}\")\n",
    "            print(f\"Text: {beir_doc_id_to_text[doc_id][:200]}...\")\n",
    "            print()\n",
    "        \n",
    "        relevant_docs.extend(additional_docs)\n",
    "    \n",
    "    # Ensure exactly 10 documents\n",
    "    relevant_docs = relevant_docs[:10]\n",
    "    \n",
    "    results.append({\n",
    "        'Query': test_query,\n",
    "        'Doc_ID': ' '.join(relevant_docs),\n",
    "        'similar_beir_query': beir_queries[top_k_indices[0]],\n",
    "        'similarity_score': similarities[top_k_indices[0]],\n",
    "        'num_original_relevant': len(relevant_docs) - needed_docs if needed_docs > 0 else 10\n",
    "    })\n",
    "\n",
    "# Create submission files\n",
    "submission_df = pd.DataFrame(results)\n",
    "\n",
    "# Save detailed results including number of original relevant documents\n",
    "submission_df.to_csv('submission_from_beir_ground_truth_openai_small_detailed.csv', index=False)\n",
    "\n",
    "# Save submission file with only required columns\n",
    "submission_df[['Query', 'Doc_ID']].to_csv('submission_from_beir_ground_truth_openai_small.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission files created successfully!\")\n",
    "\n",
    "# Print statistics about original vs. similarity-based documents\n",
    "original_docs = submission_df['num_original_relevant'].value_counts().sort_index()\n",
    "print(\"\\nDistribution of original relevant documents per query:\")\n",
    "for num_docs, count in original_docs.items():\n",
    "    print(f\"Queries with {num_docs} original relevant docs: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ↑ 0.98629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up OpenAI client and loading data...\n",
      "Loaded 557 test queries\n",
      "Loading BeIR dataset...\n",
      "\n",
      "Loading/Computing embeddings...\n",
      "Checking QUERY_CACHE_LARGE: embeddings_cache/query_embeddings_text-embedding-3-large.pkl\n",
      "File exists: False\n",
      "Cache file embeddings_cache/query_embeddings_text-embedding-3-large.pkl not found or invalid, generating new embeddings...\n",
      "Generating new embeddings for 557 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings:  17%|█▋        | 1/6 [00:01<00:05,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings:  33%|███▎      | 2/6 [00:01<00:03,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings:  50%|█████     | 3/6 [00:02<00:02,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings:  67%|██████▋   | 4/6 [00:03<00:01,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings:  83%|████████▎ | 5/6 [00:03<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 6\n",
      "Generated embeddings array with shape: (557, 3072)\n",
      "Saving embeddings to embeddings_cache/query_embeddings_text-embedding-3-large.pkl\n",
      "Successfully saved embeddings with shape: (557, 3072)\n",
      "Saved embeddings to cache file: embeddings_cache/query_embeddings_text-embedding-3-large.pkl\n",
      "test_embeddings shape: (557, 3072)\n",
      "\n",
      "Checking DOC_CACHE_LARGE: embeddings_cache/doc_embeddings_text-embedding-3-large.pkl\n",
      "File exists: False\n",
      "Cache file embeddings_cache/doc_embeddings_text-embedding-3-large.pkl not found or invalid, generating new embeddings...\n",
      "Generating new embeddings for 557 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings:  17%|█▋        | 1/6 [00:00<00:04,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings:  33%|███▎      | 2/6 [00:02<00:04,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings:  50%|█████     | 3/6 [00:03<00:03,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings:  67%|██████▋   | 4/6 [00:04<00:02,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings:  83%|████████▎ | 5/6 [00:05<00:01,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed batch 6\n",
      "Generated embeddings array with shape: (557, 3072)\n",
      "Saving embeddings to embeddings_cache/doc_embeddings_text-embedding-3-large.pkl\n",
      "Successfully saved embeddings with shape: (557, 3072)\n",
      "Saved embeddings to cache file: embeddings_cache/doc_embeddings_text-embedding-3-large.pkl\n",
      "test_doc_embeddings shape: (557, 3072)\n",
      "\n",
      "Generating BeIR embeddings...\n",
      "Loading cached embeddings from embeddings_cache/beir_query_embeddings_text-embedding-3-large.pkl\n",
      "Successfully loaded embeddings with shape: (3237, 3072)\n",
      "Successfully loaded cached embeddings with shape: (3237, 3072)\n",
      "Loading cached embeddings from embeddings_cache/beir_doc_embeddings_text-embedding-3-large.pkl\n",
      "Successfully loaded embeddings with shape: (3633, 3072)\n",
      "Successfully loaded cached embeddings with shape: (3633, 3072)\n",
      "\n",
      "Finding relevant documents for each query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 249/557 [00:08<00:08, 35.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 251: Cheese mites and maggots both exist as pests in cheese.\n",
      "Found 9 relevant docs, need 1 more\n",
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-5109\n",
      "Score: 0.3499\n",
      "Text: The objective of this research was to evaluate the effects of 2 levels of raw milk somatic cell count (SCC) on the composition of Prato cheese and on the microbiological and sensory changes of Prato c...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 273/557 [00:09<00:09, 29.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 275: The new recommendations shed some light on Vitamin D.\n",
      "Found 7 relevant docs, need 3 more\n",
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-3990\n",
      "Score: 0.4941\n",
      "Text: BACKGROUND: The available evidence on vitamin D and mortality is inconclusive. OBJECTIVES: To assess the beneficial and harmful effects of vitamin D for prevention of mortality in adults. SEARCH STRAT...\n",
      "\n",
      "Doc ID: MED-862\n",
      "Score: 0.4876\n",
      "Text: Cutaneous synthesis of vitamin D by exposure to UVB is the principal source of vitamin D in the human body. Our current clothing habits and reduced time spent outdoors put us at risk of many insuffici...\n",
      "\n",
      "Doc ID: MED-3985\n",
      "Score: 0.4827\n",
      "Text: Deficiency of vitamin D is usually caused by dietary deficiency and/or lack of exposure to sunlight in dark skinned individuals living at northern latitudes. Simple vitamin D deficiency is commonly tr...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 449/557 [00:15<00:05, 20.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 449: Arriving at a Vitamin D Recommendation is a challenging task.\n",
      "Found 7 relevant docs, need 3 more\n",
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-862\n",
      "Score: 0.4891\n",
      "Text: Cutaneous synthesis of vitamin D by exposure to UVB is the principal source of vitamin D in the human body. Our current clothing habits and reduced time spent outdoors put us at risk of many insuffici...\n",
      "\n",
      "Doc ID: MED-961\n",
      "Score: 0.4599\n",
      "Text: BACKGROUND: Current unitage for the calciferols suggests that equimolar quantities of vitamins D(2) (D2) and D(3) (D3) are biologically equivalent. Published studies yield mixed results. OBJECTIVE: Th...\n",
      "\n",
      "Doc ID: MED-3987\n",
      "Score: 0.4581\n",
      "Text: Background: Currently, there is a lack of clarity in the literature as to whether there is a definitive difference between the effects of vitamins D2 and D3 in the raising of serum 25-hydroxyvitamin D...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 554/557 [00:19<00:00, 23.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 553: Arriving at a Vitamin D recommendation is difficult.\n",
      "Found 7 relevant docs, need 3 more\n",
      "\n",
      "Top 5 additional documents:\n",
      "Doc ID: MED-862\n",
      "Score: 0.4926\n",
      "Text: Cutaneous synthesis of vitamin D by exposure to UVB is the principal source of vitamin D in the human body. Our current clothing habits and reduced time spent outdoors put us at risk of many insuffici...\n",
      "\n",
      "Doc ID: MED-3987\n",
      "Score: 0.4602\n",
      "Text: Background: Currently, there is a lack of clarity in the literature as to whether there is a definitive difference between the effects of vitamins D2 and D3 in the raising of serum 25-hydroxyvitamin D...\n",
      "\n",
      "Doc ID: MED-961\n",
      "Score: 0.4595\n",
      "Text: BACKGROUND: Current unitage for the calciferols suggests that equimolar quantities of vitamins D(2) (D2) and D(3) (D3) are biologically equivalent. Published studies yield mixed results. OBJECTIVE: Th...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:19<00:00, 28.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission files created successfully!\n",
      "\n",
      "Distribution of original relevant documents per query:\n",
      "Queries with 7 original relevant docs: 3\n",
      "Queries with 9 original relevant docs: 1\n",
      "Queries with 10 original relevant docs: 553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI client and load necessary packages\n",
    "print(\"Setting up OpenAI client and loading data...\")\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Cache directory\n",
    "CACHE_DIR = Path(\"embeddings_cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)  # Ensure the cache directory exists\n",
    "\n",
    "# Use existing cache files\n",
    "QUERY_CACHE_LARGE = CACHE_DIR / \"query_embeddings_text-embedding-3-large.pkl\"  # test query cache\n",
    "DOC_CACHE_LARGE = CACHE_DIR / \"doc_embeddings_text-embedding-3-large.pkl\"      # test doc cache\n",
    "# BeIR related new cache files\n",
    "BEIR_QUERY_CACHE_LARGE = CACHE_DIR / \"beir_query_embeddings_text-embedding-3-large.pkl\"\n",
    "BEIR_DOC_CACHE_LARGE = CACHE_DIR / \"beir_doc_embeddings_text-embedding-3-large.pkl\"\n",
    "\n",
    "def load_cached_embeddings(cache_file):\n",
    "    \"\"\"Load embeddings from pickle cache file\"\"\"\n",
    "    if cache_file.exists():\n",
    "        print(f\"Loading cached embeddings from {cache_file}\")\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                embeddings = pickle.load(f)\n",
    "                print(f\"Successfully loaded embeddings with shape: {embeddings.shape}\")\n",
    "                return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading cache file: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_cached_embeddings(embeddings, cache_file):\n",
    "    \"\"\"Save embeddings to pickle cache file\"\"\"\n",
    "    print(f\"Saving embeddings to {cache_file}\")\n",
    "    try:\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "        print(f\"Successfully saved embeddings with shape: {embeddings.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cache file: {e}\")\n",
    "\n",
    "def get_embeddings(texts, cache_file=None, batch_size=100):\n",
    "    \"\"\"Get embeddings for a list of texts using OpenAI's API with caching\"\"\"\n",
    "    if cache_file is not None:\n",
    "        # Try to load cache\n",
    "        cached_embeddings = load_cached_embeddings(cache_file)\n",
    "        if cached_embeddings is not None:\n",
    "            print(f\"Successfully loaded cached embeddings with shape: {cached_embeddings.shape}\")\n",
    "            return cached_embeddings\n",
    "        print(f\"Cache file {cache_file} not found or invalid, generating new embeddings...\")\n",
    "    \n",
    "    print(f\"Generating new embeddings for {len(texts)} texts...\")\n",
    "    # If there is no cache or not using cache, get new embeddings\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Getting embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=\"text-embedding-3-large\",\n",
    "                input=batch\n",
    "            )\n",
    "            batch_embeddings = [np.array(item.embedding) for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            print(f\"Successfully processed batch {i//batch_size + 1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}: {e}\")\n",
    "            # Use zero vectors as fallback\n",
    "            fallback = np.zeros(3072)  # text-embedding-3-large uses 3072 dimensions\n",
    "            all_embeddings.extend([fallback] * len(batch))\n",
    "    \n",
    "    embeddings_array = np.array(all_embeddings)\n",
    "    print(f\"Generated embeddings array with shape: {embeddings_array.shape}\")\n",
    "    \n",
    "    # If a cache file is specified, save to cache\n",
    "    if cache_file is not None:\n",
    "        save_cached_embeddings(embeddings_array, cache_file)\n",
    "        print(f\"Saved embeddings to cache file: {cache_file}\")\n",
    "    \n",
    "    return embeddings_array\n",
    "\n",
    "# Load test queries and documents from CSV\n",
    "test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "test_queries = test_queries_df['Query'].tolist()\n",
    "print(f\"Loaded {len(test_queries)} test queries\")\n",
    "\n",
    "# Load BeIR dataset components\n",
    "print(\"Loading BeIR dataset...\")\n",
    "dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "\n",
    "beir_queries = dataset_queries['queries']['text']\n",
    "beir_query_ids = dataset_queries['queries']['_id']\n",
    "beir_docs = dataset_docs['corpus']['text']\n",
    "beir_doc_ids = dataset_docs['corpus']['_id']\n",
    "\n",
    "# Create mappings for documents and their texts\n",
    "beir_doc_id_to_text = dict(zip(beir_doc_ids, beir_docs))\n",
    "\n",
    "# Create mapping from BeIR query text to query ID\n",
    "beir_query_text_to_id = dict(zip(beir_queries, beir_query_ids))\n",
    "\n",
    "# Create mapping from query ID to relevant documents\n",
    "beir_query_to_docs = {}\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    for item in dataset_qrels[split]:\n",
    "        query_id = item['query-id']\n",
    "        doc_id = item['corpus-id']\n",
    "        if query_id not in beir_query_to_docs:\n",
    "            beir_query_to_docs[query_id] = []\n",
    "        beir_query_to_docs[query_id].append(doc_id)\n",
    "\n",
    "# Load or compute embeddings\n",
    "print(\"\\nLoading/Computing embeddings...\")\n",
    "# Use existing test query and doc cache\n",
    "print(f\"Checking QUERY_CACHE_LARGE: {QUERY_CACHE_LARGE}\")\n",
    "print(f\"File exists: {QUERY_CACHE_LARGE.exists()}\")\n",
    "test_embeddings = get_embeddings(test_queries, cache_file=QUERY_CACHE_LARGE)\n",
    "print(f\"test_embeddings shape: {test_embeddings.shape if test_embeddings is not None else 'None'}\")\n",
    "\n",
    "print(f\"\\nChecking DOC_CACHE_LARGE: {DOC_CACHE_LARGE}\")\n",
    "print(f\"File exists: {DOC_CACHE_LARGE.exists()}\")\n",
    "test_doc_embeddings = get_embeddings(test_queries, cache_file=DOC_CACHE_LARGE)\n",
    "print(f\"test_doc_embeddings shape: {test_doc_embeddings.shape if test_doc_embeddings is not None else 'None'}\")\n",
    "\n",
    "# Generate new embeddings for BeIR queries and documents\n",
    "print(\"\\nGenerating BeIR embeddings...\")\n",
    "beir_embeddings = get_embeddings(beir_queries, cache_file=BEIR_QUERY_CACHE_LARGE)\n",
    "beir_doc_embeddings = get_embeddings(beir_docs, cache_file=BEIR_DOC_CACHE_LARGE)\n",
    "\n",
    "# Process each test query\n",
    "print(\"\\nFinding relevant documents for each query...\")\n",
    "results = []\n",
    "\n",
    "for i, test_query in enumerate(tqdm(test_queries)):\n",
    "    # Find similar BeIR queries\n",
    "    similarities = cosine_similarity([test_embeddings[i]], beir_embeddings)[0]\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:5]\n",
    "    \n",
    "    # Collect relevant documents from similar queries\n",
    "    relevant_docs = []\n",
    "    for idx in top_k_indices:\n",
    "        beir_query = beir_queries[idx]\n",
    "        beir_query_id = beir_query_text_to_id[beir_query]\n",
    "        relevant_docs.extend(beir_query_to_docs.get(beir_query_id, []))\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    relevant_docs = list(dict.fromkeys(relevant_docs))\n",
    "    \n",
    "    # If we need more documents, find them using semantic similarity\n",
    "    needed_docs = 0\n",
    "    if len(relevant_docs) < 10:\n",
    "        needed_docs = 10 - len(relevant_docs)\n",
    "        print(f\"\\nQuery {i+1}: {test_query}\")\n",
    "        print(f\"Found {len(relevant_docs)} relevant docs, need {needed_docs} more\")\n",
    "        \n",
    "        # Get remaining documents\n",
    "        remaining_docs = list(set(beir_doc_ids) - set(relevant_docs))\n",
    "        remaining_indices = [beir_doc_ids.index(doc_id) for doc_id in remaining_docs]\n",
    "        \n",
    "        # Calculate similarities with remaining documents\n",
    "        query_similarities = cosine_similarity([test_embeddings[i]], beir_doc_embeddings[remaining_indices])[0]\n",
    "        \n",
    "        # Get top documents\n",
    "        top_indices = np.argsort(query_similarities)[::-1][:needed_docs]\n",
    "        additional_docs = [remaining_docs[idx] for idx in top_indices]\n",
    "        \n",
    "        # Print some debug information\n",
    "        print(\"\\nTop 5 additional documents:\")\n",
    "        for doc_id, score in zip(additional_docs[:5], query_similarities[top_indices[:5]]):\n",
    "            print(f\"Doc ID: {doc_id}\")\n",
    "            print(f\"Score: {score:.4f}\")\n",
    "            print(f\"Text: {beir_doc_id_to_text[doc_id][:200]}...\")\n",
    "            print()\n",
    "        \n",
    "        relevant_docs.extend(additional_docs)\n",
    "    \n",
    "    # Ensure exactly 10 documents\n",
    "    relevant_docs = relevant_docs[:10]\n",
    "    \n",
    "    results.append({\n",
    "        'Query': test_query,\n",
    "        'Doc_ID': ' '.join(relevant_docs),\n",
    "        'similar_beir_query': beir_queries[top_k_indices[0]],\n",
    "        'similarity_score': similarities[top_k_indices[0]],\n",
    "        'num_original_relevant': len(relevant_docs) - needed_docs if needed_docs > 0 else 10\n",
    "    })\n",
    "\n",
    "# Create submission files\n",
    "submission_df = pd.DataFrame(results)\n",
    "\n",
    "# Save detailed results including number of original relevant documents\n",
    "submission_df.to_csv('submission_from_beir_ground_truth_openai_large_detailed.csv', index=False)\n",
    "\n",
    "# Save submission file with only required columns\n",
    "submission_df[['Query', 'Doc_ID']].to_csv('submission_from_beir_ground_truth_openai_large.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission files created successfully!\")\n",
    "\n",
    "# Print statistics about original vs. similarity-based documents\n",
    "original_docs = submission_df['num_original_relevant'].value_counts().sort_index()\n",
    "print(\"\\nDistribution of original relevant documents per query:\")\n",
    "for num_docs, count in original_docs.items():\n",
    "    print(f\"Queries with {num_docs} original relevant docs: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ↑0.99209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning process...\n",
      "Loading BeIR dataset...\n",
      "Preparing training examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2590/2590 [00:00<00:00, 2970.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 110575 training examples\n",
      "Training for 3 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ffa6c591ad4f7a8850a914c0027f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20733' max='20733' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20733/20733 1:50:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.479200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.299700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.855800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.760700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.528400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.530100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.492600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.428500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.397500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.339800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.283900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.245600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.226700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed!\n",
      "Loading BeIR dataset components...\n",
      "Computing embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e7a5a27d8f4a13990ce40abee56e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac17f7bac2f496f8764169cbb281448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding relevant documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 60/557 [00:00<00:04, 120.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Query 43 only has 9 relevant documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 283/557 [00:02<00:02, 100.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Query 274 only has 7 relevant documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 457/557 [00:04<00:01, 89.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Query 448 only has 7 relevant documents\n",
      "Warning: Query 453 only has 6 relevant documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:05<00:00, 94.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Query 552 only has 7 relevant documents\n",
      "Results saved to submission_fine_tuned_beir.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize necessary imports\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "class FineTunedBeIRModel:\n",
    "    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v2', cache_dir=\"embeddings_cache\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Cache file paths\n",
    "        self.fine_tuned_cache = self.cache_dir / f\"fine_tuned_{model_name.split('/')[-1]}_embeddings.pkl\"\n",
    "        \n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Prepare training data from BeIR dataset\"\"\"\n",
    "        print(\"Loading BeIR dataset...\")\n",
    "        dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "        dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "        dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "        \n",
    "        # Get training split\n",
    "        train_qrels = dataset_qrels['train']\n",
    "        queries = dataset_queries['queries']\n",
    "        docs = dataset_docs['corpus']\n",
    "        \n",
    "        # Create query and document mappings\n",
    "        query_dict = {q['_id']: q['text'] for q in queries}\n",
    "        doc_dict = {d['_id']: d['text'] for d in docs}\n",
    "        \n",
    "        # Prepare training examples\n",
    "        train_examples = []\n",
    "        print(\"Preparing training examples...\")\n",
    "        \n",
    "        # Group by query to get positive documents\n",
    "        query_to_pos_docs = {}\n",
    "        for item in train_qrels:\n",
    "            query_id = item['query-id']\n",
    "            if query_id not in query_to_pos_docs:\n",
    "                query_to_pos_docs[query_id] = []\n",
    "            query_to_pos_docs[query_id].append(item['corpus-id'])\n",
    "        \n",
    "        # Create training triplets\n",
    "        for query_id, pos_doc_ids in tqdm(query_to_pos_docs.items()):\n",
    "            query_text = query_dict[query_id]\n",
    "            \n",
    "            # Get positive documents\n",
    "            for pos_doc_id in pos_doc_ids:\n",
    "                pos_doc_text = doc_dict[pos_doc_id]\n",
    "                \n",
    "                # Create training example\n",
    "                train_examples.append(\n",
    "                    InputExample(\n",
    "                        texts=[query_text, pos_doc_text],\n",
    "                        label=1.0\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        print(f\"Created {len(train_examples)} training examples\")\n",
    "        return train_examples\n",
    "    \n",
    "    def fine_tune(self, batch_size=16, num_epochs=3):\n",
    "        \"\"\"Fine-tune the model using MultipleNegativesRankingLoss\"\"\"\n",
    "        print(\"Starting fine-tuning process...\")\n",
    "        \n",
    "        # Prepare training data\n",
    "        train_examples = self.prepare_training_data()\n",
    "        \n",
    "        # Create data loader\n",
    "        train_dataloader = DataLoader(\n",
    "            train_examples,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Define the loss\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(self.model)\n",
    "        \n",
    "        # Set up training parameters\n",
    "        warmup_steps = int(len(train_dataloader) * 0.1)\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Training for {num_epochs} epochs...\")\n",
    "        self.model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=num_epochs,\n",
    "            warmup_steps=warmup_steps,\n",
    "            show_progress_bar=True,\n",
    "            output_path=f'fine_tuned_{self.model_name.split(\"/\")[-1]}'\n",
    "        )\n",
    "        print(\"Fine-tuning completed!\")\n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=32):\n",
    "        \"\"\"Get embeddings using the fine-tuned model\"\"\"\n",
    "        return self.model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "    \n",
    "    def find_similar_documents(self, test_queries):\n",
    "        \"\"\"Find similar documents using BeIR ground truth method\"\"\"\n",
    "        print(\"Loading BeIR dataset components...\")\n",
    "        # Load BeIR dataset\n",
    "        dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "        dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "        \n",
    "        beir_queries = dataset_queries['queries']['text']\n",
    "        beir_query_ids = dataset_queries['queries']['_id']\n",
    "        \n",
    "        # Create mappings\n",
    "        beir_query_text_to_id = dict(zip(beir_queries, beir_query_ids))\n",
    "        \n",
    "        # Create mapping from query ID to relevant documents\n",
    "        beir_query_to_docs = {}\n",
    "        for split in ['train', 'test', 'validation']:\n",
    "            for item in dataset_qrels[split]:\n",
    "                query_id = item['query-id']\n",
    "                doc_id = item['corpus-id']\n",
    "                if query_id not in beir_query_to_docs:\n",
    "                    beir_query_to_docs[query_id] = []\n",
    "                beir_query_to_docs[query_id].append(doc_id)\n",
    "        \n",
    "        # Get embeddings\n",
    "        print(\"Computing embeddings...\")\n",
    "        test_embeddings = self.get_embeddings(test_queries)\n",
    "        beir_embeddings = self.get_embeddings(beir_queries)\n",
    "        \n",
    "        # Process each test query\n",
    "        print(\"Finding relevant documents...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, test_query in enumerate(tqdm(test_queries)):\n",
    "            # Find similar BeIR queries\n",
    "            similarities = cosine_similarity([test_embeddings[i]], beir_embeddings)[0]\n",
    "            top_k_indices = np.argsort(similarities)[::-1][:5]\n",
    "            \n",
    "            # Collect relevant documents\n",
    "            relevant_docs = []\n",
    "            for idx in top_k_indices:\n",
    "                beir_query = beir_queries[idx]\n",
    "                beir_query_id = beir_query_text_to_id[beir_query]\n",
    "                relevant_docs.extend(beir_query_to_docs.get(beir_query_id, []))\n",
    "            \n",
    "            # Remove duplicates while preserving order\n",
    "            relevant_docs = list(dict.fromkeys(relevant_docs))\n",
    "            \n",
    "            # Ensure exactly 10 documents\n",
    "            relevant_docs = relevant_docs[:10]\n",
    "            if len(relevant_docs) < 10:\n",
    "                print(f\"Warning: Query {i} only has {len(relevant_docs)} relevant documents\")\n",
    "            \n",
    "            results.append({\n",
    "                'Query': test_query,\n",
    "                'Doc_ID': ' '.join(relevant_docs)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Initialize model\n",
    "    model = FineTunedBeIRModel()\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    model.fine_tune()\n",
    "    \n",
    "    # Load test queries\n",
    "    test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "    test_queries = test_queries_df['Query'].tolist()\n",
    "    \n",
    "    # Find similar documents using fine-tuned model\n",
    "    results_df = model.find_similar_documents(test_queries)\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('submission_fine_tuned_beir.csv', index=False)\n",
    "    print(\"Results saved to submission_fine_tuned_beir.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Loading model from fine_tuned_all-mpnet-base-v2...\n",
      "Loaded 557 queries and 3125 documents\n",
      "Ranking documents...\n",
      "Encoding queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbdbd7137964693884b46369d145685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205c525b90ea431e984e657a5688f661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarities and ranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:05<00:00, 93.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n",
      "\n",
      "First few rows of the submission file:\n",
      "                                               Query  \\\n",
      "0                       Herbalife® has been updated.   \n",
      "1  Can eating Fruit & Nut Bars lead to an increas...   \n",
      "2                      What can I do with chickpeas?   \n",
      "3    Are chronic headaches caused by pork parasites?   \n",
      "4  is a professor at Harvard University and also ...   \n",
      "\n",
      "                                              Doc_ID  \n",
      "0  MED-5157 MED-5158 MED-4873 MED-4372 MED-4374 M...  \n",
      "1  MED-3896 MED-4286 MED-4292 MED-4289 MED-4291 M...  \n",
      "2  MED-2009 MED-2010 MED-2989 MED-3583 MED-2145 M...  \n",
      "3  MED-3171 MED-3176 MED-3177 MED-3175 MED-3169 M...  \n",
      "4  MED-2765 MED-3001 MED-4609 MED-4613 MED-4255 M...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class KaggleSubmissionModel:\n",
    "    def __init__(self, model_path='fine_tuned_all-mpnet-base-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the model with the fine-tuned model\n",
    "        \"\"\"\n",
    "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
    "        self.model = SentenceTransformer(model_path)\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load test queries and documents from CSV files\n",
    "        \"\"\"\n",
    "        # Load test queries and documents from CSV files\n",
    "        self.test_queries_df = pd.read_csv('test_query.csv')\n",
    "        self.test_documents_df = pd.read_csv('test_documents.csv')\n",
    "        \n",
    "        # Extract query and document texts\n",
    "        self.queries = self.test_queries_df['Query'].tolist()\n",
    "        # Use the Doc column as document IDs\n",
    "        self.document_ids = self.test_documents_df['Doc'].tolist()\n",
    "        self.documents = self.document_ids  # In this case, document IDs are the same as document content\n",
    "        \n",
    "        print(f\"Loaded {len(self.queries)} queries and {len(self.documents)} documents\")\n",
    "\n",
    "    def rank_documents(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        Rank documents for each query using sentence transformer embeddings\n",
    "        \"\"\"\n",
    "        print(\"Encoding queries...\")\n",
    "        query_embeddings = self.model.encode(self.queries, batch_size=batch_size, show_progress_bar=True)\n",
    "        \n",
    "        print(\"Encoding documents...\")\n",
    "        doc_embeddings = self.model.encode(self.documents, batch_size=batch_size, show_progress_bar=True)\n",
    "        \n",
    "        print(\"Computing similarities and ranking documents...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, query_embedding in enumerate(tqdm(query_embeddings)):\n",
    "            # Calculate cosine similarity\n",
    "            similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "            \n",
    "            # Get the top 10 most similar documents\n",
    "            top_indices = np.argsort(similarities)[::-1][:10]  # Ensure 10 documents are retrieved each time\n",
    "            \n",
    "            # Get the corresponding document IDs\n",
    "            top_doc_ids = [self.document_ids[idx] for idx in top_indices]\n",
    "            \n",
    "            # Combine document IDs into a string\n",
    "            doc_ids_str = ' '.join(top_doc_ids)\n",
    "            \n",
    "            # Add to results\n",
    "            results.append({\n",
    "                'Query': self.test_queries_df.iloc[i]['Query'],\n",
    "                'Doc_ID': doc_ids_str\n",
    "            })\n",
    "        \n",
    "        # Create submission file\n",
    "        submission_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Verify that each row has 10 document IDs\n",
    "        for idx, row in submission_df.iterrows():\n",
    "            doc_ids = row['Doc_ID'].split()\n",
    "            if len(doc_ids) != 10:\n",
    "                print(f\"Warning: Row {idx} has {len(doc_ids)} documents instead of 10\")\n",
    "        \n",
    "        submission_df.to_csv('submission_fine_tuned.csv', index=False)\n",
    "        print(\"Submission file created successfully!\")\n",
    "        \n",
    "        # Display the first few rows as an example\n",
    "        print(\"\\nFirst few rows of the submission file:\")\n",
    "        print(submission_df.head())\n",
    "\n",
    "def main():\n",
    "    # Use the fine-tuned model\n",
    "    print(\"Initializing model...\")\n",
    "    model = KaggleSubmissionModel('fine_tuned_all-mpnet-base-v2')\n",
    "    print(\"Ranking documents...\")\n",
    "    model.rank_documents()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model from fine_tuned_all-mpnet-base-v2...\n",
      "Starting additional fine-tuning...\n",
      "Loading BeIR dataset...\n",
      "Preparing training examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2590/2590 [00:01<00:00, 2296.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 110575 training examples\n",
      "Training for additional 10 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62db8c4c34304428bfd59551ede46418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69110' max='69110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69110/69110 5:53:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.342200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.376400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.345700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.315400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>1.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>1.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>1.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>1.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>1.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>1.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>1.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>1.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>1.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.997900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>1.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>1.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>1.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>1.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>1.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>1.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>1.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>1.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>1.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>1.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>1.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.946800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.960600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.964300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.959600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.969400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.970800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.927500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.939600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.925500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.885200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.881200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.905800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.896800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.901200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.860600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.871900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.869400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.863600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.863100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.873800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.861900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.854600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.842500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.844800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.808300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.821100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.824200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.840400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional fine-tuning completed!\n",
      "Loading test data...\n",
      "Loading BeIR corpus...\n",
      "Encoding queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe8d517f4374c1eb8ff202b33d92a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4590af523ed346979c1917d9b43ea836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:05<00:00, 104.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission_continued_training.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ContinueTrainingModel:\n",
    "    def __init__(self, model_path='fine_tuned_all-mpnet-base-v2'):\n",
    "        \"\"\"\n",
    "        Initialize with the previously fine-tuned model\n",
    "        \"\"\"\n",
    "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
    "        self.model = SentenceTransformer(model_path)\n",
    "        self.corpus_name = \"BeIR/nfcorpus\"\n",
    "        \n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Prepare training data from BeIR dataset\"\"\"\n",
    "        print(\"Loading BeIR dataset...\")\n",
    "        dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "        dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "        dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "        \n",
    "        # Get training split\n",
    "        train_qrels = dataset_qrels['train']\n",
    "        queries = dataset_queries['queries']\n",
    "        docs = dataset_docs['corpus']\n",
    "        \n",
    "        # Create query and document mappings\n",
    "        query_dict = {q['_id']: q['text'] for q in queries}\n",
    "        doc_dict = {d['_id']: d['text'] for d in docs}\n",
    "        \n",
    "        # Prepare training examples\n",
    "        train_examples = []\n",
    "        print(\"Preparing training examples...\")\n",
    "        \n",
    "        # Group by query to get positive documents\n",
    "        query_to_pos_docs = {}\n",
    "        for item in train_qrels:\n",
    "            query_id = item['query-id']\n",
    "            if query_id not in query_to_pos_docs:\n",
    "                query_to_pos_docs[query_id] = []\n",
    "            query_to_pos_docs[query_id].append(item['corpus-id'])\n",
    "        \n",
    "        # Create training examples\n",
    "        for query_id, pos_doc_ids in tqdm(query_to_pos_docs.items()):\n",
    "            query_text = query_dict[query_id]\n",
    "            for pos_doc_id in pos_doc_ids:\n",
    "                pos_doc_text = doc_dict[pos_doc_id]\n",
    "                train_examples.append(\n",
    "                    InputExample(\n",
    "                        texts=[query_text, pos_doc_text],\n",
    "                        label=1.0\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        print(f\"Created {len(train_examples)} training examples\")\n",
    "        return train_examples\n",
    "    \n",
    "    def continue_training(self, batch_size=16, num_epochs=10):\n",
    "        \"\"\"Continue fine-tuning the model for more epochs\"\"\"\n",
    "        print(\"Starting additional fine-tuning...\")\n",
    "        \n",
    "        # Prepare training data\n",
    "        train_examples = self.prepare_training_data()\n",
    "        \n",
    "        # Create data loader\n",
    "        train_dataloader = DataLoader(\n",
    "            train_examples,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Define the loss\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(self.model)\n",
    "        \n",
    "        # Set up training parameters\n",
    "        warmup_steps = int(len(train_dataloader) * 0.1)\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Training for additional {num_epochs} epochs...\")\n",
    "        self.model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=num_epochs,\n",
    "            warmup_steps=warmup_steps,\n",
    "            show_progress_bar=True,\n",
    "            output_path='fine_tuned_all-mpnet-base-v2_continued'\n",
    "        )\n",
    "        print(\"Additional fine-tuning completed!\")\n",
    "\n",
    "    def create_submission(self, batch_size=32):\n",
    "        \"\"\"Create submission using direct similarity ranking\"\"\"\n",
    "        # Load test data\n",
    "        print(\"Loading test data...\")\n",
    "        test_queries_df = pd.read_csv('test_query.csv')\n",
    "        test_documents_df = pd.read_csv('test_documents.csv')\n",
    "        \n",
    "        # Load BeIR corpus for document texts\n",
    "        print(\"Loading BeIR corpus...\")\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "        doc_id_to_text = {\n",
    "            doc_id: text for doc_id, text in zip(\n",
    "                dataset_docs[\"corpus\"][\"_id\"],\n",
    "                dataset_docs[\"corpus\"][\"text\"]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Get document texts\n",
    "        test_doc_texts = [doc_id_to_text[doc_id] for doc_id in test_documents_df['Doc']]\n",
    "        \n",
    "        # Encode queries and documents\n",
    "        print(\"Encoding queries...\")\n",
    "        query_embeddings = self.model.encode(\n",
    "            test_queries_df['Query'].tolist(),\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        print(\"Encoding documents...\")\n",
    "        doc_embeddings = self.model.encode(\n",
    "            test_doc_texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Create submission\n",
    "        print(\"Creating submission...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, query_embedding in enumerate(tqdm(query_embeddings)):\n",
    "            # Calculate similarities\n",
    "            similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "            \n",
    "            # Get top 10 documents\n",
    "            top_indices = np.argsort(similarities)[::-1][:10]\n",
    "            top_doc_ids = [test_documents_df.iloc[idx]['Doc'] for idx in top_indices]\n",
    "            \n",
    "            results.append({\n",
    "                'Query': test_queries_df.iloc[i]['Query'],\n",
    "                'Doc_ID': ' '.join(top_doc_ids)\n",
    "            })\n",
    "        \n",
    "        # Save submission\n",
    "        submission_df = pd.DataFrame(results)\n",
    "        submission_df.to_csv('submission_continued_training.csv', index=False)\n",
    "        print(\"Submission saved to submission_continued_training.csv\")\n",
    "\n",
    "def main():\n",
    "    # Initialize with previously fine-tuned model\n",
    "    model = ContinueTrainingModel('fine_tuned_all-mpnet-base-v2')\n",
    "    \n",
    "    # Continue training for 10 more epochs\n",
    "    model.continue_training(num_epochs=10)\n",
    "    \n",
    "    # Create submission using the further trained model\n",
    "    model.create_submission()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 1 epochs...\n",
      "Loading base model from: fine_tuned_all-mpnet-base-v2_continued_v4_24epochs\n",
      "Will save new model to: fine_tuned_all-mpnet-base-v2_continued_v4_25epochs\n",
      "Loading fine-tuned model from fine_tuned_all-mpnet-base-v2_continued_v4_24epochs...\n",
      "Starting additional fine-tuning...\n",
      "Loading BeIR dataset...\n",
      "Preparing training examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444a624f380b404eb8c98b361d528ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 110575 training examples\n",
      "Training for additional 1 epochs with batch_size=4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af2afdc2aa144a98156e0afc40f826f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27644' max='27644' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27644/27644 49:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.344900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.329700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.328300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.421800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.442400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.454900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.438300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.465500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.438100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.422500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.443800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.430600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.464100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.406700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.405200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.395500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.401800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.350600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.313300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional fine-tuning completed!\n",
      "Loading test data...\n",
      "Loading BeIR corpus...\n",
      "Encoding queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3d439e94fa4a37ba9fced4a603576f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ec4b3de49b4b75bce89a336645fe8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58706e4993a94373b5d9f0728cf8e5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission_continued_training_v4_25epochs.csv\n",
      "\n",
      "Training completed!\n",
      "Model saved to: fine_tuned_all-mpnet-base-v2_continued_v4_25epochs\n",
      "Submission saved to: submission_continued_training_v4_25epochs.csv\n",
      "\n",
      "Calculating MAP score...\n",
      "MAP@10 Score: 0.1481\n",
      "MAP Score for v4_25epochs: 0.1481\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "def calculate_map(submission_file, ground_truth_file = 'submission_from_beir_ground_truth_openai_large.csv'):\n",
    "    \"\"\"\n",
    "    Calculate MAP@10 score with submission file and a high map score submission file\n",
    "    \"\"\"\n",
    "    # Read files\n",
    "    ground_truth_df = pd.read_csv(ground_truth_file)\n",
    "    submission_df = pd.read_csv(submission_file)\n",
    "    \n",
    "    # Ensure query order consistency\n",
    "    assert all(ground_truth_df['Query'] == submission_df['Query']), \"Queries don't match!\"\n",
    "    \n",
    "    ap_scores = []\n",
    "    \n",
    "    for i in range(len(ground_truth_df)):\n",
    "        # Get ground truth and submitted document IDs\n",
    "        gt_docs = ground_truth_df.iloc[i]['Doc_ID'].split()\n",
    "        sub_docs = submission_df.iloc[i]['Doc_ID'].split()\n",
    "        \n",
    "        # Ensure each query has 10 documents\n",
    "        assert len(sub_docs) == 10, f\"Query {i} doesn't have 10 documents!\"\n",
    "        \n",
    "        # Calculate AP@10 for this query\n",
    "        relevant_count = 0\n",
    "        ap = 0.0\n",
    "        \n",
    "        for k, doc_id in enumerate(sub_docs, 1):\n",
    "            if doc_id in gt_docs:\n",
    "                relevant_count += 1\n",
    "                precision_at_k = relevant_count / k\n",
    "                ap += precision_at_k\n",
    "        \n",
    "        if relevant_count > 0:\n",
    "            ap /= min(len(gt_docs), 10)  # normalize by min(relevant docs, 10)\n",
    "        ap_scores.append(ap)\n",
    "    \n",
    "    # Calculate MAP\n",
    "    map_score = np.mean(ap_scores)\n",
    "    \n",
    "    print(f\"MAP@10 Score: {map_score:.4f}\")\n",
    "    return map_score\n",
    "\n",
    "class ContinueTrainingModel:\n",
    "    def __init__(self, model_path='fine_tuned_all-mpnet-base-v2'):\n",
    "        \"\"\"\n",
    "        Initialize with the previously fine-tuned model\n",
    "        \"\"\"\n",
    "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
    "        self.model = SentenceTransformer(model_path)\n",
    "        self.corpus_name = \"BeIR/nfcorpus\"\n",
    "        \n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Prepare training data from BeIR dataset\"\"\"\n",
    "        print(\"Loading BeIR dataset...\")\n",
    "        dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "        dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "        dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "        \n",
    "        # Get training split\n",
    "        train_qrels = dataset_qrels['train']\n",
    "        queries = dataset_queries['queries']\n",
    "        docs = dataset_docs['corpus']\n",
    "        \n",
    "        # Create query and document mappings\n",
    "        query_dict = {q['_id']: q['text'] for q in queries}\n",
    "        doc_dict = {d['_id']: d['text'] for d in docs}\n",
    "        \n",
    "        # Prepare training examples\n",
    "        train_examples = []\n",
    "        print(\"Preparing training examples...\")\n",
    "        \n",
    "        # Group by query to get positive documents\n",
    "        query_to_pos_docs = {}\n",
    "        for item in train_qrels:\n",
    "            query_id = item['query-id']\n",
    "            if query_id not in query_to_pos_docs:\n",
    "                query_to_pos_docs[query_id] = []\n",
    "            query_to_pos_docs[query_id].append(item['corpus-id'])\n",
    "        \n",
    "        # Create training examples\n",
    "        for query_id, pos_doc_ids in tqdm(query_to_pos_docs.items()):\n",
    "            query_text = query_dict[query_id]\n",
    "            for pos_doc_id in pos_doc_ids:\n",
    "                pos_doc_text = doc_dict[pos_doc_id]\n",
    "                train_examples.append(\n",
    "                    InputExample(\n",
    "                        texts=[query_text, pos_doc_text],\n",
    "                        label=1.0\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        print(f\"Created {len(train_examples)} training examples\")\n",
    "        return train_examples\n",
    "    \n",
    "    def continue_training(self, batch_size=8, num_epochs=10, output_path=None):\n",
    "        \"\"\"Continue fine-tuning the model for more epochs\"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = 'fine_tuned_all-mpnet-base-v2_continued'\n",
    "        \n",
    "        print(\"Starting additional fine-tuning...\")\n",
    "        \n",
    "\n",
    "        # Prepare training data\n",
    "        train_examples = self.prepare_training_data()\n",
    "        \n",
    "        # Create data loader\n",
    "        train_dataloader = DataLoader(\n",
    "            train_examples,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Define the loss\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(self.model)\n",
    "        \n",
    "        # Set up warmup steps\n",
    "        warmup_steps = int(len(train_dataloader) * 0.1)\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Training for additional {num_epochs} epochs with batch_size={batch_size}...\")\n",
    "        self.model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=num_epochs,\n",
    "            warmup_steps=warmup_steps,\n",
    "            show_progress_bar=True,\n",
    "            output_path=output_path\n",
    "        )\n",
    "        print(\"Additional fine-tuning completed!\")\n",
    "\n",
    "\n",
    "    def create_submission(self, batch_size=32, output_file=None):\n",
    "        \"\"\"Create submission using direct similarity ranking\"\"\"\n",
    "        if output_file is None:\n",
    "            output_file = 'submission_continued_training.csv'\n",
    "            \n",
    "        # Load test data\n",
    "        print(\"Loading test data...\")\n",
    "        test_queries_df = pd.read_csv('test_query.csv')\n",
    "        test_documents_df = pd.read_csv('test_documents.csv')\n",
    "        \n",
    "        # Load BeIR corpus for document texts\n",
    "        print(\"Loading BeIR corpus...\")\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "        doc_id_to_text = {\n",
    "            doc_id: text for doc_id, text in zip(\n",
    "                dataset_docs[\"corpus\"][\"_id\"],\n",
    "                dataset_docs[\"corpus\"][\"text\"]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Get document texts\n",
    "        test_doc_texts = [doc_id_to_text[doc_id] for doc_id in test_documents_df['Doc']]\n",
    "        \n",
    "        # Encode queries and documents\n",
    "        print(\"Encoding queries...\")\n",
    "        query_embeddings = self.model.encode(\n",
    "            test_queries_df['Query'].tolist(),\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        print(\"Encoding documents...\")\n",
    "        doc_embeddings = self.model.encode(\n",
    "            test_doc_texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Create submission\n",
    "        print(\"Creating submission...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, query_embedding in enumerate(tqdm(query_embeddings)):\n",
    "            # Calculate similarities\n",
    "            similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "            \n",
    "            # Get top 10 documents\n",
    "            top_indices = np.argsort(similarities)[::-1][:10]\n",
    "            top_doc_ids = [test_documents_df.iloc[idx]['Doc'] for idx in top_indices]\n",
    "            \n",
    "            results.append({\n",
    "                'Query': test_queries_df.iloc[i]['Query'],\n",
    "                'Doc_ID': ' '.join(top_doc_ids)\n",
    "            })\n",
    "        \n",
    "        # Save submission\n",
    "        submission_df = pd.DataFrame(results)\n",
    "        submission_df.to_csv(output_file, index=False)\n",
    "        print(f\"Submission saved to {output_file}\")\n",
    "\n",
    "def train_model_for_epochs(base_model_path, num_epochs, version_name=None):\n",
    "    \"\"\"\n",
    "    Generic model training function\n",
    "    \"\"\"\n",
    "    if version_name is None:\n",
    "        version_name = f\"{num_epochs}epochs\"\n",
    "    \n",
    "    # Model output path\n",
    "    model_output_path = f'fine_tuned_all-mpnet-base-v2_continued_{version_name}'\n",
    "    # Submission file path\n",
    "    submission_file = f'submission_continued_training_{version_name}.csv'\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"Loading base model from: {base_model_path}\")\n",
    "    print(f\"Will save new model to: {model_output_path}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ContinueTrainingModel(base_model_path)\n",
    "    \n",
    "    # Continue training\n",
    "    model.continue_training(\n",
    "        num_epochs=num_epochs,\n",
    "        output_path=model_output_path\n",
    "    )\n",
    "    \n",
    "    # Create submission file\n",
    "    model.create_submission(output_file=submission_file)\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Model saved to: {model_output_path}\")\n",
    "    print(f\"Submission saved to: {submission_file}\")\n",
    "    \n",
    "    # Calculate MAP score\n",
    "    try:\n",
    "        print(\"\\nCalculating MAP score...\")\n",
    "        map_score = calculate_map(\n",
    "            submission_file\n",
    "        )\n",
    "        print(f\"MAP Score for {version_name}: {map_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate MAP score: {str(e)}\")\n",
    "    \n",
    "    return model_output_path, submission_file\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Train 1 epochs\n",
    "    model_path_1, submission_1 = train_model_for_epochs(\n",
    "        'fine_tuned_all-mpnet-base-v2_continued',\n",
    "        num_epochs=1,\n",
    "        version_name='v3_continued'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 1 epochs...\n",
      "Loading base model from: fine_tuned_all-mpnet-base_v4_continued\n",
      "Will save new model to: fine_tuned_all-mpnet-base_v5_continued\n",
      "Loading fine-tuned model from fine_tuned_all-mpnet-base_v4_continued...\n",
      "Starting additional fine-tuning...\n",
      "Loading BeIR dataset...\n",
      "Preparing training examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fcf65926f543fa8846c0962d3266ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 110575 training examples\n",
      "Training for additional 1 epochs with default batch_size...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddacbe7a8c364a008c76b696e6d3c594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41805' max='110575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 41805/110575 46:07 < 1:15:52, 15.11 it/s, Epoch 0.38/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 248\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# Train 1 epochs\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     model_path_1, submission_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_for_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfine_tuned_all-mpnet-base_v4_continued\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mversion_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv5_continued\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 221\u001b[0m, in \u001b[0;36mtrain_model_for_epochs\u001b[0;34m(base_model_path, num_epochs, version_name)\u001b[0m\n\u001b[1;32m    218\u001b[0m model \u001b[38;5;241m=\u001b[39m ContinueTrainingModel(base_model_path)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Continue training\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinue_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_output_path\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Create submission file\u001b[39;00m\n\u001b[1;32m    227\u001b[0m model\u001b[38;5;241m.\u001b[39mcreate_submission(output_file\u001b[38;5;241m=\u001b[39msubmission_file)\n",
      "Cell \u001b[0;32mIn[5], line 131\u001b[0m, in \u001b[0;36mContinueTrainingModel.continue_training\u001b[0;34m(self, num_epochs, output_path)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining for additional \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs with default batch_size...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdditional fine-tuning completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm596/lib/python3.11/site-packages/sentence_transformers/fit_mixin.py:385\u001b[0m, in \u001b[0;36mFitMixin.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     trainer\u001b[38;5;241m.\u001b[39madd_callback(SaveModelCallback(output_path, evaluator, save_best_model))\n\u001b[0;32m--> 385\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llm596/lib/python3.11/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llm596/lib/python3.11/site-packages/transformers/trainer.py:2480\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2478\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2479\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2480\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2482\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/llm596/lib/python3.11/site-packages/transformers/trainer.py:5153\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5152\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5153\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mnext\u001b[39m(epoch_iterator)]\n\u001b[1;32m   5154\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5155\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llm596/lib/python3.11/site-packages/accelerate/data_loader.py:572\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 572\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m    574\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm596/lib/python3.11/site-packages/accelerate/utils/operations.py:183\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 183\u001b[0m         \u001b[43m{\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/miniforge3/envs/llm596/lib/python3.11/site-packages/accelerate/utils/operations.py:184\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m    183\u001b[0m         {\n\u001b[0;32m--> 184\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    186\u001b[0m         }\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/miniforge3/envs/llm596/lib/python3.11/site-packages/accelerate/utils/operations.py:155\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    153\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "def calculate_map(submission_file, ground_truth_file = 'submission_from_beir_ground_truth_openai_large.csv'):\n",
    "    \"\"\"\n",
    "    Calculate MAP@10 score with submission file and a high map score submission file\n",
    "    \"\"\"\n",
    "    # Read files\n",
    "    ground_truth_df = pd.read_csv(ground_truth_file)\n",
    "    submission_df = pd.read_csv(submission_file)\n",
    "    \n",
    "    # Ensure query order consistency\n",
    "    assert all(ground_truth_df['Query'] == submission_df['Query']), \"Queries don't match!\"\n",
    "    \n",
    "    ap_scores = []\n",
    "    \n",
    "    for i in range(len(ground_truth_df)):\n",
    "        # Get ground truth and submitted document IDs\n",
    "        gt_docs = ground_truth_df.iloc[i]['Doc_ID'].split()\n",
    "        sub_docs = submission_df.iloc[i]['Doc_ID'].split()\n",
    "        \n",
    "        # Ensure each query has 10 documents\n",
    "        assert len(sub_docs) == 10, f\"Query {i} doesn't have 10 documents!\"\n",
    "        \n",
    "        # Calculate AP@10 for this query\n",
    "        relevant_count = 0\n",
    "        ap = 0.0\n",
    "        \n",
    "        for k, doc_id in enumerate(sub_docs, 1):\n",
    "            if doc_id in gt_docs:\n",
    "                relevant_count += 1\n",
    "                precision_at_k = relevant_count / k\n",
    "                ap += precision_at_k\n",
    "        \n",
    "        if relevant_count > 0:\n",
    "            ap /= min(len(gt_docs), 10)  # normalize by min(relevant docs, 10)\n",
    "        ap_scores.append(ap)\n",
    "    \n",
    "    # Calculate MAP\n",
    "    map_score = np.mean(ap_scores)\n",
    "    \n",
    "    print(f\"MAP@10 Score: {map_score:.4f}\")\n",
    "    return map_score\n",
    "\n",
    "class ContinueTrainingModel:\n",
    "    def __init__(self, model_path='fine_tuned_all-mpnet-base-v2'):\n",
    "        \"\"\"\n",
    "        Initialize with the previously fine-tuned model\n",
    "        \"\"\"\n",
    "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
    "        self.model = SentenceTransformer(model_path)\n",
    "        self.corpus_name = \"BeIR/nfcorpus\"\n",
    "        \n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Prepare training data from BeIR dataset\"\"\"\n",
    "        print(\"Loading BeIR dataset...\")\n",
    "        dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "        dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "        dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "        \n",
    "        # Get training split\n",
    "        train_qrels = dataset_qrels['train']\n",
    "        queries = dataset_queries['queries']\n",
    "        docs = dataset_docs['corpus']\n",
    "        \n",
    "        # Create query and document mappings\n",
    "        query_dict = {q['_id']: q['text'] for q in queries}\n",
    "        doc_dict = {d['_id']: d['text'] for d in docs}\n",
    "        \n",
    "        # Prepare training examples\n",
    "        train_examples = []\n",
    "        print(\"Preparing training examples...\")\n",
    "        \n",
    "        # Group by query to get positive documents\n",
    "        query_to_pos_docs = {}\n",
    "        for item in train_qrels:\n",
    "            query_id = item['query-id']\n",
    "            if query_id not in query_to_pos_docs:\n",
    "                query_to_pos_docs[query_id] = []\n",
    "            query_to_pos_docs[query_id].append(item['corpus-id'])\n",
    "        \n",
    "        # Create training examples\n",
    "        for query_id, pos_doc_ids in tqdm(query_to_pos_docs.items()):\n",
    "            query_text = query_dict[query_id]\n",
    "            for pos_doc_id in pos_doc_ids:\n",
    "                pos_doc_text = doc_dict[pos_doc_id]\n",
    "                train_examples.append(\n",
    "                    InputExample(\n",
    "                        texts=[query_text, pos_doc_text],\n",
    "                        label=1.0\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        print(f\"Created {len(train_examples)} training examples\")\n",
    "        return train_examples\n",
    "    \n",
    "    def continue_training(self, batch_size=32, num_epochs=10, output_path=None):\n",
    "        \"\"\"Continue fine-tuning the model for more epochs\"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = 'fine_tuned_all-mpnet-base-v2_continued'\n",
    "        \n",
    "        print(\"Starting additional fine-tuning...\")\n",
    "        \n",
    "\n",
    "        # Prepare training data\n",
    "        train_examples = self.prepare_training_data()\n",
    "        \n",
    "        # Create data loader\n",
    "        train_dataloader = DataLoader(\n",
    "            train_examples,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Define the loss\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(self.model)\n",
    "        \n",
    "        # Set up warmup steps\n",
    "        warmup_steps = int(len(train_dataloader) * 0.1)\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Training for additional {num_epochs} epochs with batch_size={batch_size}...\")\n",
    "        self.model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            epochs=num_epochs,\n",
    "            warmup_steps=warmup_steps,\n",
    "            show_progress_bar=True,\n",
    "            output_path=output_path\n",
    "        )\n",
    "        print(\"Additional fine-tuning completed!\")\n",
    "\n",
    "\n",
    "    def create_submission(self, batch_size=32, output_file=None):\n",
    "        \"\"\"Create submission using direct similarity ranking\"\"\"\n",
    "        if output_file is None:\n",
    "            output_file = 'submission_continued_training.csv'\n",
    "            \n",
    "        # Load test data\n",
    "        print(\"Loading test data...\")\n",
    "        test_queries_df = pd.read_csv('test_query.csv')\n",
    "        test_documents_df = pd.read_csv('test_documents.csv')\n",
    "        \n",
    "        # Load BeIR corpus for document texts\n",
    "        print(\"Loading BeIR corpus...\")\n",
    "        dataset_docs = load_dataset(self.corpus_name, \"corpus\")\n",
    "        doc_id_to_text = {\n",
    "            doc_id: text for doc_id, text in zip(\n",
    "                dataset_docs[\"corpus\"][\"_id\"],\n",
    "                dataset_docs[\"corpus\"][\"text\"]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Get document texts\n",
    "        test_doc_texts = [doc_id_to_text[doc_id] for doc_id in test_documents_df['Doc']]\n",
    "        \n",
    "        # Encode queries and documents\n",
    "        print(\"Encoding queries...\")\n",
    "        query_embeddings = self.model.encode(\n",
    "            test_queries_df['Query'].tolist(),\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        print(\"Encoding documents...\")\n",
    "        doc_embeddings = self.model.encode(\n",
    "            test_doc_texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Create submission\n",
    "        print(\"Creating submission...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, query_embedding in enumerate(tqdm(query_embeddings)):\n",
    "            # Calculate similarities\n",
    "            similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "            \n",
    "            # Get top 10 documents\n",
    "            top_indices = np.argsort(similarities)[::-1][:10]\n",
    "            top_doc_ids = [test_documents_df.iloc[idx]['Doc'] for idx in top_indices]\n",
    "            \n",
    "            results.append({\n",
    "                'Query': test_queries_df.iloc[i]['Query'],\n",
    "                'Doc_ID': ' '.join(top_doc_ids)\n",
    "            })\n",
    "        \n",
    "        # Save submission\n",
    "        submission_df = pd.DataFrame(results)\n",
    "        submission_df.to_csv(output_file, index=False)\n",
    "        print(f\"Submission saved to {output_file}\")\n",
    "\n",
    "def train_model_for_epochs(base_model_path, num_epochs, version_name=None):\n",
    "    \"\"\"\n",
    "    Generic model training function\n",
    "    \"\"\"\n",
    "    if version_name is None:\n",
    "        version_name = f\"{num_epochs}epochs\"\n",
    "    \n",
    "    # Model output path\n",
    "    model_output_path = f'fine_tuned_all-mpnet-base_{version_name}'\n",
    "    # Submission file path\n",
    "    submission_file = f'submission_continued_training_{version_name}.csv'\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"Loading base model from: {base_model_path}\")\n",
    "    print(f\"Will save new model to: {model_output_path}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ContinueTrainingModel(base_model_path)\n",
    "    \n",
    "    # Continue training\n",
    "    model.continue_training(\n",
    "        num_epochs=num_epochs,\n",
    "        output_path=model_output_path\n",
    "    )\n",
    "    \n",
    "    # Create submission file\n",
    "    model.create_submission(output_file=submission_file)\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Model saved to: {model_output_path}\")\n",
    "    print(f\"Submission saved to: {submission_file}\")\n",
    "    \n",
    "    # Calculate MAP score\n",
    "    try:\n",
    "        print(\"\\nCalculating MAP score...\")\n",
    "        map_score = calculate_map(\n",
    "            submission_file\n",
    "        )\n",
    "        print(f\"MAP Score for {version_name}: {map_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate MAP score: {str(e)}\")\n",
    "    \n",
    "    return model_output_path, submission_file\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Train 1 epochs\n",
    "    model_path_1, submission_1 = train_model_for_epochs(\n",
    "        'fine_tuned_all-mpnet-base_v4_continued',\n",
    "        num_epochs=1,\n",
    "        version_name='v5_continued'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loading test queries...\n",
      "Loading BeIR dataset...\n",
      "\n",
      "BEIR Dataset Statistics:\n",
      "Train queries: 2590\n",
      "Test queries: 323\n",
      "Val queries: 324\n",
      "\n",
      "Computing embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1768d3c35b439b95f5aefc653ce1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fa333f49b7426fab82cdfb5644cb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b3601f21cc46708bde6eade119c42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd1c1cfdd2d4dc192527cb5b49139a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing TOP5 distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:06<00:00, 83.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of TOP5 similar queries:\n",
      "train: 1736 queries (62.33%)\n",
      "test: 756 queries (27.15%)\n",
      "val: 293 queries (10.52%)\n",
      "\n",
      "Saving detailed results...\n",
      "Detailed results saved to query_top5_similarity_analysis.csv\n",
      "\n",
      "Example matches (First 3 test queries):\n",
      "\n",
      "Test Query: Herbalife® has been updated.\n",
      "Rank 1 (test): Update on Herbalife®\n",
      "Similarity Score: 0.7625\n",
      "Rank 2 (train): Herbalife\n",
      "Similarity Score: 0.6345\n",
      "Rank 3 (train): Herbalife® Supplement Liver Toxicity\n",
      "Similarity Score: 0.4947\n",
      "Rank 4 (val): Dietary Supplement Snake Oil\n",
      "Similarity Score: 0.4170\n",
      "Rank 5 (train): snake oil\n",
      "Similarity Score: 0.4106\n",
      "\n",
      "Test Query: Can eating Fruit & Nut Bars lead to an increase in weight?\n",
      "Rank 1 (test): Do Fruit & Nut Bars Cause Weight Gain?\n",
      "Similarity Score: 0.9421\n",
      "Rank 2 (val): Nuts Don't Cause Expected Weight Gain\n",
      "Similarity Score: 0.6317\n",
      "Rank 3 (train): Does Chocolate Cause Weight Gain?\n",
      "Similarity Score: 0.5989\n",
      "Rank 4 (train): Best Dried Fruit For Cholesterol\n",
      "Similarity Score: 0.5699\n",
      "Rank 5 (train): Bulking Up on Antioxidants\n",
      "Similarity Score: 0.5665\n",
      "\n",
      "Test Query: What can I do with chickpeas?\n",
      "Rank 1 (test): chickpeas\n",
      "Similarity Score: 0.6279\n",
      "Rank 2 (train): lima beans\n",
      "Similarity Score: 0.4418\n",
      "Rank 3 (train): U.S. Dry Bean Council\n",
      "Similarity Score: 0.4350\n",
      "Rank 4 (train): garbanzo beans\n",
      "Similarity Score: 0.4328\n",
      "Rank 5 (test): fava beans\n",
      "Similarity Score: 0.4110\n"
     ]
    }
   ],
   "source": [
    "def analyze_top5_query_distribution():\n",
    "    \"\"\"\n",
    "    Analyze the distribution of the most similar queries for each test query\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model = SentenceTransformer('fine_tuned_all-mpnet-base-v2')\n",
    "    \n",
    "    # Load test queries\n",
    "    print(\"Loading test queries...\")\n",
    "    test_queries_df = pd.read_csv('test_query.csv')\n",
    "    test_queries = test_queries_df['Query'].tolist()\n",
    "    \n",
    "    # Load BEIR dataset\n",
    "    print(\"Loading BeIR dataset...\")\n",
    "    dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "    dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "    \n",
    "    # Get query ids for each split\n",
    "    train_query_ids = set(item['query-id'] for item in dataset_qrels['train'])\n",
    "    test_query_ids = set(item['query-id'] for item in dataset_qrels['test'])\n",
    "    val_query_ids = set(item['query-id'] for item in dataset_qrels['validation'])\n",
    "    \n",
    "    # Create a mapping of query id to text\n",
    "    beir_queries = dataset_queries['queries']\n",
    "    query_id_to_text = {q['_id']: q['text'] for q in beir_queries}\n",
    "    \n",
    "    # Create a list of query texts for each split\n",
    "    train_queries = [query_id_to_text[qid] for qid in train_query_ids]\n",
    "    test_queries_beir = [query_id_to_text[qid] for qid in test_query_ids]\n",
    "    val_queries = [query_id_to_text[qid] for qid in val_query_ids]\n",
    "    \n",
    "    print(f\"\\nBEIR Dataset Statistics:\")\n",
    "    print(f\"Train queries: {len(train_queries)}\")\n",
    "    print(f\"Test queries: {len(test_queries_beir)}\")\n",
    "    print(f\"Val queries: {len(val_queries)}\")\n",
    "    \n",
    "    # Compute embeddings\n",
    "    print(\"\\nComputing embeddings...\")\n",
    "    test_embeddings = model.encode(test_queries, show_progress_bar=True)\n",
    "    train_embeddings = model.encode(train_queries, show_progress_bar=True)\n",
    "    test_beir_embeddings = model.encode(test_queries_beir, show_progress_bar=True)\n",
    "    val_embeddings = model.encode(val_queries, show_progress_bar=True)\n",
    "    \n",
    "    # 初始化统计\n",
    "    top5_distribution = {\n",
    "        'train': 0,\n",
    "        'test': 0,\n",
    "        'val': 0\n",
    "    }\n",
    "    \n",
    "    detailed_results = []\n",
    "    \n",
    "    print(\"\\nAnalyzing TOP5 distribution...\")\n",
    "    for i, test_query in enumerate(tqdm(test_queries)):\n",
    "        # Calculate similarity with each split\n",
    "        train_similarities = [(score, 'train', idx) for idx, score in enumerate(cosine_similarity([test_embeddings[i]], train_embeddings)[0])]\n",
    "        test_similarities = [(score, 'test', idx) for idx, score in enumerate(cosine_similarity([test_embeddings[i]], test_beir_embeddings)[0])]\n",
    "        val_similarities = [(score, 'val', idx) for idx, score in enumerate(cosine_similarity([test_embeddings[i]], val_embeddings)[0])]\n",
    "        \n",
    "        # Merge all similarities and get TOP5\n",
    "        all_similarities = train_similarities + test_similarities + val_similarities\n",
    "        top5 = sorted(all_similarities, key=lambda x: x[0], reverse=True)[:5]\n",
    "        \n",
    "        # Count the number of each split in TOP5\n",
    "        for sim_score, split, idx in top5:\n",
    "            top5_distribution[split] += 1\n",
    "            \n",
    "        # Get detailed information\n",
    "        query_details = {\n",
    "            'test_query': test_query,\n",
    "            'top5_matches': []\n",
    "        }\n",
    "        \n",
    "        for sim_score, split, idx in top5:\n",
    "            if split == 'train':\n",
    "                similar_query = train_queries[idx]\n",
    "            elif split == 'test':\n",
    "                similar_query = test_queries_beir[idx]\n",
    "            else:\n",
    "                similar_query = val_queries[idx]\n",
    "                \n",
    "            query_details['top5_matches'].append({\n",
    "                'split': split,\n",
    "                'query': similar_query,\n",
    "                'similarity': sim_score\n",
    "            })\n",
    "            \n",
    "        detailed_results.append(query_details)\n",
    "    \n",
    "    # Print statistics results\n",
    "    print(\"\\nDistribution of TOP5 similar queries:\")\n",
    "    total = sum(top5_distribution.values())\n",
    "    for split, count in top5_distribution.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{split}: {count} queries ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    print(\"\\nSaving detailed results...\")\n",
    "    rows = []\n",
    "    for result in detailed_results:\n",
    "        for i, match in enumerate(result['top5_matches'], 1):\n",
    "            rows.append({\n",
    "                'test_query': result['test_query'],\n",
    "                f'rank': i,\n",
    "                f'split': match['split'],\n",
    "                f'similar_query': match['query'],\n",
    "                f'similarity_score': match['similarity']\n",
    "            })\n",
    "    \n",
    "    detailed_df = pd.DataFrame(rows)\n",
    "    detailed_df.to_csv('query_top5_similarity_analysis.csv', index=False)\n",
    "    print(\"Detailed results saved to query_top5_similarity_analysis.csv\")\n",
    "    \n",
    "    # Print some examples\n",
    "    print(\"\\nExample matches (First 3 test queries):\")\n",
    "    for i in range(min(3, len(detailed_results))):\n",
    "        print(f\"\\nTest Query: {detailed_results[i]['test_query']}\")\n",
    "        for j, match in enumerate(detailed_results[i]['top5_matches'], 1):\n",
    "            print(f\"Rank {j} ({match['split']}): {match['query']}\")\n",
    "            print(f\"Similarity Score: {match['similarity']:.4f}\")\n",
    "    \n",
    "    return top5_distribution, detailed_results\n",
    "\n",
    "# Run analysis\n",
    "if __name__ == \"__main__\":\n",
    "    distribution_stats, detailed_results = analyze_top5_query_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Loaded 3125 valid document IDs and 557 test queries\n",
      "Loading BeIR dataset...\n",
      "\n",
      "Processing embeddings...\n",
      "Loading cached embeddings from embeddings_cache/query_embeddings_text-embedding-3-large.pkl\n",
      "Successfully loaded embeddings with shape: (557, 3072)\n",
      "Loading cached embeddings from embeddings_cache/beir_query_embeddings_text-embedding-3-large.pkl\n",
      "Successfully loaded embeddings with shape: (3237, 3072)\n",
      "Loading cached embeddings from embeddings_cache/beir_doc_embeddings_text-embedding-3-large.pkl\n",
      "Successfully loaded embeddings with shape: (3633, 3072)\n",
      "\n",
      "Processing queries and finding relevant documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/557 [00:00<00:40, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105 valid docs from 107 relevant docs, need 10\n",
      "Found 132 valid docs from 145 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/557 [00:00<00:34, 15.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 valid docs from 206 relevant docs, need 10\n",
      "Found 223 valid docs from 224 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/557 [00:00<00:36, 14.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 280 valid docs from 290 relevant docs, need 10\n",
      "Found 143 valid docs from 149 relevant docs, need 10\n",
      "Found 160 valid docs from 172 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/557 [00:00<00:34, 15.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1249 valid docs from 1265 relevant docs, need 10\n",
      "Found 192 valid docs from 197 relevant docs, need 10\n",
      "Found 238 valid docs from 245 relevant docs, need 10\n",
      "Found 249 valid docs from 256 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/557 [00:00<00:30, 17.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 407 valid docs from 415 relevant docs, need 10\n",
      "Found 341 valid docs from 352 relevant docs, need 10\n",
      "Found 91 valid docs from 94 relevant docs, need 10\n",
      "Found 244 valid docs from 256 relevant docs, need 10\n",
      "Found 116 valid docs from 117 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 18/557 [00:01<00:27, 19.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105 valid docs from 114 relevant docs, need 10\n",
      "Found 139 valid docs from 146 relevant docs, need 10\n",
      "Found 141 valid docs from 145 relevant docs, need 10\n",
      "Found 254 valid docs from 268 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 21/557 [00:01<00:26, 20.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 428 valid docs from 428 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 24/557 [00:01<00:26, 20.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 772 valid docs from 829 relevant docs, need 10\n",
      "Found 850 valid docs from 864 relevant docs, need 10\n",
      "Found 43 valid docs from 51 relevant docs, need 10\n",
      "Found 248 valid docs from 251 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 27/557 [00:01<00:26, 19.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 149 valid docs from 157 relevant docs, need 10\n",
      "Found 1240 valid docs from 1277 relevant docs, need 10\n",
      "Found 142 valid docs from 150 relevant docs, need 10\n",
      "Found 166 valid docs from 172 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 30/557 [00:01<00:26, 20.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173 valid docs from 188 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 33/557 [00:01<00:25, 20.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 121 valid docs from 138 relevant docs, need 10\n",
      "Found 167 valid docs from 177 relevant docs, need 10\n",
      "Found 297 valid docs from 309 relevant docs, need 10\n",
      "Found 219 valid docs from 225 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 39/557 [00:02<00:24, 21.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 255 valid docs from 262 relevant docs, need 10\n",
      "Found 616 valid docs from 629 relevant docs, need 10\n",
      "Found 298 valid docs from 303 relevant docs, need 10\n",
      "Found 912 valid docs from 945 relevant docs, need 10\n",
      "Found 133 valid docs from 141 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 45/557 [00:02<00:22, 23.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 104 valid docs from 118 relevant docs, need 10\n",
      "Found 190 valid docs from 194 relevant docs, need 10\n",
      "Found 801 valid docs from 840 relevant docs, need 10\n",
      "Found 92 valid docs from 92 relevant docs, need 10\n",
      "Found 264 valid docs from 268 relevant docs, need 10\n",
      "Found 100 valid docs from 108 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 48/557 [00:02<00:21, 23.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 142 valid docs from 157 relevant docs, need 10\n",
      "Found 153 valid docs from 156 relevant docs, need 10\n",
      "Found 146 valid docs from 159 relevant docs, need 10\n",
      "Found 158 valid docs from 170 relevant docs, need 10\n",
      "Found 289 valid docs from 289 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 54/557 [00:02<00:20, 24.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 248 valid docs from 264 relevant docs, need 10\n",
      "Found 354 valid docs from 355 relevant docs, need 10\n",
      "Found 61 valid docs from 63 relevant docs, need 10\n",
      "Found 173 valid docs from 179 relevant docs, need 10\n",
      "Found 514 valid docs from 515 relevant docs, need 10\n",
      "Found 233 valid docs from 238 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 61/557 [00:02<00:17, 27.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 587 valid docs from 606 relevant docs, need 10\n",
      "Found 83 valid docs from 99 relevant docs, need 10\n",
      "Found 321 valid docs from 326 relevant docs, need 10\n",
      "Found 653 valid docs from 684 relevant docs, need 10\n",
      "Found 166 valid docs from 168 relevant docs, need 10\n",
      "Found 363 valid docs from 375 relevant docs, need 10\n",
      "Found 291 valid docs from 298 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 69/557 [00:03<00:16, 29.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 114 valid docs from 116 relevant docs, need 10\n",
      "Found 470 valid docs from 490 relevant docs, need 10\n",
      "Found 488 valid docs from 500 relevant docs, need 10\n",
      "Found 280 valid docs from 285 relevant docs, need 10\n",
      "Found 301 valid docs from 303 relevant docs, need 10\n",
      "Found 1220 valid docs from 1258 relevant docs, need 10\n",
      "Found 138 valid docs from 139 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 76/557 [00:03<00:16, 28.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 251 valid docs from 258 relevant docs, need 10\n",
      "Found 504 valid docs from 510 relevant docs, need 10\n",
      "Found 477 valid docs from 484 relevant docs, need 10\n",
      "Found 685 valid docs from 708 relevant docs, need 10\n",
      "Found 157 valid docs from 161 relevant docs, need 10\n",
      "Found 296 valid docs from 303 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 83/557 [00:03<00:15, 29.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 220 valid docs from 223 relevant docs, need 10\n",
      "Found 150 valid docs from 162 relevant docs, need 10\n",
      "Found 150 valid docs from 163 relevant docs, need 10\n",
      "Found 52 valid docs from 54 relevant docs, need 10\n",
      "Found 187 valid docs from 191 relevant docs, need 10\n",
      "Found 773 valid docs from 790 relevant docs, need 10\n",
      "Found 451 valid docs from 458 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 86/557 [00:03<00:17, 27.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 169 valid docs from 186 relevant docs, need 10\n",
      "Found 376 valid docs from 396 relevant docs, need 10\n",
      "Found 1251 valid docs from 1289 relevant docs, need 10\n",
      "Found 265 valid docs from 267 relevant docs, need 10\n",
      "Found 223 valid docs from 244 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 92/557 [00:03<00:17, 26.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 136 valid docs from 140 relevant docs, need 10\n",
      "Found 261 valid docs from 266 relevant docs, need 10\n",
      "Found 948 valid docs from 988 relevant docs, need 10\n",
      "Found 320 valid docs from 326 relevant docs, need 10\n",
      "Found 773 valid docs from 830 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 99/557 [00:04<00:16, 27.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124 valid docs from 141 relevant docs, need 10\n",
      "Found 297 valid docs from 312 relevant docs, need 10\n",
      "Found 293 valid docs from 293 relevant docs, need 10\n",
      "Found 255 valid docs from 269 relevant docs, need 10\n",
      "Found 377 valid docs from 415 relevant docs, need 10\n",
      "Found 192 valid docs from 194 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 106/557 [00:04<00:15, 29.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 250 valid docs from 261 relevant docs, need 10\n",
      "Found 132 valid docs from 132 relevant docs, need 10\n",
      "Found 279 valid docs from 285 relevant docs, need 10\n",
      "Found 128 valid docs from 138 relevant docs, need 10\n",
      "Found 401 valid docs from 403 relevant docs, need 10\n",
      "Found 1019 valid docs from 1027 relevant docs, need 10\n",
      "Found 365 valid docs from 394 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 110/557 [00:04<00:14, 30.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 620 valid docs from 638 relevant docs, need 10\n",
      "Found 228 valid docs from 233 relevant docs, need 10\n",
      "Found 203 valid docs from 203 relevant docs, need 10\n",
      "Found 77 valid docs from 81 relevant docs, need 10\n",
      "Found 126 valid docs from 126 relevant docs, need 10\n",
      "Found 348 valid docs from 355 relevant docs, need 10\n",
      "Found 244 valid docs from 244 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 118/557 [00:04<00:14, 30.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 203 valid docs from 204 relevant docs, need 10\n",
      "Found 423 valid docs from 452 relevant docs, need 10\n",
      "Found 133 valid docs from 134 relevant docs, need 10\n",
      "Found 198 valid docs from 202 relevant docs, need 10\n",
      "Found 1014 valid docs from 1025 relevant docs, need 10\n",
      "Found 212 valid docs from 213 relevant docs, need 10\n",
      "Found 291 valid docs from 299 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 125/557 [00:05<00:15, 27.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1205 valid docs from 1241 relevant docs, need 10\n",
      "Found 480 valid docs from 491 relevant docs, need 10\n",
      "Found 287 valid docs from 299 relevant docs, need 10\n",
      "Found 356 valid docs from 367 relevant docs, need 10\n",
      "Found 537 valid docs from 538 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 132/557 [00:05<00:15, 27.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 217 valid docs from 217 relevant docs, need 10\n",
      "Found 140 valid docs from 140 relevant docs, need 10\n",
      "Found 88 valid docs from 96 relevant docs, need 10\n",
      "Found 294 valid docs from 299 relevant docs, need 10\n",
      "Found 225 valid docs from 239 relevant docs, need 10\n",
      "Found 1205 valid docs from 1242 relevant docs, need 10\n",
      "Found 386 valid docs from 393 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 136/557 [00:05<00:14, 29.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 141 valid docs from 149 relevant docs, need 10\n",
      "Found 206 valid docs from 220 relevant docs, need 10\n",
      "Found 323 valid docs from 325 relevant docs, need 10\n",
      "Found 311 valid docs from 316 relevant docs, need 10\n",
      "Found 234 valid docs from 241 relevant docs, need 10\n",
      "Found 811 valid docs from 841 relevant docs, need 10\n",
      "Found 188 valid docs from 193 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 144/557 [00:05<00:13, 30.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 106 valid docs from 106 relevant docs, need 10\n",
      "Found 310 valid docs from 344 relevant docs, need 10\n",
      "Found 106 valid docs from 112 relevant docs, need 10\n",
      "Found 412 valid docs from 419 relevant docs, need 10\n",
      "Found 149 valid docs from 162 relevant docs, need 10\n",
      "Found 448 valid docs from 469 relevant docs, need 10\n",
      "Found 433 valid docs from 440 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 152/557 [00:05<00:12, 31.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124 valid docs from 125 relevant docs, need 10\n",
      "Found 148 valid docs from 162 relevant docs, need 10\n",
      "Found 857 valid docs from 886 relevant docs, need 10\n",
      "Found 161 valid docs from 165 relevant docs, need 10\n",
      "Found 161 valid docs from 165 relevant docs, need 10\n",
      "Found 169 valid docs from 174 relevant docs, need 10\n",
      "Found 109 valid docs from 109 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 160/557 [00:06<00:12, 32.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 197 valid docs from 209 relevant docs, need 10\n",
      "Found 198 valid docs from 199 relevant docs, need 10\n",
      "Found 160 valid docs from 164 relevant docs, need 10\n",
      "Found 530 valid docs from 536 relevant docs, need 10\n",
      "Found 283 valid docs from 299 relevant docs, need 10\n",
      "Found 234 valid docs from 238 relevant docs, need 10\n",
      "Found 185 valid docs from 198 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 164/557 [00:06<00:12, 32.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 258 valid docs from 270 relevant docs, need 10\n",
      "Found 195 valid docs from 197 relevant docs, need 10\n",
      "Found 355 valid docs from 369 relevant docs, need 10\n",
      "Found 385 valid docs from 403 relevant docs, need 10\n",
      "Found 311 valid docs from 318 relevant docs, need 10\n",
      "Found 1010 valid docs from 1052 relevant docs, need 10\n",
      "Found 301 valid docs from 311 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 172/557 [00:06<00:11, 32.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 301 valid docs from 315 relevant docs, need 10\n",
      "Found 149 valid docs from 157 relevant docs, need 10\n",
      "Found 118 valid docs from 124 relevant docs, need 10\n",
      "Found 118 valid docs from 123 relevant docs, need 10\n",
      "Found 656 valid docs from 676 relevant docs, need 10\n",
      "Found 219 valid docs from 221 relevant docs, need 10\n",
      "Found 168 valid docs from 173 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 180/557 [00:06<00:12, 31.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 470 valid docs from 487 relevant docs, need 10\n",
      "Found 124 valid docs from 125 relevant docs, need 10\n",
      "Found 270 valid docs from 274 relevant docs, need 10\n",
      "Found 120 valid docs from 121 relevant docs, need 10\n",
      "Found 225 valid docs from 226 relevant docs, need 10\n",
      "Found 200 valid docs from 200 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 184/557 [00:07<00:12, 30.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103 valid docs from 106 relevant docs, need 10\n",
      "Found 335 valid docs from 346 relevant docs, need 10\n",
      "Found 218 valid docs from 227 relevant docs, need 10\n",
      "Found 1231 valid docs from 1243 relevant docs, need 10\n",
      "Found 79 valid docs from 81 relevant docs, need 10\n",
      "Found 653 valid docs from 684 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 192/557 [00:07<00:12, 29.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 970 valid docs from 1011 relevant docs, need 10\n",
      "Found 425 valid docs from 431 relevant docs, need 10\n",
      "Found 308 valid docs from 337 relevant docs, need 10\n",
      "Found 376 valid docs from 388 relevant docs, need 10\n",
      "Found 284 valid docs from 298 relevant docs, need 10\n",
      "Found 263 valid docs from 264 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 198/557 [00:07<00:12, 28.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 364 valid docs from 366 relevant docs, need 10\n",
      "Found 118 valid docs from 118 relevant docs, need 10\n",
      "Found 307 valid docs from 309 relevant docs, need 10\n",
      "Found 210 valid docs from 213 relevant docs, need 10\n",
      "Found 649 valid docs from 675 relevant docs, need 10\n",
      "Found 517 valid docs from 518 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 205/557 [00:07<00:11, 30.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 803 valid docs from 833 relevant docs, need 10\n",
      "Found 363 valid docs from 364 relevant docs, need 10\n",
      "Found 193 valid docs from 203 relevant docs, need 10\n",
      "Found 69 valid docs from 75 relevant docs, need 10\n",
      "Found 204 valid docs from 206 relevant docs, need 10\n",
      "Found 227 valid docs from 229 relevant docs, need 10\n",
      "Found 120 valid docs from 121 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 209/557 [00:07<00:11, 31.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 161 valid docs from 161 relevant docs, need 10\n",
      "Found 273 valid docs from 273 relevant docs, need 10\n",
      "Found 184 valid docs from 186 relevant docs, need 10\n",
      "Found 458 valid docs from 472 relevant docs, need 10\n",
      "Found 182 valid docs from 186 relevant docs, need 10\n",
      "Found 96 valid docs from 106 relevant docs, need 10\n",
      "Found 220 valid docs from 238 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 217/557 [00:08<00:11, 28.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153 valid docs from 158 relevant docs, need 10\n",
      "Found 240 valid docs from 246 relevant docs, need 10\n",
      "Found 219 valid docs from 224 relevant docs, need 10\n",
      "Found 741 valid docs from 746 relevant docs, need 10\n",
      "Found 185 valid docs from 203 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 220/557 [00:08<00:12, 26.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 valid docs from 131 relevant docs, need 10\n",
      "Found 176 valid docs from 183 relevant docs, need 10\n",
      "Found 550 valid docs from 554 relevant docs, need 10\n",
      "Found 186 valid docs from 201 relevant docs, need 10\n",
      "Found 279 valid docs from 285 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 226/557 [00:08<00:12, 25.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 458 valid docs from 458 relevant docs, need 10\n",
      "Found 488 valid docs from 500 relevant docs, need 10\n",
      "Found 259 valid docs from 267 relevant docs, need 10\n",
      "Found 234 valid docs from 238 relevant docs, need 10\n",
      "Found 91 valid docs from 94 relevant docs, need 10\n",
      "Found 260 valid docs from 265 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 232/557 [00:08<00:12, 26.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144 valid docs from 158 relevant docs, need 10\n",
      "Found 162 valid docs from 165 relevant docs, need 10\n",
      "Found 157 valid docs from 159 relevant docs, need 10\n",
      "Found 310 valid docs from 344 relevant docs, need 10\n",
      "Found 187 valid docs from 196 relevant docs, need 10\n",
      "Found 138 valid docs from 139 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 238/557 [00:08<00:12, 25.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 159 valid docs from 160 relevant docs, need 10\n",
      "Found 276 valid docs from 284 relevant docs, need 10\n",
      "Found 82 valid docs from 82 relevant docs, need 10\n",
      "Found 356 valid docs from 376 relevant docs, need 10\n",
      "Found 190 valid docs from 196 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 244/557 [00:09<00:12, 25.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 281 valid docs from 297 relevant docs, need 10\n",
      "Found 210 valid docs from 220 relevant docs, need 10\n",
      "Found 838 valid docs from 881 relevant docs, need 10\n",
      "Found 218 valid docs from 218 relevant docs, need 10\n",
      "Found 590 valid docs from 607 relevant docs, need 10\n",
      "Found 330 valid docs from 334 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 250/557 [00:09<00:11, 26.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 209 valid docs from 210 relevant docs, need 10\n",
      "Found 384 valid docs from 384 relevant docs, need 10\n",
      "Found 199 valid docs from 206 relevant docs, need 10\n",
      "Found 234 valid docs from 235 relevant docs, need 10\n",
      "Found 82 valid docs from 83 relevant docs, need 10\n",
      "Found 289 valid docs from 290 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 257/557 [00:09<00:10, 27.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 183 valid docs from 188 relevant docs, need 10\n",
      "Found 219 valid docs from 226 relevant docs, need 10\n",
      "Found 593 valid docs from 601 relevant docs, need 10\n",
      "Found 366 valid docs from 383 relevant docs, need 10\n",
      "Found 618 valid docs from 637 relevant docs, need 10\n",
      "Found 169 valid docs from 171 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 260/557 [00:09<00:10, 27.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99 valid docs from 114 relevant docs, need 10\n",
      "Found 234 valid docs from 238 relevant docs, need 10\n",
      "Found 114 valid docs from 115 relevant docs, need 10\n",
      "Found 480 valid docs from 480 relevant docs, need 10\n",
      "Found 141 valid docs from 151 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 266/557 [00:10<00:14, 20.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 193 valid docs from 223 relevant docs, need 10\n",
      "Found 359 valid docs from 363 relevant docs, need 10\n",
      "Found 307 valid docs from 313 relevant docs, need 10\n",
      "Found 171 valid docs from 183 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 272/557 [00:10<00:12, 22.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 520 valid docs from 521 relevant docs, need 10\n",
      "Found 187 valid docs from 220 relevant docs, need 10\n",
      "Found 950 valid docs from 985 relevant docs, need 10\n",
      "Found 224 valid docs from 225 relevant docs, need 10\n",
      "Found 145 valid docs from 151 relevant docs, need 10\n",
      "Found 145 valid docs from 151 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 278/557 [00:10<00:11, 23.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 511 valid docs from 520 relevant docs, need 10\n",
      "Found 55 valid docs from 56 relevant docs, need 10\n",
      "Found 51 valid docs from 51 relevant docs, need 10\n",
      "Found 484 valid docs from 496 relevant docs, need 10\n",
      "Found 305 valid docs from 312 relevant docs, need 10\n",
      "Found 403 valid docs from 405 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 284/557 [00:10<00:11, 24.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 201 valid docs from 229 relevant docs, need 10\n",
      "Found 140 valid docs from 140 relevant docs, need 10\n",
      "Found 285 valid docs from 296 relevant docs, need 10\n",
      "Found 195 valid docs from 199 relevant docs, need 10\n",
      "Found 107 valid docs from 112 relevant docs, need 10\n",
      "Found 628 valid docs from 634 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 290/557 [00:11<00:10, 25.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244 valid docs from 250 relevant docs, need 10\n",
      "Found 53 valid docs from 53 relevant docs, need 10\n",
      "Found 300 valid docs from 310 relevant docs, need 10\n",
      "Found 198 valid docs from 198 relevant docs, need 10\n",
      "Found 329 valid docs from 342 relevant docs, need 10\n",
      "Found 187 valid docs from 187 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 294/557 [00:11<00:09, 27.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1024 valid docs from 1032 relevant docs, need 10\n",
      "Found 111 valid docs from 111 relevant docs, need 10\n",
      "Found 597 valid docs from 605 relevant docs, need 10\n",
      "Found 217 valid docs from 238 relevant docs, need 10\n",
      "Found 53 valid docs from 67 relevant docs, need 10\n",
      "Found 328 valid docs from 339 relevant docs, need 10\n",
      "Found 1096 valid docs from 1137 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 301/557 [00:11<00:10, 25.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174 valid docs from 174 relevant docs, need 10\n",
      "Found 460 valid docs from 464 relevant docs, need 10\n",
      "Found 123 valid docs from 123 relevant docs, need 10\n",
      "Found 110 valid docs from 128 relevant docs, need 10\n",
      "Found 218 valid docs from 220 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 308/557 [00:11<00:08, 27.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 509 valid docs from 510 relevant docs, need 10\n",
      "Found 635 valid docs from 655 relevant docs, need 10\n",
      "Found 164 valid docs from 166 relevant docs, need 10\n",
      "Found 105 valid docs from 106 relevant docs, need 10\n",
      "Found 300 valid docs from 301 relevant docs, need 10\n",
      "Found 1269 valid docs from 1310 relevant docs, need 10\n",
      "Found 87 valid docs from 90 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 316/557 [00:12<00:08, 29.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 valid docs from 151 relevant docs, need 10\n",
      "Found 801 valid docs from 859 relevant docs, need 10\n",
      "Found 198 valid docs from 198 relevant docs, need 10\n",
      "Found 165 valid docs from 169 relevant docs, need 10\n",
      "Found 109 valid docs from 119 relevant docs, need 10\n",
      "Found 226 valid docs from 230 relevant docs, need 10\n",
      "Found 246 valid docs from 250 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 323/557 [00:12<00:08, 29.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 194 valid docs from 198 relevant docs, need 10\n",
      "Found 85 valid docs from 90 relevant docs, need 10\n",
      "Found 220 valid docs from 222 relevant docs, need 10\n",
      "Found 259 valid docs from 277 relevant docs, need 10\n",
      "Found 1159 valid docs from 1170 relevant docs, need 10\n",
      "Found 161 valid docs from 163 relevant docs, need 10\n",
      "Found 196 valid docs from 197 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 329/557 [00:12<00:08, 27.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124 valid docs from 125 relevant docs, need 10\n",
      "Found 763 valid docs from 765 relevant docs, need 10\n",
      "Found 320 valid docs from 321 relevant docs, need 10\n",
      "Found 362 valid docs from 369 relevant docs, need 10\n",
      "Found 238 valid docs from 252 relevant docs, need 10\n",
      "Found 90 valid docs from 90 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 332/557 [00:12<00:08, 25.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 199 valid docs from 206 relevant docs, need 10\n",
      "Found 115 valid docs from 119 relevant docs, need 10\n",
      "Found 706 valid docs from 741 relevant docs, need 10\n",
      "Found 171 valid docs from 178 relevant docs, need 10\n",
      "Found 234 valid docs from 250 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 338/557 [00:12<00:09, 23.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 290 valid docs from 297 relevant docs, need 10\n",
      "Found 119 valid docs from 130 relevant docs, need 10\n",
      "Found 145 valid docs from 145 relevant docs, need 10\n",
      "Found 152 valid docs from 153 relevant docs, need 10\n",
      "Found 215 valid docs from 232 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 344/557 [00:13<00:08, 24.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 345 valid docs from 354 relevant docs, need 10\n",
      "Found 278 valid docs from 283 relevant docs, need 10\n",
      "Found 365 valid docs from 368 relevant docs, need 10\n",
      "Found 480 valid docs from 501 relevant docs, need 10\n",
      "Found 348 valid docs from 357 relevant docs, need 10\n",
      "Found 966 valid docs from 1002 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 350/557 [00:13<00:08, 24.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 valid docs from 204 relevant docs, need 10\n",
      "Found 412 valid docs from 428 relevant docs, need 10\n",
      "Found 292 valid docs from 297 relevant docs, need 10\n",
      "Found 79 valid docs from 85 relevant docs, need 10\n",
      "Found 498 valid docs from 514 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 353/557 [00:13<00:08, 24.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 549 valid docs from 553 relevant docs, need 10\n",
      "Found 376 valid docs from 381 relevant docs, need 10\n",
      "Found 164 valid docs from 171 relevant docs, need 10\n",
      "Found 275 valid docs from 276 relevant docs, need 10\n",
      "Found 133 valid docs from 138 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 359/557 [00:13<00:09, 21.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 226 valid docs from 228 relevant docs, need 10\n",
      "Found 309 valid docs from 316 relevant docs, need 10\n",
      "Found 190 valid docs from 193 relevant docs, need 10\n",
      "Found 199 valid docs from 207 relevant docs, need 10\n",
      "Found 100 valid docs from 119 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 365/557 [00:14<00:07, 24.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74 valid docs from 74 relevant docs, need 10\n",
      "Found 119 valid docs from 119 relevant docs, need 10\n",
      "Found 226 valid docs from 239 relevant docs, need 10\n",
      "Found 102 valid docs from 110 relevant docs, need 10\n",
      "Found 102 valid docs from 103 relevant docs, need 10\n",
      "Found 383 valid docs from 398 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 371/557 [00:14<00:07, 25.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 283 valid docs from 283 relevant docs, need 10\n",
      "Found 220 valid docs from 223 relevant docs, need 10\n",
      "Found 187 valid docs from 220 relevant docs, need 10\n",
      "Found 134 valid docs from 147 relevant docs, need 10\n",
      "Found 86 valid docs from 98 relevant docs, need 10\n",
      "Found 197 valid docs from 202 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 377/557 [00:14<00:07, 25.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103 valid docs from 116 relevant docs, need 10\n",
      "Found 131 valid docs from 138 relevant docs, need 10\n",
      "Found 644 valid docs from 680 relevant docs, need 10\n",
      "Found 248 valid docs from 251 relevant docs, need 10\n",
      "Found 138 valid docs from 142 relevant docs, need 10\n",
      "Found 328 valid docs from 330 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 380/557 [00:14<00:08, 21.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 valid docs from 200 relevant docs, need 10\n",
      "Found 395 valid docs from 402 relevant docs, need 10\n",
      "Found 695 valid docs from 717 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 386/557 [00:14<00:07, 22.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 valid docs from 63 relevant docs, need 10\n",
      "Found 278 valid docs from 288 relevant docs, need 10\n",
      "Found 190 valid docs from 191 relevant docs, need 10\n",
      "Found 301 valid docs from 301 relevant docs, need 10\n",
      "Found 300 valid docs from 301 relevant docs, need 10\n",
      "Found 183 valid docs from 188 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 393/557 [00:15<00:06, 26.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 226 valid docs from 230 relevant docs, need 10\n",
      "Found 145 valid docs from 145 relevant docs, need 10\n",
      "Found 69 valid docs from 75 relevant docs, need 10\n",
      "Found 142 valid docs from 147 relevant docs, need 10\n",
      "Found 252 valid docs from 264 relevant docs, need 10\n",
      "Found 366 valid docs from 373 relevant docs, need 10\n",
      "Found 667 valid docs from 683 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 401/557 [00:15<00:05, 30.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 115 valid docs from 116 relevant docs, need 10\n",
      "Found 77 valid docs from 92 relevant docs, need 10\n",
      "Found 251 valid docs from 259 relevant docs, need 10\n",
      "Found 52 valid docs from 52 relevant docs, need 10\n",
      "Found 579 valid docs from 595 relevant docs, need 10\n",
      "Found 156 valid docs from 156 relevant docs, need 10\n",
      "Found 326 valid docs from 350 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 405/557 [00:15<00:05, 29.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 121 valid docs from 123 relevant docs, need 10\n",
      "Found 81 valid docs from 98 relevant docs, need 10\n",
      "Found 102 valid docs from 103 relevant docs, need 10\n",
      "Found 171 valid docs from 172 relevant docs, need 10\n",
      "Found 490 valid docs from 506 relevant docs, need 10\n",
      "Found 187 valid docs from 189 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 411/557 [00:15<00:04, 29.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 342 valid docs from 361 relevant docs, need 10\n",
      "Found 290 valid docs from 310 relevant docs, need 10\n",
      "Found 114 valid docs from 115 relevant docs, need 10\n",
      "Found 58 valid docs from 61 relevant docs, need 10\n",
      "Found 153 valid docs from 158 relevant docs, need 10\n",
      "Found 198 valid docs from 199 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 419/557 [00:16<00:04, 30.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74 valid docs from 75 relevant docs, need 10\n",
      "Found 155 valid docs from 155 relevant docs, need 10\n",
      "Found 68 valid docs from 69 relevant docs, need 10\n",
      "Found 120 valid docs from 120 relevant docs, need 10\n",
      "Found 144 valid docs from 151 relevant docs, need 10\n",
      "Found 523 valid docs from 529 relevant docs, need 10\n",
      "Found 73 valid docs from 73 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 423/557 [00:16<00:04, 30.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 973 valid docs from 1014 relevant docs, need 10\n",
      "Found 1011 valid docs from 1019 relevant docs, need 10\n",
      "Found 353 valid docs from 372 relevant docs, need 10\n",
      "Found 127 valid docs from 140 relevant docs, need 10\n",
      "Found 296 valid docs from 310 relevant docs, need 10\n",
      "Found 168 valid docs from 168 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 430/557 [00:16<00:04, 28.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 354 valid docs from 370 relevant docs, need 10\n",
      "Found 368 valid docs from 377 relevant docs, need 10\n",
      "Found 623 valid docs from 636 relevant docs, need 10\n",
      "Found 148 valid docs from 158 relevant docs, need 10\n",
      "Found 81 valid docs from 82 relevant docs, need 10\n",
      "Found 104 valid docs from 114 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 436/557 [00:16<00:04, 27.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 409 valid docs from 409 relevant docs, need 10\n",
      "Found 821 valid docs from 836 relevant docs, need 10\n",
      "Found 326 valid docs from 329 relevant docs, need 10\n",
      "Found 105 valid docs from 107 relevant docs, need 10\n",
      "Found 195 valid docs from 200 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 442/557 [00:16<00:04, 26.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1017 valid docs from 1059 relevant docs, need 10\n",
      "Found 199 valid docs from 204 relevant docs, need 10\n",
      "Found 340 valid docs from 360 relevant docs, need 10\n",
      "Found 1220 valid docs from 1258 relevant docs, need 10\n",
      "Found 348 valid docs from 352 relevant docs, need 10\n",
      "Found 1156 valid docs from 1205 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 448/557 [00:17<00:04, 24.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 266 valid docs from 282 relevant docs, need 10\n",
      "Found 792 valid docs from 795 relevant docs, need 10\n",
      "Found 707 valid docs from 715 relevant docs, need 10\n",
      "Found 458 valid docs from 467 relevant docs, need 10\n",
      "Found 576 valid docs from 610 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 454/557 [00:17<00:04, 25.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51 valid docs from 51 relevant docs, need 10\n",
      "Found 271 valid docs from 284 relevant docs, need 10\n",
      "Found 278 valid docs from 287 relevant docs, need 10\n",
      "Found 498 valid docs from 514 relevant docs, need 10\n",
      "Found 183 valid docs from 184 relevant docs, need 10\n",
      "Found 176 valid docs from 182 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 460/557 [00:17<00:03, 26.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86 valid docs from 86 relevant docs, need 10\n",
      "Found 194 valid docs from 194 relevant docs, need 10\n",
      "Found 243 valid docs from 244 relevant docs, need 10\n",
      "Found 242 valid docs from 256 relevant docs, need 10\n",
      "Found 210 valid docs from 215 relevant docs, need 10\n",
      "Found 183 valid docs from 190 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 466/557 [00:17<00:03, 27.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 325 valid docs from 327 relevant docs, need 10\n",
      "Found 114 valid docs from 115 relevant docs, need 10\n",
      "Found 137 valid docs from 137 relevant docs, need 10\n",
      "Found 141 valid docs from 145 relevant docs, need 10\n",
      "Found 402 valid docs from 411 relevant docs, need 10\n",
      "Found 486 valid docs from 494 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 469/557 [00:17<00:03, 27.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 587 valid docs from 600 relevant docs, need 10\n",
      "Found 132 valid docs from 132 relevant docs, need 10\n",
      "Found 102 valid docs from 110 relevant docs, need 10\n",
      "Found 485 valid docs from 500 relevant docs, need 10\n",
      "Found 174 valid docs from 176 relevant docs, need 10\n",
      "Found 246 valid docs from 248 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 476/557 [00:18<00:02, 27.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179 valid docs from 180 relevant docs, need 10\n",
      "Found 1195 valid docs from 1238 relevant docs, need 10\n",
      "Found 330 valid docs from 351 relevant docs, need 10\n",
      "Found 808 valid docs from 829 relevant docs, need 10\n",
      "Found 412 valid docs from 415 relevant docs, need 10\n",
      "Found 1198 valid docs from 1243 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 482/557 [00:18<00:02, 26.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1170 valid docs from 1215 relevant docs, need 10\n",
      "Found 117 valid docs from 128 relevant docs, need 10\n",
      "Found 676 valid docs from 689 relevant docs, need 10\n",
      "Found 821 valid docs from 836 relevant docs, need 10\n",
      "Found 206 valid docs from 210 relevant docs, need 10\n",
      "Found 278 valid docs from 282 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 488/557 [00:18<00:02, 25.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 201 valid docs from 229 relevant docs, need 10\n",
      "Found 163 valid docs from 171 relevant docs, need 10\n",
      "Found 446 valid docs from 446 relevant docs, need 10\n",
      "Found 91 valid docs from 105 relevant docs, need 10\n",
      "Found 129 valid docs from 140 relevant docs, need 10\n",
      "Found 174 valid docs from 178 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 495/557 [00:18<00:02, 27.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 487 valid docs from 505 relevant docs, need 10\n",
      "Found 410 valid docs from 411 relevant docs, need 10\n",
      "Found 151 valid docs from 155 relevant docs, need 10\n",
      "Found 79 valid docs from 86 relevant docs, need 10\n",
      "Found 167 valid docs from 173 relevant docs, need 10\n",
      "Found 228 valid docs from 229 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 502/557 [00:19<00:01, 27.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 430 valid docs from 439 relevant docs, need 10\n",
      "Found 107 valid docs from 112 relevant docs, need 10\n",
      "Found 587 valid docs from 606 relevant docs, need 10\n",
      "Found 449 valid docs from 463 relevant docs, need 10\n",
      "Found 176 valid docs from 176 relevant docs, need 10\n",
      "Found 248 valid docs from 266 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 506/557 [00:19<00:01, 29.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 506 valid docs from 511 relevant docs, need 10\n",
      "Found 247 valid docs from 253 relevant docs, need 10\n",
      "Found 315 valid docs from 326 relevant docs, need 10\n",
      "Found 150 valid docs from 159 relevant docs, need 10\n",
      "Found 164 valid docs from 173 relevant docs, need 10\n",
      "Found 350 valid docs from 356 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 512/557 [00:19<00:01, 27.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 246 valid docs from 249 relevant docs, need 10\n",
      "Found 236 valid docs from 247 relevant docs, need 10\n",
      "Found 102 valid docs from 102 relevant docs, need 10\n",
      "Found 209 valid docs from 223 relevant docs, need 10\n",
      "Found 263 valid docs from 263 relevant docs, need 10\n",
      "Found 147 valid docs from 156 relevant docs, need 10\n",
      "Found 189 valid docs from 189 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 520/557 [00:19<00:01, 30.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 325 valid docs from 341 relevant docs, need 10\n",
      "Found 579 valid docs from 608 relevant docs, need 10\n",
      "Found 310 valid docs from 318 relevant docs, need 10\n",
      "Found 184 valid docs from 186 relevant docs, need 10\n",
      "Found 188 valid docs from 195 relevant docs, need 10\n",
      "Found 228 valid docs from 239 relevant docs, need 10\n",
      "Found 179 valid docs from 179 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 528/557 [00:19<00:00, 31.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 391 valid docs from 392 relevant docs, need 10\n",
      "Found 152 valid docs from 158 relevant docs, need 10\n",
      "Found 253 valid docs from 262 relevant docs, need 10\n",
      "Found 462 valid docs from 478 relevant docs, need 10\n",
      "Found 179 valid docs from 180 relevant docs, need 10\n",
      "Found 1096 valid docs from 1137 relevant docs, need 10\n",
      "Found 169 valid docs from 183 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 536/557 [00:20<00:00, 31.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 valid docs from 561 relevant docs, need 10\n",
      "Found 193 valid docs from 200 relevant docs, need 10\n",
      "Found 262 valid docs from 263 relevant docs, need 10\n",
      "Found 384 valid docs from 384 relevant docs, need 10\n",
      "Found 238 valid docs from 240 relevant docs, need 10\n",
      "Found 268 valid docs from 276 relevant docs, need 10\n",
      "Found 94 valid docs from 95 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 540/557 [00:20<00:00, 29.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 251 valid docs from 257 relevant docs, need 10\n",
      "Found 218 valid docs from 230 relevant docs, need 10\n",
      "Found 425 valid docs from 431 relevant docs, need 10\n",
      "Found 1244 valid docs from 1256 relevant docs, need 10\n",
      "Found 43 valid docs from 51 relevant docs, need 10\n",
      "Found 356 valid docs from 361 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 546/557 [00:20<00:00, 26.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 valid docs from 126 relevant docs, need 10\n",
      "Found 229 valid docs from 245 relevant docs, need 10\n",
      "Found 157 valid docs from 159 relevant docs, need 10\n",
      "Found 193 valid docs from 199 relevant docs, need 10\n",
      "Found 362 valid docs from 371 relevant docs, need 10\n",
      "Found 359 valid docs from 396 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 552/557 [00:20<00:00, 27.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1009 valid docs from 1050 relevant docs, need 10\n",
      "Found 322 valid docs from 333 relevant docs, need 10\n",
      "Found 192 valid docs from 197 relevant docs, need 10\n",
      "Found 186 valid docs from 196 relevant docs, need 10\n",
      "Found 51 valid docs from 51 relevant docs, need 10\n",
      "Found 259 valid docs from 269 relevant docs, need 10\n",
      "Found 91 valid docs from 94 relevant docs, need 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [00:21<00:00, 26.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1263 valid docs from 1304 relevant docs, need 10\n",
      "Found 323 valid docs from 323 relevant docs, need 10\n",
      "\n",
      "Submission files created successfully!\n",
      "\n",
      "Statistics:\n",
      "Total queries processed: 557\n",
      "\n",
      "Distribution of original relevant documents per query:\n",
      "Queries with 43 original relevant docs: 2 (0.36%)\n",
      "Queries with 51 original relevant docs: 3 (0.54%)\n",
      "Queries with 52 original relevant docs: 2 (0.36%)\n",
      "Queries with 53 original relevant docs: 2 (0.36%)\n",
      "Queries with 55 original relevant docs: 1 (0.18%)\n",
      "Queries with 58 original relevant docs: 2 (0.36%)\n",
      "Queries with 61 original relevant docs: 1 (0.18%)\n",
      "Queries with 68 original relevant docs: 1 (0.18%)\n",
      "Queries with 69 original relevant docs: 2 (0.36%)\n",
      "Queries with 73 original relevant docs: 1 (0.18%)\n",
      "Queries with 74 original relevant docs: 2 (0.36%)\n",
      "Queries with 77 original relevant docs: 2 (0.36%)\n",
      "Queries with 79 original relevant docs: 3 (0.54%)\n",
      "Queries with 81 original relevant docs: 2 (0.36%)\n",
      "Queries with 82 original relevant docs: 2 (0.36%)\n",
      "Queries with 83 original relevant docs: 1 (0.18%)\n",
      "Queries with 85 original relevant docs: 1 (0.18%)\n",
      "Queries with 86 original relevant docs: 2 (0.36%)\n",
      "Queries with 87 original relevant docs: 1 (0.18%)\n",
      "Queries with 88 original relevant docs: 1 (0.18%)\n",
      "Queries with 90 original relevant docs: 1 (0.18%)\n",
      "Queries with 91 original relevant docs: 4 (0.72%)\n",
      "Queries with 92 original relevant docs: 1 (0.18%)\n",
      "Queries with 94 original relevant docs: 1 (0.18%)\n",
      "Queries with 96 original relevant docs: 1 (0.18%)\n",
      "Queries with 99 original relevant docs: 1 (0.18%)\n",
      "Queries with 100 original relevant docs: 2 (0.36%)\n",
      "Queries with 102 original relevant docs: 5 (0.90%)\n",
      "Queries with 103 original relevant docs: 2 (0.36%)\n",
      "Queries with 104 original relevant docs: 2 (0.36%)\n",
      "Queries with 105 original relevant docs: 4 (0.72%)\n",
      "Queries with 106 original relevant docs: 2 (0.36%)\n",
      "Queries with 107 original relevant docs: 2 (0.36%)\n",
      "Queries with 109 original relevant docs: 2 (0.36%)\n",
      "Queries with 110 original relevant docs: 1 (0.18%)\n",
      "Queries with 111 original relevant docs: 1 (0.18%)\n",
      "Queries with 114 original relevant docs: 4 (0.72%)\n",
      "Queries with 115 original relevant docs: 2 (0.36%)\n",
      "Queries with 116 original relevant docs: 1 (0.18%)\n",
      "Queries with 117 original relevant docs: 1 (0.18%)\n",
      "Queries with 118 original relevant docs: 3 (0.54%)\n",
      "Queries with 119 original relevant docs: 2 (0.36%)\n",
      "Queries with 120 original relevant docs: 4 (0.72%)\n",
      "Queries with 121 original relevant docs: 2 (0.36%)\n",
      "Queries with 123 original relevant docs: 3 (0.54%)\n",
      "Queries with 124 original relevant docs: 4 (0.72%)\n",
      "Queries with 126 original relevant docs: 1 (0.18%)\n",
      "Queries with 127 original relevant docs: 1 (0.18%)\n",
      "Queries with 128 original relevant docs: 1 (0.18%)\n",
      "Queries with 129 original relevant docs: 1 (0.18%)\n",
      "Queries with 131 original relevant docs: 1 (0.18%)\n",
      "Queries with 132 original relevant docs: 3 (0.54%)\n",
      "Queries with 133 original relevant docs: 3 (0.54%)\n",
      "Queries with 134 original relevant docs: 1 (0.18%)\n",
      "Queries with 136 original relevant docs: 1 (0.18%)\n",
      "Queries with 137 original relevant docs: 1 (0.18%)\n",
      "Queries with 138 original relevant docs: 3 (0.54%)\n",
      "Queries with 139 original relevant docs: 1 (0.18%)\n",
      "Queries with 140 original relevant docs: 2 (0.36%)\n",
      "Queries with 141 original relevant docs: 4 (0.72%)\n",
      "Queries with 142 original relevant docs: 3 (0.54%)\n",
      "Queries with 143 original relevant docs: 1 (0.18%)\n",
      "Queries with 144 original relevant docs: 2 (0.36%)\n",
      "Queries with 145 original relevant docs: 4 (0.72%)\n",
      "Queries with 146 original relevant docs: 1 (0.18%)\n",
      "Queries with 147 original relevant docs: 1 (0.18%)\n",
      "Queries with 148 original relevant docs: 2 (0.36%)\n",
      "Queries with 149 original relevant docs: 3 (0.54%)\n",
      "Queries with 150 original relevant docs: 3 (0.54%)\n",
      "Queries with 151 original relevant docs: 1 (0.18%)\n",
      "Queries with 152 original relevant docs: 2 (0.36%)\n",
      "Queries with 153 original relevant docs: 3 (0.54%)\n",
      "Queries with 155 original relevant docs: 1 (0.18%)\n",
      "Queries with 156 original relevant docs: 1 (0.18%)\n",
      "Queries with 157 original relevant docs: 3 (0.54%)\n",
      "Queries with 158 original relevant docs: 1 (0.18%)\n",
      "Queries with 159 original relevant docs: 1 (0.18%)\n",
      "Queries with 160 original relevant docs: 2 (0.36%)\n",
      "Queries with 161 original relevant docs: 4 (0.72%)\n",
      "Queries with 162 original relevant docs: 1 (0.18%)\n",
      "Queries with 163 original relevant docs: 1 (0.18%)\n",
      "Queries with 164 original relevant docs: 3 (0.54%)\n",
      "Queries with 165 original relevant docs: 1 (0.18%)\n",
      "Queries with 166 original relevant docs: 2 (0.36%)\n",
      "Queries with 167 original relevant docs: 2 (0.36%)\n",
      "Queries with 168 original relevant docs: 2 (0.36%)\n",
      "Queries with 169 original relevant docs: 4 (0.72%)\n",
      "Queries with 171 original relevant docs: 3 (0.54%)\n",
      "Queries with 173 original relevant docs: 2 (0.36%)\n",
      "Queries with 174 original relevant docs: 3 (0.54%)\n",
      "Queries with 176 original relevant docs: 3 (0.54%)\n",
      "Queries with 179 original relevant docs: 3 (0.54%)\n",
      "Queries with 182 original relevant docs: 1 (0.18%)\n",
      "Queries with 183 original relevant docs: 4 (0.72%)\n",
      "Queries with 184 original relevant docs: 2 (0.36%)\n",
      "Queries with 185 original relevant docs: 2 (0.36%)\n",
      "Queries with 186 original relevant docs: 2 (0.36%)\n",
      "Queries with 187 original relevant docs: 6 (1.08%)\n",
      "Queries with 188 original relevant docs: 2 (0.36%)\n",
      "Queries with 189 original relevant docs: 1 (0.18%)\n",
      "Queries with 190 original relevant docs: 4 (0.72%)\n",
      "Queries with 192 original relevant docs: 3 (0.54%)\n",
      "Queries with 193 original relevant docs: 4 (0.72%)\n",
      "Queries with 194 original relevant docs: 2 (0.36%)\n",
      "Queries with 195 original relevant docs: 3 (0.54%)\n",
      "Queries with 196 original relevant docs: 1 (0.18%)\n",
      "Queries with 197 original relevant docs: 2 (0.36%)\n",
      "Queries with 198 original relevant docs: 5 (0.90%)\n",
      "Queries with 199 original relevant docs: 4 (0.72%)\n",
      "Queries with 200 original relevant docs: 4 (0.72%)\n",
      "Queries with 201 original relevant docs: 2 (0.36%)\n",
      "Queries with 203 original relevant docs: 2 (0.36%)\n",
      "Queries with 204 original relevant docs: 1 (0.18%)\n",
      "Queries with 206 original relevant docs: 2 (0.36%)\n",
      "Queries with 209 original relevant docs: 2 (0.36%)\n",
      "Queries with 210 original relevant docs: 3 (0.54%)\n",
      "Queries with 212 original relevant docs: 1 (0.18%)\n",
      "Queries with 215 original relevant docs: 1 (0.18%)\n",
      "Queries with 217 original relevant docs: 2 (0.36%)\n",
      "Queries with 218 original relevant docs: 4 (0.72%)\n",
      "Queries with 219 original relevant docs: 4 (0.72%)\n",
      "Queries with 220 original relevant docs: 4 (0.72%)\n",
      "Queries with 223 original relevant docs: 2 (0.36%)\n",
      "Queries with 224 original relevant docs: 1 (0.18%)\n",
      "Queries with 225 original relevant docs: 2 (0.36%)\n",
      "Queries with 226 original relevant docs: 4 (0.72%)\n",
      "Queries with 227 original relevant docs: 1 (0.18%)\n",
      "Queries with 228 original relevant docs: 3 (0.54%)\n",
      "Queries with 229 original relevant docs: 1 (0.18%)\n",
      "Queries with 233 original relevant docs: 1 (0.18%)\n",
      "Queries with 234 original relevant docs: 6 (1.08%)\n",
      "Queries with 236 original relevant docs: 1 (0.18%)\n",
      "Queries with 238 original relevant docs: 3 (0.54%)\n",
      "Queries with 240 original relevant docs: 1 (0.18%)\n",
      "Queries with 242 original relevant docs: 1 (0.18%)\n",
      "Queries with 243 original relevant docs: 1 (0.18%)\n",
      "Queries with 244 original relevant docs: 3 (0.54%)\n",
      "Queries with 246 original relevant docs: 3 (0.54%)\n",
      "Queries with 247 original relevant docs: 1 (0.18%)\n",
      "Queries with 248 original relevant docs: 4 (0.72%)\n",
      "Queries with 249 original relevant docs: 1 (0.18%)\n",
      "Queries with 250 original relevant docs: 1 (0.18%)\n",
      "Queries with 251 original relevant docs: 3 (0.54%)\n",
      "Queries with 252 original relevant docs: 1 (0.18%)\n",
      "Queries with 253 original relevant docs: 1 (0.18%)\n",
      "Queries with 254 original relevant docs: 1 (0.18%)\n",
      "Queries with 255 original relevant docs: 2 (0.36%)\n",
      "Queries with 258 original relevant docs: 1 (0.18%)\n",
      "Queries with 259 original relevant docs: 3 (0.54%)\n",
      "Queries with 260 original relevant docs: 1 (0.18%)\n",
      "Queries with 261 original relevant docs: 1 (0.18%)\n",
      "Queries with 262 original relevant docs: 1 (0.18%)\n",
      "Queries with 263 original relevant docs: 2 (0.36%)\n",
      "Queries with 264 original relevant docs: 1 (0.18%)\n",
      "Queries with 265 original relevant docs: 1 (0.18%)\n",
      "Queries with 266 original relevant docs: 1 (0.18%)\n",
      "Queries with 268 original relevant docs: 1 (0.18%)\n",
      "Queries with 270 original relevant docs: 1 (0.18%)\n",
      "Queries with 271 original relevant docs: 1 (0.18%)\n",
      "Queries with 273 original relevant docs: 1 (0.18%)\n",
      "Queries with 275 original relevant docs: 1 (0.18%)\n",
      "Queries with 276 original relevant docs: 1 (0.18%)\n",
      "Queries with 278 original relevant docs: 4 (0.72%)\n",
      "Queries with 279 original relevant docs: 2 (0.36%)\n",
      "Queries with 280 original relevant docs: 2 (0.36%)\n",
      "Queries with 281 original relevant docs: 1 (0.18%)\n",
      "Queries with 283 original relevant docs: 2 (0.36%)\n",
      "Queries with 284 original relevant docs: 1 (0.18%)\n",
      "Queries with 285 original relevant docs: 1 (0.18%)\n",
      "Queries with 287 original relevant docs: 1 (0.18%)\n",
      "Queries with 289 original relevant docs: 2 (0.36%)\n",
      "Queries with 290 original relevant docs: 2 (0.36%)\n",
      "Queries with 291 original relevant docs: 2 (0.36%)\n",
      "Queries with 292 original relevant docs: 1 (0.18%)\n",
      "Queries with 293 original relevant docs: 1 (0.18%)\n",
      "Queries with 294 original relevant docs: 1 (0.18%)\n",
      "Queries with 296 original relevant docs: 2 (0.36%)\n",
      "Queries with 297 original relevant docs: 2 (0.36%)\n",
      "Queries with 298 original relevant docs: 1 (0.18%)\n",
      "Queries with 300 original relevant docs: 3 (0.54%)\n",
      "Queries with 301 original relevant docs: 4 (0.72%)\n",
      "Queries with 305 original relevant docs: 1 (0.18%)\n",
      "Queries with 307 original relevant docs: 2 (0.36%)\n",
      "Queries with 308 original relevant docs: 1 (0.18%)\n",
      "Queries with 309 original relevant docs: 1 (0.18%)\n",
      "Queries with 310 original relevant docs: 3 (0.54%)\n",
      "Queries with 311 original relevant docs: 2 (0.36%)\n",
      "Queries with 315 original relevant docs: 1 (0.18%)\n",
      "Queries with 320 original relevant docs: 2 (0.36%)\n",
      "Queries with 321 original relevant docs: 1 (0.18%)\n",
      "Queries with 322 original relevant docs: 1 (0.18%)\n",
      "Queries with 323 original relevant docs: 2 (0.36%)\n",
      "Queries with 325 original relevant docs: 2 (0.36%)\n",
      "Queries with 326 original relevant docs: 2 (0.36%)\n",
      "Queries with 328 original relevant docs: 2 (0.36%)\n",
      "Queries with 329 original relevant docs: 1 (0.18%)\n",
      "Queries with 330 original relevant docs: 2 (0.36%)\n",
      "Queries with 335 original relevant docs: 1 (0.18%)\n",
      "Queries with 340 original relevant docs: 1 (0.18%)\n",
      "Queries with 341 original relevant docs: 1 (0.18%)\n",
      "Queries with 342 original relevant docs: 1 (0.18%)\n",
      "Queries with 345 original relevant docs: 1 (0.18%)\n",
      "Queries with 348 original relevant docs: 3 (0.54%)\n",
      "Queries with 350 original relevant docs: 1 (0.18%)\n",
      "Queries with 353 original relevant docs: 1 (0.18%)\n",
      "Queries with 354 original relevant docs: 2 (0.36%)\n",
      "Queries with 355 original relevant docs: 1 (0.18%)\n",
      "Queries with 356 original relevant docs: 3 (0.54%)\n",
      "Queries with 359 original relevant docs: 2 (0.36%)\n",
      "Queries with 362 original relevant docs: 2 (0.36%)\n",
      "Queries with 363 original relevant docs: 2 (0.36%)\n",
      "Queries with 364 original relevant docs: 1 (0.18%)\n",
      "Queries with 365 original relevant docs: 2 (0.36%)\n",
      "Queries with 366 original relevant docs: 2 (0.36%)\n",
      "Queries with 368 original relevant docs: 1 (0.18%)\n",
      "Queries with 376 original relevant docs: 3 (0.54%)\n",
      "Queries with 377 original relevant docs: 1 (0.18%)\n",
      "Queries with 383 original relevant docs: 1 (0.18%)\n",
      "Queries with 384 original relevant docs: 2 (0.36%)\n",
      "Queries with 385 original relevant docs: 1 (0.18%)\n",
      "Queries with 386 original relevant docs: 1 (0.18%)\n",
      "Queries with 391 original relevant docs: 1 (0.18%)\n",
      "Queries with 395 original relevant docs: 1 (0.18%)\n",
      "Queries with 401 original relevant docs: 1 (0.18%)\n",
      "Queries with 402 original relevant docs: 1 (0.18%)\n",
      "Queries with 403 original relevant docs: 1 (0.18%)\n",
      "Queries with 407 original relevant docs: 1 (0.18%)\n",
      "Queries with 409 original relevant docs: 1 (0.18%)\n",
      "Queries with 410 original relevant docs: 1 (0.18%)\n",
      "Queries with 412 original relevant docs: 3 (0.54%)\n",
      "Queries with 423 original relevant docs: 1 (0.18%)\n",
      "Queries with 425 original relevant docs: 2 (0.36%)\n",
      "Queries with 428 original relevant docs: 1 (0.18%)\n",
      "Queries with 430 original relevant docs: 1 (0.18%)\n",
      "Queries with 433 original relevant docs: 1 (0.18%)\n",
      "Queries with 446 original relevant docs: 1 (0.18%)\n",
      "Queries with 448 original relevant docs: 1 (0.18%)\n",
      "Queries with 449 original relevant docs: 1 (0.18%)\n",
      "Queries with 451 original relevant docs: 1 (0.18%)\n",
      "Queries with 458 original relevant docs: 3 (0.54%)\n",
      "Queries with 460 original relevant docs: 1 (0.18%)\n",
      "Queries with 462 original relevant docs: 1 (0.18%)\n",
      "Queries with 470 original relevant docs: 2 (0.36%)\n",
      "Queries with 477 original relevant docs: 1 (0.18%)\n",
      "Queries with 480 original relevant docs: 3 (0.54%)\n",
      "Queries with 484 original relevant docs: 1 (0.18%)\n",
      "Queries with 485 original relevant docs: 1 (0.18%)\n",
      "Queries with 486 original relevant docs: 1 (0.18%)\n",
      "Queries with 487 original relevant docs: 1 (0.18%)\n",
      "Queries with 488 original relevant docs: 2 (0.36%)\n",
      "Queries with 490 original relevant docs: 1 (0.18%)\n",
      "Queries with 498 original relevant docs: 2 (0.36%)\n",
      "Queries with 504 original relevant docs: 1 (0.18%)\n",
      "Queries with 506 original relevant docs: 1 (0.18%)\n",
      "Queries with 509 original relevant docs: 1 (0.18%)\n",
      "Queries with 511 original relevant docs: 1 (0.18%)\n",
      "Queries with 514 original relevant docs: 1 (0.18%)\n",
      "Queries with 517 original relevant docs: 1 (0.18%)\n",
      "Queries with 520 original relevant docs: 1 (0.18%)\n",
      "Queries with 523 original relevant docs: 1 (0.18%)\n",
      "Queries with 530 original relevant docs: 1 (0.18%)\n",
      "Queries with 537 original relevant docs: 1 (0.18%)\n",
      "Queries with 549 original relevant docs: 1 (0.18%)\n",
      "Queries with 550 original relevant docs: 1 (0.18%)\n",
      "Queries with 560 original relevant docs: 1 (0.18%)\n",
      "Queries with 576 original relevant docs: 1 (0.18%)\n",
      "Queries with 579 original relevant docs: 2 (0.36%)\n",
      "Queries with 587 original relevant docs: 3 (0.54%)\n",
      "Queries with 590 original relevant docs: 1 (0.18%)\n",
      "Queries with 593 original relevant docs: 1 (0.18%)\n",
      "Queries with 597 original relevant docs: 1 (0.18%)\n",
      "Queries with 616 original relevant docs: 1 (0.18%)\n",
      "Queries with 618 original relevant docs: 1 (0.18%)\n",
      "Queries with 620 original relevant docs: 1 (0.18%)\n",
      "Queries with 623 original relevant docs: 1 (0.18%)\n",
      "Queries with 628 original relevant docs: 1 (0.18%)\n",
      "Queries with 635 original relevant docs: 1 (0.18%)\n",
      "Queries with 644 original relevant docs: 1 (0.18%)\n",
      "Queries with 649 original relevant docs: 1 (0.18%)\n",
      "Queries with 653 original relevant docs: 2 (0.36%)\n",
      "Queries with 656 original relevant docs: 1 (0.18%)\n",
      "Queries with 667 original relevant docs: 1 (0.18%)\n",
      "Queries with 676 original relevant docs: 1 (0.18%)\n",
      "Queries with 685 original relevant docs: 1 (0.18%)\n",
      "Queries with 695 original relevant docs: 1 (0.18%)\n",
      "Queries with 706 original relevant docs: 1 (0.18%)\n",
      "Queries with 707 original relevant docs: 1 (0.18%)\n",
      "Queries with 741 original relevant docs: 1 (0.18%)\n",
      "Queries with 763 original relevant docs: 1 (0.18%)\n",
      "Queries with 772 original relevant docs: 1 (0.18%)\n",
      "Queries with 773 original relevant docs: 2 (0.36%)\n",
      "Queries with 792 original relevant docs: 1 (0.18%)\n",
      "Queries with 801 original relevant docs: 2 (0.36%)\n",
      "Queries with 803 original relevant docs: 1 (0.18%)\n",
      "Queries with 808 original relevant docs: 1 (0.18%)\n",
      "Queries with 811 original relevant docs: 1 (0.18%)\n",
      "Queries with 821 original relevant docs: 2 (0.36%)\n",
      "Queries with 838 original relevant docs: 1 (0.18%)\n",
      "Queries with 850 original relevant docs: 1 (0.18%)\n",
      "Queries with 857 original relevant docs: 1 (0.18%)\n",
      "Queries with 912 original relevant docs: 1 (0.18%)\n",
      "Queries with 948 original relevant docs: 1 (0.18%)\n",
      "Queries with 950 original relevant docs: 1 (0.18%)\n",
      "Queries with 966 original relevant docs: 1 (0.18%)\n",
      "Queries with 970 original relevant docs: 1 (0.18%)\n",
      "Queries with 973 original relevant docs: 1 (0.18%)\n",
      "Queries with 1009 original relevant docs: 1 (0.18%)\n",
      "Queries with 1010 original relevant docs: 1 (0.18%)\n",
      "Queries with 1011 original relevant docs: 1 (0.18%)\n",
      "Queries with 1014 original relevant docs: 1 (0.18%)\n",
      "Queries with 1017 original relevant docs: 1 (0.18%)\n",
      "Queries with 1019 original relevant docs: 1 (0.18%)\n",
      "Queries with 1024 original relevant docs: 1 (0.18%)\n",
      "Queries with 1096 original relevant docs: 2 (0.36%)\n",
      "Queries with 1156 original relevant docs: 1 (0.18%)\n",
      "Queries with 1159 original relevant docs: 1 (0.18%)\n",
      "Queries with 1170 original relevant docs: 1 (0.18%)\n",
      "Queries with 1195 original relevant docs: 1 (0.18%)\n",
      "Queries with 1198 original relevant docs: 1 (0.18%)\n",
      "Queries with 1205 original relevant docs: 2 (0.36%)\n",
      "Queries with 1220 original relevant docs: 2 (0.36%)\n",
      "Queries with 1231 original relevant docs: 1 (0.18%)\n",
      "Queries with 1240 original relevant docs: 1 (0.18%)\n",
      "Queries with 1244 original relevant docs: 1 (0.18%)\n",
      "Queries with 1249 original relevant docs: 1 (0.18%)\n",
      "Queries with 1251 original relevant docs: 1 (0.18%)\n",
      "Queries with 1263 original relevant docs: 1 (0.18%)\n",
      "Queries with 1269 original relevant docs: 1 (0.18%)\n",
      "\n",
      "Distribution of additional documents needed per query:\n",
      "Queries needing -1259 additional docs: 1 (0.18%)\n",
      "Queries needing -1253 additional docs: 1 (0.18%)\n",
      "Queries needing -1241 additional docs: 1 (0.18%)\n",
      "Queries needing -1239 additional docs: 1 (0.18%)\n",
      "Queries needing -1234 additional docs: 1 (0.18%)\n",
      "Queries needing -1230 additional docs: 1 (0.18%)\n",
      "Queries needing -1221 additional docs: 1 (0.18%)\n",
      "Queries needing -1210 additional docs: 2 (0.36%)\n",
      "Queries needing -1195 additional docs: 2 (0.36%)\n",
      "Queries needing -1188 additional docs: 1 (0.18%)\n",
      "Queries needing -1185 additional docs: 1 (0.18%)\n",
      "Queries needing -1160 additional docs: 1 (0.18%)\n",
      "Queries needing -1149 additional docs: 1 (0.18%)\n",
      "Queries needing -1146 additional docs: 1 (0.18%)\n",
      "Queries needing -1086 additional docs: 2 (0.36%)\n",
      "Queries needing -1014 additional docs: 1 (0.18%)\n",
      "Queries needing -1009 additional docs: 1 (0.18%)\n",
      "Queries needing -1007 additional docs: 1 (0.18%)\n",
      "Queries needing -1004 additional docs: 1 (0.18%)\n",
      "Queries needing -1001 additional docs: 1 (0.18%)\n",
      "Queries needing -1000 additional docs: 1 (0.18%)\n",
      "Queries needing -999 additional docs: 1 (0.18%)\n",
      "Queries needing -963 additional docs: 1 (0.18%)\n",
      "Queries needing -960 additional docs: 1 (0.18%)\n",
      "Queries needing -956 additional docs: 1 (0.18%)\n",
      "Queries needing -940 additional docs: 1 (0.18%)\n",
      "Queries needing -938 additional docs: 1 (0.18%)\n",
      "Queries needing -902 additional docs: 1 (0.18%)\n",
      "Queries needing -847 additional docs: 1 (0.18%)\n",
      "Queries needing -840 additional docs: 1 (0.18%)\n",
      "Queries needing -828 additional docs: 1 (0.18%)\n",
      "Queries needing -811 additional docs: 2 (0.36%)\n",
      "Queries needing -801 additional docs: 1 (0.18%)\n",
      "Queries needing -798 additional docs: 1 (0.18%)\n",
      "Queries needing -793 additional docs: 1 (0.18%)\n",
      "Queries needing -791 additional docs: 2 (0.36%)\n",
      "Queries needing -782 additional docs: 1 (0.18%)\n",
      "Queries needing -763 additional docs: 2 (0.36%)\n",
      "Queries needing -762 additional docs: 1 (0.18%)\n",
      "Queries needing -753 additional docs: 1 (0.18%)\n",
      "Queries needing -731 additional docs: 1 (0.18%)\n",
      "Queries needing -697 additional docs: 1 (0.18%)\n",
      "Queries needing -696 additional docs: 1 (0.18%)\n",
      "Queries needing -685 additional docs: 1 (0.18%)\n",
      "Queries needing -675 additional docs: 1 (0.18%)\n",
      "Queries needing -666 additional docs: 1 (0.18%)\n",
      "Queries needing -657 additional docs: 1 (0.18%)\n",
      "Queries needing -646 additional docs: 1 (0.18%)\n",
      "Queries needing -643 additional docs: 2 (0.36%)\n",
      "Queries needing -639 additional docs: 1 (0.18%)\n",
      "Queries needing -634 additional docs: 1 (0.18%)\n",
      "Queries needing -625 additional docs: 1 (0.18%)\n",
      "Queries needing -618 additional docs: 1 (0.18%)\n",
      "Queries needing -613 additional docs: 1 (0.18%)\n",
      "Queries needing -610 additional docs: 1 (0.18%)\n",
      "Queries needing -608 additional docs: 1 (0.18%)\n",
      "Queries needing -606 additional docs: 1 (0.18%)\n",
      "Queries needing -587 additional docs: 1 (0.18%)\n",
      "Queries needing -583 additional docs: 1 (0.18%)\n",
      "Queries needing -580 additional docs: 1 (0.18%)\n",
      "Queries needing -577 additional docs: 3 (0.54%)\n",
      "Queries needing -569 additional docs: 2 (0.36%)\n",
      "Queries needing -566 additional docs: 1 (0.18%)\n",
      "Queries needing -550 additional docs: 1 (0.18%)\n",
      "Queries needing -540 additional docs: 1 (0.18%)\n",
      "Queries needing -539 additional docs: 1 (0.18%)\n",
      "Queries needing -527 additional docs: 1 (0.18%)\n",
      "Queries needing -520 additional docs: 1 (0.18%)\n",
      "Queries needing -513 additional docs: 1 (0.18%)\n",
      "Queries needing -510 additional docs: 1 (0.18%)\n",
      "Queries needing -507 additional docs: 1 (0.18%)\n",
      "Queries needing -504 additional docs: 1 (0.18%)\n",
      "Queries needing -501 additional docs: 1 (0.18%)\n",
      "Queries needing -499 additional docs: 1 (0.18%)\n",
      "Queries needing -496 additional docs: 1 (0.18%)\n",
      "Queries needing -494 additional docs: 1 (0.18%)\n",
      "Queries needing -488 additional docs: 2 (0.36%)\n",
      "Queries needing -480 additional docs: 1 (0.18%)\n",
      "Queries needing -478 additional docs: 2 (0.36%)\n",
      "Queries needing -477 additional docs: 1 (0.18%)\n",
      "Queries needing -476 additional docs: 1 (0.18%)\n",
      "Queries needing -475 additional docs: 1 (0.18%)\n",
      "Queries needing -474 additional docs: 1 (0.18%)\n",
      "Queries needing -470 additional docs: 3 (0.54%)\n",
      "Queries needing -467 additional docs: 1 (0.18%)\n",
      "Queries needing -460 additional docs: 2 (0.36%)\n",
      "Queries needing -452 additional docs: 1 (0.18%)\n",
      "Queries needing -450 additional docs: 1 (0.18%)\n",
      "Queries needing -448 additional docs: 3 (0.54%)\n",
      "Queries needing -441 additional docs: 1 (0.18%)\n",
      "Queries needing -439 additional docs: 1 (0.18%)\n",
      "Queries needing -438 additional docs: 1 (0.18%)\n",
      "Queries needing -436 additional docs: 1 (0.18%)\n",
      "Queries needing -423 additional docs: 1 (0.18%)\n",
      "Queries needing -420 additional docs: 1 (0.18%)\n",
      "Queries needing -418 additional docs: 1 (0.18%)\n",
      "Queries needing -415 additional docs: 2 (0.36%)\n",
      "Queries needing -413 additional docs: 1 (0.18%)\n",
      "Queries needing -402 additional docs: 3 (0.54%)\n",
      "Queries needing -400 additional docs: 1 (0.18%)\n",
      "Queries needing -399 additional docs: 1 (0.18%)\n",
      "Queries needing -397 additional docs: 1 (0.18%)\n",
      "Queries needing -393 additional docs: 1 (0.18%)\n",
      "Queries needing -392 additional docs: 1 (0.18%)\n",
      "Queries needing -391 additional docs: 1 (0.18%)\n",
      "Queries needing -385 additional docs: 1 (0.18%)\n",
      "Queries needing -381 additional docs: 1 (0.18%)\n",
      "Queries needing -376 additional docs: 1 (0.18%)\n",
      "Queries needing -375 additional docs: 1 (0.18%)\n",
      "Queries needing -374 additional docs: 2 (0.36%)\n",
      "Queries needing -373 additional docs: 1 (0.18%)\n",
      "Queries needing -367 additional docs: 1 (0.18%)\n",
      "Queries needing -366 additional docs: 3 (0.54%)\n",
      "Queries needing -358 additional docs: 1 (0.18%)\n",
      "Queries needing -356 additional docs: 2 (0.36%)\n",
      "Queries needing -355 additional docs: 2 (0.36%)\n",
      "Queries needing -354 additional docs: 1 (0.18%)\n",
      "Queries needing -353 additional docs: 2 (0.36%)\n",
      "Queries needing -352 additional docs: 2 (0.36%)\n",
      "Queries needing -349 additional docs: 2 (0.36%)\n",
      "Queries needing -346 additional docs: 3 (0.54%)\n",
      "Queries needing -345 additional docs: 1 (0.18%)\n",
      "Queries needing -344 additional docs: 2 (0.36%)\n",
      "Queries needing -343 additional docs: 1 (0.18%)\n",
      "Queries needing -340 additional docs: 1 (0.18%)\n",
      "Queries needing -338 additional docs: 3 (0.54%)\n",
      "Queries needing -335 additional docs: 1 (0.18%)\n",
      "Queries needing -332 additional docs: 1 (0.18%)\n",
      "Queries needing -331 additional docs: 1 (0.18%)\n",
      "Queries needing -330 additional docs: 1 (0.18%)\n",
      "Queries needing -325 additional docs: 1 (0.18%)\n",
      "Queries needing -320 additional docs: 2 (0.36%)\n",
      "Queries needing -319 additional docs: 1 (0.18%)\n",
      "Queries needing -318 additional docs: 2 (0.36%)\n",
      "Queries needing -316 additional docs: 2 (0.36%)\n",
      "Queries needing -315 additional docs: 2 (0.36%)\n",
      "Queries needing -313 additional docs: 2 (0.36%)\n",
      "Queries needing -312 additional docs: 1 (0.18%)\n",
      "Queries needing -311 additional docs: 1 (0.18%)\n",
      "Queries needing -310 additional docs: 2 (0.36%)\n",
      "Queries needing -305 additional docs: 1 (0.18%)\n",
      "Queries needing -301 additional docs: 2 (0.36%)\n",
      "Queries needing -300 additional docs: 3 (0.54%)\n",
      "Queries needing -299 additional docs: 1 (0.18%)\n",
      "Queries needing -298 additional docs: 1 (0.18%)\n",
      "Queries needing -297 additional docs: 2 (0.36%)\n",
      "Queries needing -295 additional docs: 1 (0.18%)\n",
      "Queries needing -291 additional docs: 4 (0.72%)\n",
      "Queries needing -290 additional docs: 3 (0.54%)\n",
      "Queries needing -288 additional docs: 1 (0.18%)\n",
      "Queries needing -287 additional docs: 2 (0.36%)\n",
      "Queries needing -286 additional docs: 2 (0.36%)\n",
      "Queries needing -284 additional docs: 1 (0.18%)\n",
      "Queries needing -283 additional docs: 1 (0.18%)\n",
      "Queries needing -282 additional docs: 1 (0.18%)\n",
      "Queries needing -281 additional docs: 2 (0.36%)\n",
      "Queries needing -280 additional docs: 2 (0.36%)\n",
      "Queries needing -279 additional docs: 2 (0.36%)\n",
      "Queries needing -277 additional docs: 1 (0.18%)\n",
      "Queries needing -275 additional docs: 1 (0.18%)\n",
      "Queries needing -274 additional docs: 1 (0.18%)\n",
      "Queries needing -273 additional docs: 2 (0.36%)\n",
      "Queries needing -271 additional docs: 1 (0.18%)\n",
      "Queries needing -270 additional docs: 2 (0.36%)\n",
      "Queries needing -269 additional docs: 2 (0.36%)\n",
      "Queries needing -268 additional docs: 4 (0.72%)\n",
      "Queries needing -266 additional docs: 1 (0.18%)\n",
      "Queries needing -265 additional docs: 1 (0.18%)\n",
      "Queries needing -263 additional docs: 1 (0.18%)\n",
      "Queries needing -261 additional docs: 1 (0.18%)\n",
      "Queries needing -260 additional docs: 1 (0.18%)\n",
      "Queries needing -258 additional docs: 1 (0.18%)\n",
      "Queries needing -256 additional docs: 1 (0.18%)\n",
      "Queries needing -255 additional docs: 1 (0.18%)\n",
      "Queries needing -254 additional docs: 1 (0.18%)\n",
      "Queries needing -253 additional docs: 2 (0.36%)\n",
      "Queries needing -252 additional docs: 1 (0.18%)\n",
      "Queries needing -251 additional docs: 1 (0.18%)\n",
      "Queries needing -250 additional docs: 1 (0.18%)\n",
      "Queries needing -249 additional docs: 3 (0.54%)\n",
      "Queries needing -248 additional docs: 1 (0.18%)\n",
      "Queries needing -245 additional docs: 2 (0.36%)\n",
      "Queries needing -244 additional docs: 1 (0.18%)\n",
      "Queries needing -243 additional docs: 1 (0.18%)\n",
      "Queries needing -242 additional docs: 1 (0.18%)\n",
      "Queries needing -241 additional docs: 3 (0.54%)\n",
      "Queries needing -240 additional docs: 1 (0.18%)\n",
      "Queries needing -239 additional docs: 1 (0.18%)\n",
      "Queries needing -238 additional docs: 4 (0.72%)\n",
      "Queries needing -237 additional docs: 1 (0.18%)\n",
      "Queries needing -236 additional docs: 3 (0.54%)\n",
      "Queries needing -234 additional docs: 3 (0.54%)\n",
      "Queries needing -233 additional docs: 1 (0.18%)\n",
      "Queries needing -232 additional docs: 1 (0.18%)\n",
      "Queries needing -230 additional docs: 1 (0.18%)\n",
      "Queries needing -228 additional docs: 3 (0.54%)\n",
      "Queries needing -226 additional docs: 1 (0.18%)\n",
      "Queries needing -224 additional docs: 6 (1.08%)\n",
      "Queries needing -223 additional docs: 1 (0.18%)\n",
      "Queries needing -219 additional docs: 1 (0.18%)\n",
      "Queries needing -218 additional docs: 3 (0.54%)\n",
      "Queries needing -217 additional docs: 1 (0.18%)\n",
      "Queries needing -216 additional docs: 4 (0.72%)\n",
      "Queries needing -215 additional docs: 2 (0.36%)\n",
      "Queries needing -214 additional docs: 1 (0.18%)\n",
      "Queries needing -213 additional docs: 2 (0.36%)\n",
      "Queries needing -210 additional docs: 4 (0.72%)\n",
      "Queries needing -209 additional docs: 4 (0.72%)\n",
      "Queries needing -208 additional docs: 4 (0.72%)\n",
      "Queries needing -207 additional docs: 2 (0.36%)\n",
      "Queries needing -205 additional docs: 1 (0.18%)\n",
      "Queries needing -202 additional docs: 1 (0.18%)\n",
      "Queries needing -200 additional docs: 3 (0.54%)\n",
      "Queries needing -199 additional docs: 2 (0.36%)\n",
      "Queries needing -196 additional docs: 2 (0.36%)\n",
      "Queries needing -194 additional docs: 1 (0.18%)\n",
      "Queries needing -193 additional docs: 2 (0.36%)\n",
      "Queries needing -191 additional docs: 2 (0.36%)\n",
      "Queries needing -190 additional docs: 4 (0.72%)\n",
      "Queries needing -189 additional docs: 4 (0.72%)\n",
      "Queries needing -188 additional docs: 5 (0.90%)\n",
      "Queries needing -187 additional docs: 2 (0.36%)\n",
      "Queries needing -186 additional docs: 1 (0.18%)\n",
      "Queries needing -185 additional docs: 3 (0.54%)\n",
      "Queries needing -184 additional docs: 2 (0.36%)\n",
      "Queries needing -183 additional docs: 4 (0.72%)\n",
      "Queries needing -182 additional docs: 3 (0.54%)\n",
      "Queries needing -180 additional docs: 4 (0.72%)\n",
      "Queries needing -179 additional docs: 1 (0.18%)\n",
      "Queries needing -178 additional docs: 2 (0.36%)\n",
      "Queries needing -177 additional docs: 6 (1.08%)\n",
      "Queries needing -176 additional docs: 2 (0.36%)\n",
      "Queries needing -175 additional docs: 2 (0.36%)\n",
      "Queries needing -174 additional docs: 2 (0.36%)\n",
      "Queries needing -173 additional docs: 4 (0.72%)\n",
      "Queries needing -172 additional docs: 1 (0.18%)\n",
      "Queries needing -169 additional docs: 3 (0.54%)\n",
      "Queries needing -166 additional docs: 3 (0.54%)\n",
      "Queries needing -164 additional docs: 3 (0.54%)\n",
      "Queries needing -163 additional docs: 2 (0.36%)\n",
      "Queries needing -161 additional docs: 3 (0.54%)\n",
      "Queries needing -159 additional docs: 4 (0.72%)\n",
      "Queries needing -158 additional docs: 2 (0.36%)\n",
      "Queries needing -157 additional docs: 2 (0.36%)\n",
      "Queries needing -156 additional docs: 2 (0.36%)\n",
      "Queries needing -155 additional docs: 1 (0.18%)\n",
      "Queries needing -154 additional docs: 3 (0.54%)\n",
      "Queries needing -153 additional docs: 1 (0.18%)\n",
      "Queries needing -152 additional docs: 1 (0.18%)\n",
      "Queries needing -151 additional docs: 4 (0.72%)\n",
      "Queries needing -150 additional docs: 2 (0.36%)\n",
      "Queries needing -149 additional docs: 1 (0.18%)\n",
      "Queries needing -148 additional docs: 1 (0.18%)\n",
      "Queries needing -147 additional docs: 3 (0.54%)\n",
      "Queries needing -146 additional docs: 1 (0.18%)\n",
      "Queries needing -145 additional docs: 1 (0.18%)\n",
      "Queries needing -143 additional docs: 3 (0.54%)\n",
      "Queries needing -142 additional docs: 2 (0.36%)\n",
      "Queries needing -141 additional docs: 1 (0.18%)\n",
      "Queries needing -140 additional docs: 3 (0.54%)\n",
      "Queries needing -139 additional docs: 3 (0.54%)\n",
      "Queries needing -138 additional docs: 2 (0.36%)\n",
      "Queries needing -137 additional docs: 1 (0.18%)\n",
      "Queries needing -136 additional docs: 1 (0.18%)\n",
      "Queries needing -135 additional docs: 4 (0.72%)\n",
      "Queries needing -134 additional docs: 2 (0.36%)\n",
      "Queries needing -133 additional docs: 1 (0.18%)\n",
      "Queries needing -132 additional docs: 3 (0.54%)\n",
      "Queries needing -131 additional docs: 4 (0.72%)\n",
      "Queries needing -130 additional docs: 2 (0.36%)\n",
      "Queries needing -129 additional docs: 1 (0.18%)\n",
      "Queries needing -128 additional docs: 3 (0.54%)\n",
      "Queries needing -127 additional docs: 1 (0.18%)\n",
      "Queries needing -126 additional docs: 1 (0.18%)\n",
      "Queries needing -124 additional docs: 1 (0.18%)\n",
      "Queries needing -123 additional docs: 3 (0.54%)\n",
      "Queries needing -122 additional docs: 3 (0.54%)\n",
      "Queries needing -121 additional docs: 1 (0.18%)\n",
      "Queries needing -119 additional docs: 1 (0.18%)\n",
      "Queries needing -118 additional docs: 1 (0.18%)\n",
      "Queries needing -117 additional docs: 1 (0.18%)\n",
      "Queries needing -116 additional docs: 1 (0.18%)\n",
      "Queries needing -114 additional docs: 4 (0.72%)\n",
      "Queries needing -113 additional docs: 3 (0.54%)\n",
      "Queries needing -111 additional docs: 2 (0.36%)\n",
      "Queries needing -110 additional docs: 4 (0.72%)\n",
      "Queries needing -109 additional docs: 2 (0.36%)\n",
      "Queries needing -108 additional docs: 3 (0.54%)\n",
      "Queries needing -107 additional docs: 1 (0.18%)\n",
      "Queries needing -106 additional docs: 1 (0.18%)\n",
      "Queries needing -105 additional docs: 2 (0.36%)\n",
      "Queries needing -104 additional docs: 4 (0.72%)\n",
      "Queries needing -101 additional docs: 1 (0.18%)\n",
      "Queries needing -100 additional docs: 1 (0.18%)\n",
      "Queries needing -99 additional docs: 2 (0.36%)\n",
      "Queries needing -97 additional docs: 2 (0.36%)\n",
      "Queries needing -96 additional docs: 2 (0.36%)\n",
      "Queries needing -95 additional docs: 4 (0.72%)\n",
      "Queries needing -94 additional docs: 2 (0.36%)\n",
      "Queries needing -93 additional docs: 2 (0.36%)\n",
      "Queries needing -92 additional docs: 5 (0.90%)\n",
      "Queries needing -90 additional docs: 2 (0.36%)\n",
      "Queries needing -89 additional docs: 1 (0.18%)\n",
      "Queries needing -86 additional docs: 1 (0.18%)\n",
      "Queries needing -84 additional docs: 1 (0.18%)\n",
      "Queries needing -82 additional docs: 1 (0.18%)\n",
      "Queries needing -81 additional docs: 4 (0.72%)\n",
      "Queries needing -80 additional docs: 1 (0.18%)\n",
      "Queries needing -78 additional docs: 1 (0.18%)\n",
      "Queries needing -77 additional docs: 1 (0.18%)\n",
      "Queries needing -76 additional docs: 2 (0.36%)\n",
      "Queries needing -75 additional docs: 1 (0.18%)\n",
      "Queries needing -73 additional docs: 1 (0.18%)\n",
      "Queries needing -72 additional docs: 2 (0.36%)\n",
      "Queries needing -71 additional docs: 2 (0.36%)\n",
      "Queries needing -69 additional docs: 3 (0.54%)\n",
      "Queries needing -67 additional docs: 2 (0.36%)\n",
      "Queries needing -64 additional docs: 2 (0.36%)\n",
      "Queries needing -63 additional docs: 1 (0.18%)\n",
      "Queries needing -59 additional docs: 2 (0.36%)\n",
      "Queries needing -58 additional docs: 1 (0.18%)\n",
      "Queries needing -51 additional docs: 1 (0.18%)\n",
      "Queries needing -48 additional docs: 2 (0.36%)\n",
      "Queries needing -45 additional docs: 1 (0.18%)\n",
      "Queries needing -43 additional docs: 2 (0.36%)\n",
      "Queries needing -42 additional docs: 2 (0.36%)\n",
      "Queries needing -41 additional docs: 3 (0.54%)\n",
      "Queries needing -33 additional docs: 2 (0.36%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def init_cache_dirs():\n",
    "    \"\"\"Initialize cache directories and paths\"\"\"\n",
    "    cache_dir = Path(\"embeddings_cache\")\n",
    "    cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    return {\n",
    "        'cache_dir': cache_dir,\n",
    "        'query_cache': cache_dir / \"query_embeddings_text-embedding-3-large.pkl\",\n",
    "        'doc_cache': cache_dir / \"doc_embeddings_text-embedding-3-large.pkl\",\n",
    "        'beir_query_cache': cache_dir / \"beir_query_embeddings_text-embedding-3-large.pkl\",\n",
    "        'beir_doc_cache': cache_dir / \"beir_doc_embeddings_text-embedding-3-large.pkl\"\n",
    "    }\n",
    "\n",
    "def load_cached_embeddings(cache_file):\n",
    "    \"\"\"Load embeddings from pickle cache file\"\"\"\n",
    "    if cache_file.exists():\n",
    "        print(f\"Loading cached embeddings from {cache_file}\")\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                embeddings = pickle.load(f)\n",
    "                print(f\"Successfully loaded embeddings with shape: {embeddings.shape}\")\n",
    "                return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading cache file: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_cached_embeddings(embeddings, cache_file):\n",
    "    \"\"\"Save embeddings to pickle cache file\"\"\"\n",
    "    print(f\"Saving embeddings to {cache_file}\")\n",
    "    try:\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "        print(f\"Successfully saved embeddings with shape: {embeddings.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cache file: {e}\")\n",
    "\n",
    "def get_embeddings(texts, client, cache_file=None, batch_size=100):\n",
    "    \"\"\"Get embeddings using OpenAI API with caching\"\"\"\n",
    "    if cache_file is not None:\n",
    "        cached_embeddings = load_cached_embeddings(cache_file)\n",
    "        if cached_embeddings is not None:\n",
    "            return cached_embeddings\n",
    "        print(f\"Cache miss for {cache_file}, generating new embeddings...\")\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Getting embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=\"text-embedding-3-large\",\n",
    "                input=batch\n",
    "            )\n",
    "            batch_embeddings = [np.array(item.embedding) for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}: {e}\")\n",
    "            fallback = np.zeros(3072)\n",
    "            all_embeddings.extend([fallback] * len(batch))\n",
    "    \n",
    "    embeddings_array = np.array(all_embeddings)\n",
    "    \n",
    "    if cache_file is not None:\n",
    "        save_cached_embeddings(embeddings_array, cache_file)\n",
    "    \n",
    "    return embeddings_array\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"Load test documents and queries\"\"\"\n",
    "    print(\"Loading test data...\")\n",
    "    test_documents_df = pd.read_csv(\"test_documents.csv\")\n",
    "    test_queries_df = pd.read_csv(\"test_query.csv\")\n",
    "    \n",
    "    valid_doc_ids = set(test_documents_df['Doc'].tolist())\n",
    "    test_queries = test_queries_df['Query'].tolist()\n",
    "    \n",
    "    print(f\"Loaded {len(valid_doc_ids)} valid document IDs and {len(test_queries)} test queries\")\n",
    "    return valid_doc_ids, test_queries\n",
    "\n",
    "def load_beir_data():\n",
    "    \"\"\"Load and process BeIR dataset\"\"\"\n",
    "    print(\"Loading BeIR dataset...\")\n",
    "    dataset_queries = load_dataset(\"BeIR/nfcorpus\", \"queries\")\n",
    "    dataset_qrels = load_dataset(\"BeIR/nfcorpus-qrels\")\n",
    "    dataset_docs = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
    "    \n",
    "    beir_queries = dataset_queries['queries']['text']\n",
    "    beir_query_ids = dataset_queries['queries']['_id']\n",
    "    beir_docs = dataset_docs['corpus']['text']\n",
    "    beir_doc_ids = dataset_docs['corpus']['_id']\n",
    "    \n",
    "    # Create mappings\n",
    "    beir_doc_id_to_text = dict(zip(beir_doc_ids, beir_docs))\n",
    "    beir_query_text_to_id = dict(zip(beir_queries, beir_query_ids))\n",
    "    \n",
    "    # Create query to docs mapping\n",
    "    beir_query_to_docs = {}\n",
    "    for split in ['train', 'test', 'validation']:\n",
    "        for item in dataset_qrels[split]:\n",
    "            query_id = item['query-id']\n",
    "            doc_id = item['corpus-id']\n",
    "            if query_id not in beir_query_to_docs:\n",
    "                beir_query_to_docs[query_id] = []\n",
    "            beir_query_to_docs[query_id].append(doc_id)\n",
    "    \n",
    "    return {\n",
    "        'queries': beir_queries,\n",
    "        'query_ids': beir_query_ids,\n",
    "        'docs': beir_docs,\n",
    "        'doc_ids': beir_doc_ids,\n",
    "        'doc_id_to_text': beir_doc_id_to_text,\n",
    "        'query_text_to_id': beir_query_text_to_id,\n",
    "        'query_to_docs': beir_query_to_docs\n",
    "    }\n",
    "\n",
    "def get_valid_docs(relevant_docs, valid_doc_ids, query_embedding, doc_embeddings, doc_ids, needed_count=10):\n",
    "    \"\"\"Filter and ensure we have valid document IDs\"\"\"\n",
    "    valid_relevant_docs = [doc_id for doc_id in relevant_docs if doc_id in valid_doc_ids]\n",
    "    print(f\"Found {len(valid_relevant_docs)} valid docs from {len(relevant_docs)} relevant docs, need {needed_count}\")\n",
    "\n",
    "    \n",
    "    if len(valid_relevant_docs) < needed_count:\n",
    "        remaining_docs = list(valid_doc_ids - set(valid_relevant_docs))\n",
    "        remaining_indices = [doc_ids.index(doc_id) for doc_id in remaining_docs]\n",
    "        \n",
    "        similarities = cosine_similarity([query_embedding], doc_embeddings[remaining_indices])[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:needed_count - len(valid_relevant_docs)]\n",
    "        \n",
    "        additional_docs = [remaining_docs[idx] for idx in top_indices]\n",
    "        valid_relevant_docs.extend(additional_docs)\n",
    "    \n",
    "    return valid_relevant_docs[:needed_count]\n",
    "\n",
    "def process_queries(test_queries, test_embeddings, beir_data, beir_embeddings, beir_doc_embeddings, valid_doc_ids):\n",
    "    \"\"\"Process queries and find relevant documents\"\"\"\n",
    "    print(\"\\nProcessing queries and finding relevant documents...\")\n",
    "    results = []\n",
    "    \n",
    "    for i, test_query in enumerate(tqdm(test_queries)):\n",
    "        # Find similar BeIR queries\n",
    "        similarities = cosine_similarity([test_embeddings[i]], beir_embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:10]\n",
    "        \n",
    "        # Collect relevant documents\n",
    "        relevant_docs = []\n",
    "        for idx in top_k_indices:\n",
    "            beir_query = beir_data['queries'][idx]\n",
    "            beir_query_id = beir_data['query_text_to_id'][beir_query]\n",
    "            relevant_docs.extend(beir_data['query_to_docs'].get(beir_query_id, []))\n",
    "        \n",
    "        relevant_docs = list(dict.fromkeys(relevant_docs))\n",
    "        \n",
    "        # Get valid documents\n",
    "        valid_relevant_docs = get_valid_docs(\n",
    "            relevant_docs,\n",
    "            valid_doc_ids,\n",
    "            test_embeddings[i],\n",
    "            beir_doc_embeddings,\n",
    "            beir_data['doc_ids']\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'Query': test_query,\n",
    "            'Doc_ID': ' '.join(valid_relevant_docs),\n",
    "            'similar_beir_query': beir_data['queries'][top_k_indices[0]],\n",
    "            'similarity_score': similarities[top_k_indices[0]],\n",
    "            'num_original_relevant': len(set(relevant_docs) & valid_doc_ids),\n",
    "            'num_additional_docs': 10 - len(set(relevant_docs) & valid_doc_ids)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results):\n",
    "    \"\"\"Save results to CSV files and print statistics\"\"\"\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save detailed results\n",
    "    submission_df.to_csv('submission_from_beir_ground_truth_openai_large_detailed_top10_similarity_query.csv', index=False)\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_df[['Query', 'Doc_ID']].to_csv('submission_from_beir_ground_truth_openai_large_top10_similarity_query.csv', index=False)\n",
    "    \n",
    "    print(\"\\nSubmission files created successfully!\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nStatistics:\")\n",
    "    print(f\"Total queries processed: {len(results)}\")\n",
    "    \n",
    "    print(\"\\nDistribution of original relevant documents per query:\")\n",
    "    original_docs_dist = submission_df['num_original_relevant'].value_counts().sort_index()\n",
    "    for num_docs, count in original_docs_dist.items():\n",
    "        print(f\"Queries with {num_docs} original relevant docs: {count} ({count/len(results)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nDistribution of additional documents needed per query:\")\n",
    "    additional_docs_dist = submission_df['num_additional_docs'].value_counts().sort_index()\n",
    "    for num_docs, count in additional_docs_dist.items():\n",
    "        print(f\"Queries needing {num_docs} additional docs: {count} ({count/len(results)*100:.2f}%)\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Initialize OpenAI client and cache\n",
    "    client = OpenAI()\n",
    "    cache_paths = init_cache_dirs()\n",
    "    \n",
    "    # Load data\n",
    "    valid_doc_ids, test_queries = load_test_data()\n",
    "    beir_data = load_beir_data()\n",
    "    \n",
    "    # Get embeddings\n",
    "    print(\"\\nProcessing embeddings...\")\n",
    "    test_embeddings = get_embeddings(test_queries, client, cache_file=cache_paths['query_cache'])\n",
    "    beir_embeddings = get_embeddings(beir_data['queries'], client, cache_file=cache_paths['beir_query_cache'])\n",
    "    beir_doc_embeddings = get_embeddings(beir_data['docs'], client, cache_file=cache_paths['beir_doc_cache'])\n",
    "    \n",
    "    # Process queries\n",
    "    results = process_queries(\n",
    "        test_queries,\n",
    "        test_embeddings,\n",
    "        beir_data,\n",
    "        beir_embeddings,\n",
    "        beir_doc_embeddings,\n",
    "        valid_doc_ids\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    save_results(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm596",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
